{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Boo-specific libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Yunzhou-specific libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 10\n",
    "SPLIT = 0.2\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import and Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "raw_cases_df = pd.read_csv('Data/confirmed_diff.csv')\n",
    "raw_deaths_df = pd.read_csv('Data/deaths_diff.csv')\n",
    "raw_mobility_df = pd.read_csv('Data/Google Mobility.csv')\n",
    "raw_whole_usa_cases_df = pd.read_csv('Data/us_confirmed_cases.csv')\n",
    "raw_whole_usa_deaths_df = pd.read_csv('Data/us_confirmed_deaths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR STATES\n",
    "# Clean Mobility Data\n",
    "# mobility_df already contains only those counties and states for which\n",
    "# complete data on all 6 mobility categories exist\n",
    "mobility_df = raw_mobility_df.copy()\n",
    "mobility_df = mobility_df.rename(columns={\"retail_and_recreation_percent_change_from_baseline\": \"retail_and_recreation\",\n",
    "        \"grocery_and_pharmacy_percent_change_from_baseline\": \"grocery_and_pharmacy\",\n",
    "        \"parks_percent_change_from_baseline\": \"parks\",\n",
    "        \"transit_stations_percent_change_from_baseline\": \"transit_stations\",\n",
    "        \"workplaces_percent_change_from_baseline\": \"workplaces\",\n",
    "        \"residential_percent_change_from_baseline\": \"residential\"})\n",
    "# Optional extreme shortening of names\n",
    "if True:\n",
    "    mobility_df = mobility_df.rename(columns={\"retail_and_recreation\": \"rr\",\n",
    "            \"grocery_and_pharmacy\": \"gp\",\n",
    "            \"transit_stations\": \"ts\",\n",
    "            \"workplaces\": \"wp\",\n",
    "            \"residential\": \"res\"})\n",
    "# US is the country already, do not need that data\n",
    "mobility_df.drop(columns=['country_region_code', 'country_region'], inplace=True)\n",
    "mobility_df.rename(columns={'sub_region_1': 'state', 'sub_region_2': 'county'}, inplace=True)\n",
    "\n",
    "# Extract all states only data, which have been marked with 'ZZZ' in the county name\n",
    "state_mobility_df = (mobility_df[mobility_df['county'] == 'ZZZ']).copy()\n",
    "state_mobility_df.drop(columns=['county'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR STATES\n",
    "# Clean Cases Data\n",
    "cases_df = raw_cases_df.copy()\n",
    "cases_df.rename(columns={'Admin2': 'county', 'Province_State': 'state', 'Date': 'date', 'Value': 'cases'}, inplace=True)\n",
    "# Since this is on the state level, county info is not needed\n",
    "# 'region' and 'diff' are legacy columns that are not needed here\n",
    "cases_df.drop(columns=['Country_Region', 'county', 'Lat', 'Long_', 'region', 'diff'], inplace=True)\n",
    "# Sum up cases for each state\n",
    "state_cases_df = cases_df.groupby(['state', 'date'], as_index=False).agg({'cases':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR STATES\n",
    "# Merge Google Mobility and Cases Data\n",
    "state_cases_mobility_df = state_cases_df.merge(state_mobility_df, how='inner', on=['state', 'date'])\n",
    "# Sort to ensure proper order\n",
    "state_cases_mobility_df['date'] = pd.to_datetime(state_cases_mobility_df['date'], format=\"%m/%d/%Y\")\n",
    "state_cases_mobility_df.sort_values(['state', 'date'], ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out all applicable states for later\n",
    "state_names = ['South Carolina', 'Louisiana', 'Virginia', 'Idaho', 'Iowa',\n",
    "               'Kentucky', 'Missouri', 'Oklahoma', 'Colorado', 'Illinois',\n",
    "               'Indiana', 'Mississippi', 'Nebraska', 'North Dakota', 'Ohio',\n",
    "               'Pennsylvania', 'Washington', 'Wisconsin', 'Vermont', 'Minnesota',\n",
    "               'Florida', 'North Carolina', 'California', 'New York', 'Wyoming',\n",
    "               'Michigan', 'Alaska', 'Maryland', 'Kansas', 'Tennessee', 'Texas',\n",
    "               'Maine', 'Arizona', 'Georgia', 'Arkansas', 'New Jersey',\n",
    "               'South Dakota', 'Alabama', 'Oregon', 'West Virginia',\n",
    "               'Massachusetts', 'Utah', 'Montana', 'New Hampshire', 'New Mexico',\n",
    "               'Rhode Island', 'Nevada', 'District of Columbia', 'Connecticut',\n",
    "               'Hawaii']\n",
    "# total of 50 statewide data (49 states + Washington D.C.)\n",
    "# Missing Delaware\n",
    "state_names_no_mobility = states.copy()\n",
    "state_names_no_mobility.append('Delaware')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is, \"state_cases_mobility_df\" contains confirmed cases data for each state at each point in time, with mobility data.\n",
    "\n",
    "\"state_cases_df\" only contains confirmed cases data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform Data**\n",
    "\n",
    "*Note that these models all assume,\n",
    "the underlying trend of cases for each state is the same,\n",
    "and there are no state-by-state variations in how the disease spreads,\n",
    "which is likely unrealistic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Data\n",
    "# Following Boo\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# state_cases_mobility_normalized_df = scaler.fit_transform(state_cases_mobility_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to Split Data into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize Data for LSTM\n",
    "# Objective is to predict cases for current day given previous N_STEPS days\n",
    "# However, having multiple states, each as their own individual time series,\n",
    "# complicates this.\n",
    "# input should be of shape (n_steps, n_features) with multiple samples\n",
    "\n",
    "# Modified from Boo's code\n",
    "def get_LSTM_states_dataset(dataset, n_steps=1):\n",
    "    dataset = dataset.copy()\n",
    "    dataset.drop(columns=['date'], inplace=True)\n",
    "    #print(dataset.shape)\n",
    "    dataset_by_states = []\n",
    "    # Normalize data while you are at it\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    for state in dataset['state'].unique():\n",
    "        state_values = dataset[dataset['state'] == state].drop(columns='state').values\n",
    "        state_values = scaler.fit_transform(state_values)\n",
    "        dataset_by_states.append(state_values)\n",
    "    X, y = [], []\n",
    "    for state_dataset in dataset_by_states:\n",
    "        for i in range(len(state_dataset) - n_steps - 1):\n",
    "            a = state_dataset[i:(i+n_steps)]\n",
    "            X.append(a)\n",
    "            y.append(state_dataset[i + n_steps][0])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "\n",
    "X_LSTM, y_LSTM = get_LSTM_states_dataset(state_cases_mobility_df, n_steps=N_STEPS)\n",
    "X_train_LSTM, X_test_LSTM, y_train_LSTM, y_test_LSTM = train_test_split(X_LSTM, y_LSTM, test_size=SPLIT, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize Data for CNN/Regular NN\n",
    "# Objective is to predict cases for current day given previous N_STEPS days.\n",
    "# We must add the value of the columns from previous\n",
    "# days as extra features per observation\n",
    "def get_CNN_states_dataset(dataset, n_steps=1):\n",
    "    dataset = dataset.copy()\n",
    "    dataset.drop(columns=['date'], inplace=True)\n",
    "    #print(dataset.shape)\n",
    "    dataset_by_states = []\n",
    "    # Normalize data while you are at it\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    for state in dataset['state'].unique():\n",
    "        state_values = dataset[dataset['state'] == state].drop(columns='state').values\n",
    "        state_values = scaler.fit_transform(state_values)\n",
    "        dataset_by_states.append(state_values)\n",
    "    X, y = [], []\n",
    "    for state_dataset in dataset_by_states:\n",
    "        for i in range(len(state_dataset) - n_steps - 1):\n",
    "            a = state_dataset[i:(i+n_steps)]\n",
    "            X.append(a)\n",
    "            y.append(state_dataset[i + n_steps][0])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "\n",
    "X_CNN, y_CNN = get_CNN_states_dataset(state_cases_mobility_df, n_steps=N_STEPS)\n",
    "X_train_CNN, X_test_CNN, y_train_CNN, y_test_CNN = train_test_split(X_CNN, y_CNN, test_size=SPLIT, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize Data for Yunzhou's NN\n",
    "\n",
    "def get_YNN_states_dataset(dataset, n_steps=1):\n",
    "    dataset = dataset.copy()\n",
    "    dataset.drop(columns=['date'], inplace=True)\n",
    "    #print(dataset.shape)\n",
    "    dataset_by_states = []\n",
    "    # Normalize data while you are at it\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    for state in dataset['state'].unique():\n",
    "        state_values = dataset[dataset['state'] == state].drop(columns='state').values\n",
    "        state_values = scaler.fit_transform(state_values)\n",
    "        dataset_by_states.append(state_values)\n",
    "    X, y = [], []\n",
    "    for state_dataset in dataset_by_states:\n",
    "        for i in range(len(state_dataset) - n_steps - 1):\n",
    "            a = state_dataset[i:(i+n_steps)]\n",
    "            # Flatten a, a list of lists, into a single row of features\n",
    "            flat_a = []\n",
    "            for sublist in a:\n",
    "                for item in sublist:\n",
    "                    flat_a.append(item)\n",
    "            X.append(flat_a)\n",
    "            y.append(state_dataset[i + n_steps][0])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "\n",
    "X_YNN, y_YNN = get_YNN_states_dataset(state_cases_mobility_df, n_steps=N_STEPS)\n",
    "X_train_YNN, X_test_YNN, y_train_YNN, y_test_YNN = train_test_split(X_YNN, y_YNN, test_size=SPLIT, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Model with Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LSTM, Boo's model\n",
    "def get_lstm(n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer = adam, loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "n_lstm_features = X_train_LSTM.shape[2]\n",
    "lstm = get_lstm(N_STEPS, n_lstm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2440 samples, validate on 610 samples\n",
      "Epoch 1/500\n",
      "2440/2440 [==============================] - 1s 399us/step - loss: 0.0252 - mae: 0.1046 - mse: 0.0252 - val_loss: 0.0031 - val_mae: 0.0398 - val_mse: 0.0031\n",
      "Epoch 2/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 0.0024 - mae: 0.0347 - mse: 0.0024 - val_loss: 0.0019 - val_mae: 0.0282 - val_mse: 0.0019\n",
      "Epoch 3/500\n",
      "2440/2440 [==============================] - 1s 292us/step - loss: 0.0018 - mae: 0.0315 - mse: 0.0018 - val_loss: 0.0015 - val_mae: 0.0269 - val_mse: 0.0015\n",
      "Epoch 4/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 0.0014 - mae: 0.0269 - mse: 0.0014 - val_loss: 0.0014 - val_mae: 0.0237 - val_mse: 0.0014\n",
      "Epoch 5/500\n",
      "2440/2440 [==============================] - 1s 266us/step - loss: 0.0012 - mae: 0.0249 - mse: 0.0012 - val_loss: 0.0012 - val_mae: 0.0227 - val_mse: 0.0012\n",
      "Epoch 6/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 0.0011 - mae: 0.0233 - mse: 0.0011 - val_loss: 0.0011 - val_mae: 0.0214 - val_mse: 0.0011\n",
      "Epoch 7/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 9.6685e-04 - mae: 0.0219 - mse: 9.6685e-04 - val_loss: 9.4089e-04 - val_mae: 0.0202 - val_mse: 9.4089e-04\n",
      "Epoch 8/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 9.0345e-04 - mae: 0.0213 - mse: 9.0345e-04 - val_loss: 8.7361e-04 - val_mae: 0.0189 - val_mse: 8.7361e-04\n",
      "Epoch 9/500\n",
      "2440/2440 [==============================] - 1s 308us/step - loss: 8.7582e-04 - mae: 0.0212 - mse: 8.7582e-04 - val_loss: 8.3631e-04 - val_mae: 0.0186 - val_mse: 8.3631e-04\n",
      "Epoch 10/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.5603e-04 - mae: 0.0211 - mse: 8.5603e-04 - val_loss: 7.9515e-04 - val_mae: 0.0183 - val_mse: 7.9515e-04\n",
      "Epoch 11/500\n",
      "2440/2440 [==============================] - 1s 263us/step - loss: 8.1692e-04 - mae: 0.0207 - mse: 8.1692e-04 - val_loss: 7.4250e-04 - val_mae: 0.0174 - val_mse: 7.4250e-04\n",
      "Epoch 12/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 7.6273e-04 - mae: 0.0198 - mse: 7.6273e-04 - val_loss: 6.9514e-04 - val_mae: 0.0166 - val_mse: 6.9514e-04\n",
      "Epoch 13/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 7.1098e-04 - mae: 0.0190 - mse: 7.1098e-04 - val_loss: 6.5596e-04 - val_mae: 0.0161 - val_mse: 6.5596e-04\n",
      "Epoch 14/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 6.6782e-04 - mae: 0.0183 - mse: 6.6782e-04 - val_loss: 6.2185e-04 - val_mae: 0.0157 - val_mse: 6.2185e-04\n",
      "Epoch 15/500\n",
      "2440/2440 [==============================] - 1s 262us/step - loss: 6.3059e-04 - mae: 0.0178 - mse: 6.3059e-04 - val_loss: 5.9032e-04 - val_mae: 0.0154 - val_mse: 5.9032e-04\n",
      "Epoch 16/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 5.9625e-04 - mae: 0.0173 - mse: 5.9625e-04 - val_loss: 5.6016e-04 - val_mae: 0.0151 - val_mse: 5.6016e-04\n",
      "Epoch 17/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 5.6369e-04 - mae: 0.0167 - mse: 5.6369e-04 - val_loss: 5.3123e-04 - val_mae: 0.0149 - val_mse: 5.3123e-04\n",
      "Epoch 18/500\n",
      "2440/2440 [==============================] - 1s 261us/step - loss: 5.3298e-04 - mae: 0.0162 - mse: 5.3298e-04 - val_loss: 5.0386e-04 - val_mae: 0.0146 - val_mse: 5.0386e-04\n",
      "Epoch 19/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 5.0442e-04 - mae: 0.0158 - mse: 5.0442e-04 - val_loss: 4.7839e-04 - val_mae: 0.0143 - val_mse: 4.7839e-04\n",
      "Epoch 20/500\n",
      "2440/2440 [==============================] - 1s 260us/step - loss: 4.7814e-04 - mae: 0.0153 - mse: 4.7814e-04 - val_loss: 4.5507e-04 - val_mae: 0.0141 - val_mse: 4.5507e-04\n",
      "Epoch 21/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 4.5401e-04 - mae: 0.0149 - mse: 4.5401e-04 - val_loss: 4.3400e-04 - val_mae: 0.0138 - val_mse: 4.3400e-04\n",
      "Epoch 22/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 4.3177e-04 - mae: 0.0145 - mse: 4.3177e-04 - val_loss: 4.1501e-04 - val_mae: 0.0136 - val_mse: 4.1501e-04\n",
      "Epoch 23/500\n",
      "2440/2440 [==============================] - 1s 263us/step - loss: 4.1102e-04 - mae: 0.0142 - mse: 4.1102e-04 - val_loss: 3.9762e-04 - val_mae: 0.0134 - val_mse: 3.9762e-04\n",
      "Epoch 24/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 3.9143e-04 - mae: 0.0138 - mse: 3.9143e-04 - val_loss: 3.8121e-04 - val_mae: 0.0131 - val_mse: 3.8121e-04\n",
      "Epoch 25/500\n",
      "2440/2440 [==============================] - 1s 261us/step - loss: 3.7282e-04 - mae: 0.0134 - mse: 3.7282e-04 - val_loss: 3.6529e-04 - val_mae: 0.0127 - val_mse: 3.6529e-04\n",
      "Epoch 26/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 3.5522e-04 - mae: 0.0130 - mse: 3.5522e-04 - val_loss: 3.4979e-04 - val_mae: 0.0122 - val_mse: 3.4979e-04\n",
      "Epoch 27/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 3.3884e-04 - mae: 0.0125 - mse: 3.3884e-04 - val_loss: 3.3512e-04 - val_mae: 0.0117 - val_mse: 3.3512e-04\n",
      "Epoch 28/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 3.2397e-04 - mae: 0.0121 - mse: 3.2397e-04 - val_loss: 3.2195e-04 - val_mae: 0.0112 - val_mse: 3.2195e-04\n",
      "Epoch 29/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 3.1087e-04 - mae: 0.0118 - mse: 3.1087e-04 - val_loss: 3.1086e-04 - val_mae: 0.0108 - val_mse: 3.1086e-04\n",
      "Epoch 30/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 2.9960e-04 - mae: 0.0115 - mse: 2.9960e-04 - val_loss: 3.0208e-04 - val_mae: 0.0104 - val_mse: 3.0208e-04\n",
      "Epoch 31/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 2.9005e-04 - mae: 0.0112 - mse: 2.9005e-04 - val_loss: 2.9538e-04 - val_mae: 0.0101 - val_mse: 2.9538e-04\n",
      "Epoch 32/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 2.8193e-04 - mae: 0.0110 - mse: 2.8193e-04 - val_loss: 2.9032e-04 - val_mae: 0.0099 - val_mse: 2.9032e-04\n",
      "Epoch 33/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 2.7490e-04 - mae: 0.0109 - mse: 2.7490e-04 - val_loss: 2.8641e-04 - val_mae: 0.0098 - val_mse: 2.8641e-04\n",
      "Epoch 34/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 2.6865e-04 - mae: 0.0107 - mse: 2.6865e-04 - val_loss: 2.8327e-04 - val_mae: 0.0097 - val_mse: 2.8327e-04\n",
      "Epoch 35/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 2.6295e-04 - mae: 0.0106 - mse: 2.6295e-04 - val_loss: 2.8064e-04 - val_mae: 0.0096 - val_mse: 2.8064e-04\n",
      "Epoch 36/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 2.5764e-04 - mae: 0.0105 - mse: 2.5764e-04 - val_loss: 2.7836e-04 - val_mae: 0.0096 - val_mse: 2.7836e-04\n",
      "Epoch 37/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 2.5264e-04 - mae: 0.0104 - mse: 2.5264e-04 - val_loss: 2.7632e-04 - val_mae: 0.0095 - val_mse: 2.7632e-04\n",
      "Epoch 38/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 2.4790e-04 - mae: 0.0103 - mse: 2.4790e-04 - val_loss: 2.7442e-04 - val_mae: 0.0095 - val_mse: 2.7442e-04\n",
      "Epoch 39/500\n",
      "2440/2440 [==============================] - 1s 313us/step - loss: 2.4339e-04 - mae: 0.0102 - mse: 2.4339e-04 - val_loss: 2.7263e-04 - val_mae: 0.0094 - val_mse: 2.7263e-04\n",
      "Epoch 40/500\n",
      "2440/2440 [==============================] - 1s 349us/step - loss: 2.3910e-04 - mae: 0.0101 - mse: 2.3910e-04 - val_loss: 2.7090e-04 - val_mae: 0.0094 - val_mse: 2.7090e-04\n",
      "Epoch 41/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 2.3502e-04 - mae: 0.0100 - mse: 2.3502e-04 - val_loss: 2.6921e-04 - val_mae: 0.0093 - val_mse: 2.6921e-04\n",
      "Epoch 42/500\n",
      "2440/2440 [==============================] - 1s 267us/step - loss: 2.3114e-04 - mae: 0.0100 - mse: 2.3114e-04 - val_loss: 2.6754e-04 - val_mae: 0.0093 - val_mse: 2.6754e-04\n",
      "Epoch 43/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 2.2747e-04 - mae: 0.0099 - mse: 2.2747e-04 - val_loss: 2.6589e-04 - val_mae: 0.0092 - val_mse: 2.6589e-04\n",
      "Epoch 44/500\n",
      "2440/2440 [==============================] - 1s 262us/step - loss: 2.2399e-04 - mae: 0.0098 - mse: 2.2399e-04 - val_loss: 2.6424e-04 - val_mae: 0.0092 - val_mse: 2.6424e-04\n",
      "Epoch 45/500\n",
      "2440/2440 [==============================] - 1s 325us/step - loss: 2.2070e-04 - mae: 0.0097 - mse: 2.2070e-04 - val_loss: 2.6261e-04 - val_mae: 0.0091 - val_mse: 2.6261e-04\n",
      "Epoch 46/500\n",
      "2440/2440 [==============================] - 1s 292us/step - loss: 2.1758e-04 - mae: 0.0096 - mse: 2.1758e-04 - val_loss: 2.6101e-04 - val_mae: 0.0090 - val_mse: 2.6101e-04\n",
      "Epoch 47/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 2.1464e-04 - mae: 0.0096 - mse: 2.1464e-04 - val_loss: 2.5944e-04 - val_mae: 0.0090 - val_mse: 2.5944e-04\n",
      "Epoch 48/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 2.1185e-04 - mae: 0.0095 - mse: 2.1185e-04 - val_loss: 2.5791e-04 - val_mae: 0.0089 - val_mse: 2.5791e-04\n",
      "Epoch 49/500\n",
      "2440/2440 [==============================] - 1s 338us/step - loss: 2.0921e-04 - mae: 0.0094 - mse: 2.0921e-04 - val_loss: 2.5644e-04 - val_mae: 0.0089 - val_mse: 2.5644e-04\n",
      "Epoch 50/500\n",
      "2440/2440 [==============================] - 1s 313us/step - loss: 2.0670e-04 - mae: 0.0094 - mse: 2.0670e-04 - val_loss: 2.5502e-04 - val_mae: 0.0088 - val_mse: 2.5502e-04\n",
      "Epoch 51/500\n",
      "2440/2440 [==============================] - 1s 307us/step - loss: 2.0432e-04 - mae: 0.0093 - mse: 2.0432e-04 - val_loss: 2.5368e-04 - val_mae: 0.0088 - val_mse: 2.5368e-04\n",
      "Epoch 52/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 2.0205e-04 - mae: 0.0092 - mse: 2.0205e-04 - val_loss: 2.5240e-04 - val_mae: 0.0088 - val_mse: 2.5240e-04\n",
      "Epoch 53/500\n",
      "2440/2440 [==============================] - 1s 348us/step - loss: 1.9987e-04 - mae: 0.0092 - mse: 1.9987e-04 - val_loss: 2.5118e-04 - val_mae: 0.0087 - val_mse: 2.5118e-04\n",
      "Epoch 54/500\n",
      "2440/2440 [==============================] - 1s 311us/step - loss: 1.9777e-04 - mae: 0.0091 - mse: 1.9777e-04 - val_loss: 2.5003e-04 - val_mae: 0.0087 - val_mse: 2.5003e-04\n",
      "Epoch 55/500\n",
      "2440/2440 [==============================] - 1s 310us/step - loss: 1.9575e-04 - mae: 0.0091 - mse: 1.9575e-04 - val_loss: 2.4892e-04 - val_mae: 0.0087 - val_mse: 2.4892e-04\n",
      "Epoch 56/500\n",
      "2440/2440 [==============================] - 1s 365us/step - loss: 1.9380e-04 - mae: 0.0090 - mse: 1.9380e-04 - val_loss: 2.4784e-04 - val_mae: 0.0087 - val_mse: 2.4784e-04\n",
      "Epoch 57/500\n",
      "2440/2440 [==============================] - 1s 309us/step - loss: 1.9190e-04 - mae: 0.0090 - mse: 1.9190e-04 - val_loss: 2.4678e-04 - val_mae: 0.0087 - val_mse: 2.4678e-04\n",
      "Epoch 58/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 1.9005e-04 - mae: 0.0090 - mse: 1.9005e-04 - val_loss: 2.4573e-04 - val_mae: 0.0087 - val_mse: 2.4573e-04\n",
      "Epoch 59/500\n",
      "2440/2440 [==============================] - 1s 309us/step - loss: 1.8825e-04 - mae: 0.0089 - mse: 1.8825e-04 - val_loss: 2.4466e-04 - val_mae: 0.0087 - val_mse: 2.4466e-04\n",
      "Epoch 60/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.8649e-04 - mae: 0.0089 - mse: 1.8649e-04 - val_loss: 2.4356e-04 - val_mae: 0.0087 - val_mse: 2.4356e-04\n",
      "Epoch 61/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.8479e-04 - mae: 0.0088 - mse: 1.8478e-04 - val_loss: 2.4242e-04 - val_mae: 0.0086 - val_mse: 2.4242e-04\n",
      "Epoch 62/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 1.8312e-04 - mae: 0.0088 - mse: 1.8312e-04 - val_loss: 2.4124e-04 - val_mae: 0.0086 - val_mse: 2.4124e-04\n",
      "Epoch 63/500\n",
      "2440/2440 [==============================] - 1s 310us/step - loss: 1.8150e-04 - mae: 0.0087 - mse: 1.8150e-04 - val_loss: 2.4001e-04 - val_mae: 0.0086 - val_mse: 2.4001e-04\n",
      "Epoch 64/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.7992e-04 - mae: 0.0087 - mse: 1.7992e-04 - val_loss: 2.3875e-04 - val_mae: 0.0086 - val_mse: 2.3875e-04\n",
      "Epoch 65/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.7838e-04 - mae: 0.0087 - mse: 1.7838e-04 - val_loss: 2.3745e-04 - val_mae: 0.0085 - val_mse: 2.3745e-04\n",
      "Epoch 66/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.7688e-04 - mae: 0.0086 - mse: 1.7688e-04 - val_loss: 2.3613e-04 - val_mae: 0.0085 - val_mse: 2.3613e-04\n",
      "Epoch 67/500\n",
      "2440/2440 [==============================] - 1s 302us/step - loss: 1.7543e-04 - mae: 0.0086 - mse: 1.7543e-04 - val_loss: 2.3481e-04 - val_mae: 0.0084 - val_mse: 2.3481e-04\n",
      "Epoch 68/500\n",
      "2440/2440 [==============================] - 1s 321us/step - loss: 1.7401e-04 - mae: 0.0085 - mse: 1.7401e-04 - val_loss: 2.3350e-04 - val_mae: 0.0084 - val_mse: 2.3350e-04\n",
      "Epoch 69/500\n",
      "2440/2440 [==============================] - ETA: 0s - loss: 1.7197e-04 - mae: 0.0085 - mse: 1.7198e-0 - 1s 319us/step - loss: 1.7263e-04 - mae: 0.0085 - mse: 1.7263e-04 - val_loss: 2.3220e-04 - val_mae: 0.0083 - val_mse: 2.3220e-04\n",
      "Epoch 70/500\n",
      "2440/2440 [==============================] - 1s 312us/step - loss: 1.7129e-04 - mae: 0.0085 - mse: 1.7129e-04 - val_loss: 2.3093e-04 - val_mae: 0.0083 - val_mse: 2.3093e-04\n",
      "Epoch 71/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.6998e-04 - mae: 0.0084 - mse: 1.6998e-04 - val_loss: 2.2968e-04 - val_mae: 0.0082 - val_mse: 2.2968e-04\n",
      "Epoch 72/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.6871e-04 - mae: 0.0084 - mse: 1.6871e-04 - val_loss: 2.2847e-04 - val_mae: 0.0082 - val_mse: 2.2847e-04\n",
      "Epoch 73/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.6747e-04 - mae: 0.0083 - mse: 1.6747e-04 - val_loss: 2.2730e-04 - val_mae: 0.0081 - val_mse: 2.2730e-04\n",
      "Epoch 74/500\n",
      "2440/2440 [==============================] - 1s 315us/step - loss: 1.6626e-04 - mae: 0.0083 - mse: 1.6626e-04 - val_loss: 2.2616e-04 - val_mae: 0.0081 - val_mse: 2.2616e-04\n",
      "Epoch 75/500\n",
      "2440/2440 [==============================] - 1s 310us/step - loss: 1.6508e-04 - mae: 0.0083 - mse: 1.6508e-04 - val_loss: 2.2506e-04 - val_mae: 0.0080 - val_mse: 2.2506e-04\n",
      "Epoch 76/500\n",
      "2440/2440 [==============================] - 1s 310us/step - loss: 1.6394e-04 - mae: 0.0082 - mse: 1.6394e-04 - val_loss: 2.2399e-04 - val_mae: 0.0079 - val_mse: 2.2399e-04\n",
      "Epoch 77/500\n",
      "2440/2440 [==============================] - 1s 310us/step - loss: 1.6284e-04 - mae: 0.0082 - mse: 1.6284e-04 - val_loss: 2.2298e-04 - val_mae: 0.0079 - val_mse: 2.2298e-04\n",
      "Epoch 78/500\n",
      "2440/2440 [==============================] - 1s 323us/step - loss: 1.6177e-04 - mae: 0.0082 - mse: 1.6177e-04 - val_loss: 2.2201e-04 - val_mae: 0.0078 - val_mse: 2.2201e-04\n",
      "Epoch 79/500\n",
      "2440/2440 [==============================] - 1s 307us/step - loss: 1.6076e-04 - mae: 0.0081 - mse: 1.6076e-04 - val_loss: 2.2110e-04 - val_mae: 0.0078 - val_mse: 2.2110e-04\n",
      "Epoch 80/500\n",
      "2440/2440 [==============================] - 1s 312us/step - loss: 1.5980e-04 - mae: 0.0081 - mse: 1.5980e-04 - val_loss: 2.2025e-04 - val_mae: 0.0077 - val_mse: 2.2025e-04\n",
      "Epoch 81/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.5891e-04 - mae: 0.0080 - mse: 1.5891e-04 - val_loss: 2.1947e-04 - val_mae: 0.0077 - val_mse: 2.1947e-04\n",
      "Epoch 82/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 1.5810e-04 - mae: 0.0080 - mse: 1.5810e-04 - val_loss: 2.1876e-04 - val_mae: 0.0077 - val_mse: 2.1876e-04\n",
      "Epoch 83/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.5738e-04 - mae: 0.0080 - mse: 1.5738e-04 - val_loss: 2.1812e-04 - val_mae: 0.0076 - val_mse: 2.1812e-04\n",
      "Epoch 84/500\n",
      "2440/2440 [==============================] - 1s 308us/step - loss: 1.5674e-04 - mae: 0.0080 - mse: 1.5674e-04 - val_loss: 2.1754e-04 - val_mae: 0.0076 - val_mse: 2.1754e-04\n",
      "Epoch 85/500\n",
      "2440/2440 [==============================] - 1s 325us/step - loss: 1.5618e-04 - mae: 0.0079 - mse: 1.5618e-04 - val_loss: 2.1703e-04 - val_mae: 0.0076 - val_mse: 2.1703e-04\n",
      "Epoch 86/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.5569e-04 - mae: 0.0079 - mse: 1.5569e-04 - val_loss: 2.1661e-04 - val_mae: 0.0076 - val_mse: 2.1661e-04\n",
      "Epoch 87/500\n",
      "2440/2440 [==============================] - 1s 312us/step - loss: 1.5525e-04 - mae: 0.0079 - mse: 1.5525e-04 - val_loss: 2.1633e-04 - val_mae: 0.0076 - val_mse: 2.1633e-04\n",
      "Epoch 88/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.5487e-04 - mae: 0.0079 - mse: 1.5487e-04 - val_loss: 2.1625e-04 - val_mae: 0.0076 - val_mse: 2.1625e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.5454e-04 - mae: 0.0079 - mse: 1.5454e-04 - val_loss: 2.1646e-04 - val_mae: 0.0077 - val_mse: 2.1646e-04\n",
      "Epoch 90/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.5427e-04 - mae: 0.0079 - mse: 1.5427e-04 - val_loss: 2.1707e-04 - val_mae: 0.0078 - val_mse: 2.1707e-04\n",
      "Epoch 91/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 1.5406e-04 - mae: 0.0079 - mse: 1.5406e-04 - val_loss: 2.1826e-04 - val_mae: 0.0080 - val_mse: 2.1826e-04\n",
      "Epoch 92/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.5391e-04 - mae: 0.0079 - mse: 1.5391e-04 - val_loss: 2.2018e-04 - val_mae: 0.0082 - val_mse: 2.2018e-04\n",
      "Epoch 93/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.5381e-04 - mae: 0.0079 - mse: 1.5381e-04 - val_loss: 2.2294e-04 - val_mae: 0.0085 - val_mse: 2.2294e-04\n",
      "Epoch 94/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.5365e-04 - mae: 0.0079 - mse: 1.5365e-04 - val_loss: 2.2631e-04 - val_mae: 0.0088 - val_mse: 2.2631e-04\n",
      "Epoch 95/500\n",
      "2440/2440 [==============================] - 1s 314us/step - loss: 1.5326e-04 - mae: 0.0079 - mse: 1.5326e-04 - val_loss: 2.2942e-04 - val_mae: 0.0090 - val_mse: 2.2942e-04\n",
      "Epoch 96/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.5247e-04 - mae: 0.0079 - mse: 1.5247e-04 - val_loss: 2.3094e-04 - val_mae: 0.0091 - val_mse: 2.3094e-04\n",
      "Epoch 97/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.5127e-04 - mae: 0.0079 - mse: 1.5127e-04 - val_loss: 2.3003e-04 - val_mae: 0.0091 - val_mse: 2.3003e-04\n",
      "Epoch 98/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.4994e-04 - mae: 0.0078 - mse: 1.4994e-04 - val_loss: 2.2723e-04 - val_mae: 0.0088 - val_mse: 2.2723e-04\n",
      "Epoch 99/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.4893e-04 - mae: 0.0077 - mse: 1.4893e-04 - val_loss: 2.2400e-04 - val_mae: 0.0086 - val_mse: 2.2400e-04\n",
      "Epoch 100/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.4852e-04 - mae: 0.0077 - mse: 1.4852e-04 - val_loss: 2.2170e-04 - val_mae: 0.0084 - val_mse: 2.2170e-04\n",
      "Epoch 101/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.4861e-04 - mae: 0.0077 - mse: 1.4861e-04 - val_loss: 2.2123e-04 - val_mae: 0.0084 - val_mse: 2.2123e-04\n",
      "Epoch 102/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.4899e-04 - mae: 0.0077 - mse: 1.4899e-04 - val_loss: 2.2329e-04 - val_mae: 0.0085 - val_mse: 2.2329e-04\n",
      "Epoch 103/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.4970e-04 - mae: 0.0078 - mse: 1.4970e-04 - val_loss: 2.2895e-04 - val_mae: 0.0090 - val_mse: 2.2895e-04\n",
      "Epoch 104/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.5103e-04 - mae: 0.0079 - mse: 1.5103e-04 - val_loss: 2.3937e-04 - val_mae: 0.0097 - val_mse: 2.3937e-04\n",
      "Epoch 105/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.5299e-04 - mae: 0.0080 - mse: 1.5299e-04 - val_loss: 2.5206e-04 - val_mae: 0.0103 - val_mse: 2.5206e-04\n",
      "Epoch 106/500\n",
      "2440/2440 [==============================] - 1s 318us/step - loss: 1.5455e-04 - mae: 0.0081 - mse: 1.5455e-04 - val_loss: 2.5130e-04 - val_mae: 0.0102 - val_mse: 2.5130e-04\n",
      "Epoch 107/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.5282e-04 - mae: 0.0081 - mse: 1.5282e-04 - val_loss: 2.3203e-04 - val_mae: 0.0090 - val_mse: 2.3203e-04\n",
      "Epoch 108/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.5112e-04 - mae: 0.0080 - mse: 1.5112e-04 - val_loss: 2.2016e-04 - val_mae: 0.0083 - val_mse: 2.2016e-04\n",
      "Epoch 109/500\n",
      "2440/2440 [==============================] - 1s 345us/step - loss: 1.6092e-04 - mae: 0.0084 - mse: 1.6092e-04 - val_loss: 2.2348e-04 - val_mae: 0.0086 - val_mse: 2.2348e-04\n",
      "Epoch 110/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.7011e-04 - mae: 0.0087 - mse: 1.7011e-04 - val_loss: 2.6212e-04 - val_mae: 0.0110 - val_mse: 2.6212e-04\n",
      "Epoch 111/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.7385e-04 - mae: 0.0090 - mse: 1.7385e-04 - val_loss: 2.7737e-04 - val_mae: 0.0118 - val_mse: 2.7737e-04\n",
      "Epoch 112/500\n",
      "2440/2440 [==============================] - 1s 302us/step - loss: 2.1172e-04 - mae: 0.0105 - mse: 2.1172e-04 - val_loss: 4.4148e-04 - val_mae: 0.0167 - val_mse: 4.4148e-04\n",
      "Epoch 113/500\n",
      "2440/2440 [==============================] - 1s 294us/step - loss: 2.2385e-04 - mae: 0.0106 - mse: 2.2385e-04 - val_loss: 2.5653e-04 - val_mae: 0.0104 - val_mse: 2.5653e-04\n",
      "Epoch 114/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 1.6948e-04 - mae: 0.0088 - mse: 1.6948e-04 - val_loss: 2.3546e-04 - val_mae: 0.0092 - val_mse: 2.3546e-04\n",
      "Epoch 115/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 1.6312e-04 - mae: 0.0085 - mse: 1.6312e-04 - val_loss: 2.2922e-04 - val_mae: 0.0086 - val_mse: 2.2922e-04\n",
      "Epoch 116/500\n",
      "2440/2440 [==============================] - 1s 311us/step - loss: 1.5827e-04 - mae: 0.0083 - mse: 1.5827e-04 - val_loss: 2.2640e-04 - val_mae: 0.0084 - val_mse: 2.2640e-04\n",
      "Epoch 117/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.5542e-04 - mae: 0.0081 - mse: 1.5542e-04 - val_loss: 2.2455e-04 - val_mae: 0.0083 - val_mse: 2.2455e-04\n",
      "Epoch 118/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.5333e-04 - mae: 0.0080 - mse: 1.5333e-04 - val_loss: 2.2310e-04 - val_mae: 0.0082 - val_mse: 2.2310e-04\n",
      "Epoch 119/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.5174e-04 - mae: 0.0079 - mse: 1.5174e-04 - val_loss: 2.2186e-04 - val_mae: 0.0081 - val_mse: 2.2186e-04\n",
      "Epoch 120/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 1.5050e-04 - mae: 0.0079 - mse: 1.5050e-04 - val_loss: 2.2071e-04 - val_mae: 0.0080 - val_mse: 2.2071e-04\n",
      "Epoch 121/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.4954e-04 - mae: 0.0078 - mse: 1.4954e-04 - val_loss: 2.1963e-04 - val_mae: 0.0080 - val_mse: 2.1963e-04\n",
      "Epoch 122/500\n",
      "2440/2440 [==============================] - 1s 292us/step - loss: 1.4879e-04 - mae: 0.0077 - mse: 1.4879e-04 - val_loss: 2.1860e-04 - val_mae: 0.0079 - val_mse: 2.1860e-04\n",
      "Epoch 123/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.4821e-04 - mae: 0.0077 - mse: 1.4821e-04 - val_loss: 2.1764e-04 - val_mae: 0.0079 - val_mse: 2.1764e-04\n",
      "Epoch 124/500\n",
      "2440/2440 [==============================] - 1s 318us/step - loss: 1.4779e-04 - mae: 0.0077 - mse: 1.4779e-04 - val_loss: 2.1677e-04 - val_mae: 0.0079 - val_mse: 2.1677e-04\n",
      "Epoch 125/500\n",
      "2440/2440 [==============================] - 1s 313us/step - loss: 1.4753e-04 - mae: 0.0077 - mse: 1.4753e-04 - val_loss: 2.1602e-04 - val_mae: 0.0079 - val_mse: 2.1602e-04\n",
      "Epoch 126/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.4742e-04 - mae: 0.0077 - mse: 1.4742e-04 - val_loss: 2.1541e-04 - val_mae: 0.0079 - val_mse: 2.1541e-04\n",
      "Epoch 127/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.4746e-04 - mae: 0.0077 - mse: 1.4746e-04 - val_loss: 2.1496e-04 - val_mae: 0.0079 - val_mse: 2.1496e-04\n",
      "Epoch 128/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.4760e-04 - mae: 0.0077 - mse: 1.4760e-04 - val_loss: 2.1468e-04 - val_mae: 0.0079 - val_mse: 2.1468e-04\n",
      "Epoch 129/500\n",
      "2440/2440 [==============================] - 1s 294us/step - loss: 1.4780e-04 - mae: 0.0077 - mse: 1.4780e-04 - val_loss: 2.1455e-04 - val_mae: 0.0079 - val_mse: 2.1455e-04\n",
      "Epoch 130/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.4801e-04 - mae: 0.0077 - mse: 1.4801e-04 - val_loss: 2.1453e-04 - val_mae: 0.0080 - val_mse: 2.1453e-04\n",
      "Epoch 131/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.4815e-04 - mae: 0.0077 - mse: 1.4815e-04 - val_loss: 2.1453e-04 - val_mae: 0.0080 - val_mse: 2.1453e-04\n",
      "Epoch 132/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.4816e-04 - mae: 0.0078 - mse: 1.4816e-04 - val_loss: 2.1451e-04 - val_mae: 0.0080 - val_mse: 2.1451e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.4803e-04 - mae: 0.0078 - mse: 1.4803e-04 - val_loss: 2.1443e-04 - val_mae: 0.0080 - val_mse: 2.1443e-04\n",
      "Epoch 134/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.4779e-04 - mae: 0.0078 - mse: 1.4779e-04 - val_loss: 2.1429e-04 - val_mae: 0.0080 - val_mse: 2.1429e-04\n",
      "Epoch 135/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.4749e-04 - mae: 0.0077 - mse: 1.4749e-04 - val_loss: 2.1412e-04 - val_mae: 0.0080 - val_mse: 2.1412e-04\n",
      "Epoch 136/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.4719e-04 - mae: 0.0077 - mse: 1.4719e-04 - val_loss: 2.1397e-04 - val_mae: 0.0080 - val_mse: 2.1397e-04\n",
      "Epoch 137/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.4691e-04 - mae: 0.0077 - mse: 1.4691e-04 - val_loss: 2.1389e-04 - val_mae: 0.0080 - val_mse: 2.1389e-04\n",
      "Epoch 138/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.4668e-04 - mae: 0.0077 - mse: 1.4668e-04 - val_loss: 2.1390e-04 - val_mae: 0.0081 - val_mse: 2.1390e-04\n",
      "Epoch 139/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.4650e-04 - mae: 0.0077 - mse: 1.4650e-04 - val_loss: 2.1404e-04 - val_mae: 0.0081 - val_mse: 2.1404e-04\n",
      "Epoch 140/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.4637e-04 - mae: 0.0077 - mse: 1.4637e-04 - val_loss: 2.1433e-04 - val_mae: 0.0081 - val_mse: 2.1433e-04\n",
      "Epoch 141/500\n",
      "2440/2440 [==============================] - 1s 313us/step - loss: 1.4628e-04 - mae: 0.0077 - mse: 1.4628e-04 - val_loss: 2.1480e-04 - val_mae: 0.0082 - val_mse: 2.1480e-04\n",
      "Epoch 142/500\n",
      "2440/2440 [==============================] - 1s 309us/step - loss: 1.4623e-04 - mae: 0.0078 - mse: 1.4623e-04 - val_loss: 2.1551e-04 - val_mae: 0.0083 - val_mse: 2.1551e-04\n",
      "Epoch 143/500\n",
      "2440/2440 [==============================] - 1s 312us/step - loss: 1.4624e-04 - mae: 0.0078 - mse: 1.4624e-04 - val_loss: 2.1652e-04 - val_mae: 0.0083 - val_mse: 2.1652e-04\n",
      "Epoch 144/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.4633e-04 - mae: 0.0078 - mse: 1.4633e-04 - val_loss: 2.1791e-04 - val_mae: 0.0085 - val_mse: 2.1791e-04\n",
      "Epoch 145/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 1.4651e-04 - mae: 0.0078 - mse: 1.4651e-04 - val_loss: 2.1981e-04 - val_mae: 0.0086 - val_mse: 2.1981e-04\n",
      "Epoch 146/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.4684e-04 - mae: 0.0079 - mse: 1.4684e-04 - val_loss: 2.2235e-04 - val_mae: 0.0088 - val_mse: 2.2235e-04\n",
      "Epoch 147/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.4735e-04 - mae: 0.0079 - mse: 1.4735e-04 - val_loss: 2.2563e-04 - val_mae: 0.0091 - val_mse: 2.2563e-04\n",
      "Epoch 148/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.4813e-04 - mae: 0.0080 - mse: 1.4813e-04 - val_loss: 2.2965e-04 - val_mae: 0.0093 - val_mse: 2.2965e-04\n",
      "Epoch 149/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.4921e-04 - mae: 0.0080 - mse: 1.4921e-04 - val_loss: 2.3423e-04 - val_mae: 0.0096 - val_mse: 2.3423e-04\n",
      "Epoch 150/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.5062e-04 - mae: 0.0081 - mse: 1.5062e-04 - val_loss: 2.3900e-04 - val_mae: 0.0098 - val_mse: 2.3900e-04\n",
      "Epoch 151/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.5233e-04 - mae: 0.0082 - mse: 1.5233e-04 - val_loss: 2.4366e-04 - val_mae: 0.0101 - val_mse: 2.4366e-04\n",
      "Epoch 152/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.5425e-04 - mae: 0.0083 - mse: 1.5425e-04 - val_loss: 2.4843e-04 - val_mae: 0.0103 - val_mse: 2.4843e-04\n",
      "Epoch 153/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.5635e-04 - mae: 0.0084 - mse: 1.5635e-04 - val_loss: 2.5432e-04 - val_mae: 0.0106 - val_mse: 2.5432e-04\n",
      "Epoch 154/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.5876e-04 - mae: 0.0085 - mse: 1.5876e-04 - val_loss: 2.6116e-04 - val_mae: 0.0108 - val_mse: 2.6116e-04\n",
      "Epoch 155/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 1.6154e-04 - mae: 0.0087 - mse: 1.6154e-04 - val_loss: 2.6328e-04 - val_mae: 0.0108 - val_mse: 2.6328e-04\n",
      "Epoch 156/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.6487e-04 - mae: 0.0089 - mse: 1.6487e-04 - val_loss: 2.6285e-04 - val_mae: 0.0107 - val_mse: 2.6285e-04\n",
      "Epoch 157/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.7068e-04 - mae: 0.0092 - mse: 1.7068e-04 - val_loss: 2.6550e-04 - val_mae: 0.0107 - val_mse: 2.6550e-04\n",
      "Epoch 158/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.7975e-04 - mae: 0.0096 - mse: 1.7975e-04 - val_loss: 2.5128e-04 - val_mae: 0.0098 - val_mse: 2.5128e-04\n",
      "Epoch 159/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.8368e-04 - mae: 0.0097 - mse: 1.8368e-04 - val_loss: 2.2336e-04 - val_mae: 0.0081 - val_mse: 2.2336e-04\n",
      "Epoch 160/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.7027e-04 - mae: 0.0091 - mse: 1.7027e-04 - val_loss: 2.1904e-04 - val_mae: 0.0085 - val_mse: 2.1904e-04\n",
      "Epoch 161/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.4847e-04 - mae: 0.0080 - mse: 1.4847e-04 - val_loss: 2.1095e-04 - val_mae: 0.0081 - val_mse: 2.1095e-04\n",
      "Epoch 162/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.3712e-04 - mae: 0.0074 - mse: 1.3712e-04 - val_loss: 2.1043e-04 - val_mae: 0.0081 - val_mse: 2.1043e-04\n",
      "Epoch 163/500\n",
      "2440/2440 [==============================] - 1s 315us/step - loss: 1.3482e-04 - mae: 0.0073 - mse: 1.3482e-04 - val_loss: 2.1020e-04 - val_mae: 0.0080 - val_mse: 2.1020e-04\n",
      "Epoch 164/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.3504e-04 - mae: 0.0073 - mse: 1.3504e-04 - val_loss: 2.0961e-04 - val_mae: 0.0078 - val_mse: 2.0961e-04\n",
      "Epoch 165/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.3578e-04 - mae: 0.0074 - mse: 1.3578e-04 - val_loss: 2.0903e-04 - val_mae: 0.0077 - val_mse: 2.0903e-04\n",
      "Epoch 166/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.3645e-04 - mae: 0.0075 - mse: 1.3645e-04 - val_loss: 2.0871e-04 - val_mae: 0.0076 - val_mse: 2.0871e-04\n",
      "Epoch 167/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.3696e-04 - mae: 0.0075 - mse: 1.3696e-04 - val_loss: 2.0871e-04 - val_mae: 0.0076 - val_mse: 2.0871e-04\n",
      "Epoch 168/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.3732e-04 - mae: 0.0075 - mse: 1.3732e-04 - val_loss: 2.0896e-04 - val_mae: 0.0076 - val_mse: 2.0896e-04\n",
      "Epoch 169/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.3757e-04 - mae: 0.0076 - mse: 1.3757e-04 - val_loss: 2.0935e-04 - val_mae: 0.0076 - val_mse: 2.0935e-04\n",
      "Epoch 170/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.3770e-04 - mae: 0.0076 - mse: 1.3770e-04 - val_loss: 2.0982e-04 - val_mae: 0.0076 - val_mse: 2.0982e-04\n",
      "Epoch 171/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.3776e-04 - mae: 0.0076 - mse: 1.3776e-04 - val_loss: 2.1034e-04 - val_mae: 0.0076 - val_mse: 2.1034e-04\n",
      "Epoch 172/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.3776e-04 - mae: 0.0076 - mse: 1.3776e-04 - val_loss: 2.1089e-04 - val_mae: 0.0077 - val_mse: 2.1089e-04\n",
      "Epoch 173/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.3772e-04 - mae: 0.0076 - mse: 1.3772e-04 - val_loss: 2.1145e-04 - val_mae: 0.0077 - val_mse: 2.1145e-04\n",
      "Epoch 174/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.3767e-04 - mae: 0.0076 - mse: 1.3767e-04 - val_loss: 2.1205e-04 - val_mae: 0.0077 - val_mse: 2.1205e-04\n",
      "Epoch 175/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.3762e-04 - mae: 0.0076 - mse: 1.3762e-04 - val_loss: 2.1266e-04 - val_mae: 0.0078 - val_mse: 2.1266e-04\n",
      "Epoch 176/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.3757e-04 - mae: 0.0076 - mse: 1.3757e-04 - val_loss: 2.1330e-04 - val_mae: 0.0078 - val_mse: 2.1330e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.3755e-04 - mae: 0.0076 - mse: 1.3755e-04 - val_loss: 2.1394e-04 - val_mae: 0.0079 - val_mse: 2.1394e-04\n",
      "Epoch 178/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.3754e-04 - mae: 0.0077 - mse: 1.3754e-04 - val_loss: 2.1456e-04 - val_mae: 0.0079 - val_mse: 2.1456e-04\n",
      "Epoch 179/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.3756e-04 - mae: 0.0077 - mse: 1.3756e-04 - val_loss: 2.1515e-04 - val_mae: 0.0079 - val_mse: 2.1515e-04\n",
      "Epoch 180/500\n",
      "2440/2440 [==============================] - 1s 270us/step - loss: 1.3761e-04 - mae: 0.0077 - mse: 1.3761e-04 - val_loss: 2.1567e-04 - val_mae: 0.0079 - val_mse: 2.1567e-04\n",
      "Epoch 181/500\n",
      "2440/2440 [==============================] - 1s 274us/step - loss: 1.3771e-04 - mae: 0.0077 - mse: 1.3771e-04 - val_loss: 2.1607e-04 - val_mae: 0.0079 - val_mse: 2.1607e-04\n",
      "Epoch 182/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.3785e-04 - mae: 0.0077 - mse: 1.3785e-04 - val_loss: 2.1629e-04 - val_mae: 0.0080 - val_mse: 2.1629e-04\n",
      "Epoch 183/500\n",
      "2440/2440 [==============================] - 1s 271us/step - loss: 1.3806e-04 - mae: 0.0078 - mse: 1.3806e-04 - val_loss: 2.1629e-04 - val_mae: 0.0079 - val_mse: 2.1629e-04\n",
      "Epoch 184/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.3835e-04 - mae: 0.0078 - mse: 1.3835e-04 - val_loss: 2.1602e-04 - val_mae: 0.0079 - val_mse: 2.1602e-04\n",
      "Epoch 185/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.3870e-04 - mae: 0.0078 - mse: 1.3870e-04 - val_loss: 2.1545e-04 - val_mae: 0.0079 - val_mse: 2.1545e-04\n",
      "Epoch 186/500\n",
      "2440/2440 [==============================] - 1s 272us/step - loss: 1.3908e-04 - mae: 0.0078 - mse: 1.3908e-04 - val_loss: 2.1456e-04 - val_mae: 0.0078 - val_mse: 2.1456e-04\n",
      "Epoch 187/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.3936e-04 - mae: 0.0078 - mse: 1.3936e-04 - val_loss: 2.1338e-04 - val_mae: 0.0078 - val_mse: 2.1338e-04\n",
      "Epoch 188/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.3934e-04 - mae: 0.0078 - mse: 1.3934e-04 - val_loss: 2.1196e-04 - val_mae: 0.0077 - val_mse: 2.1196e-04\n",
      "Epoch 189/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.3881e-04 - mae: 0.0078 - mse: 1.3881e-04 - val_loss: 2.1050e-04 - val_mae: 0.0077 - val_mse: 2.1050e-04\n",
      "Epoch 190/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.3768e-04 - mae: 0.0077 - mse: 1.3768e-04 - val_loss: 2.0926e-04 - val_mae: 0.0077 - val_mse: 2.0926e-04\n",
      "Epoch 191/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.3620e-04 - mae: 0.0076 - mse: 1.3620e-04 - val_loss: 2.0842e-04 - val_mae: 0.0077 - val_mse: 2.0842e-04\n",
      "Epoch 192/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.3475e-04 - mae: 0.0074 - mse: 1.3475e-04 - val_loss: 2.0797e-04 - val_mae: 0.0078 - val_mse: 2.0797e-04\n",
      "Epoch 193/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.3357e-04 - mae: 0.0074 - mse: 1.3357e-04 - val_loss: 2.0777e-04 - val_mae: 0.0078 - val_mse: 2.0777e-04\n",
      "Epoch 194/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.3263e-04 - mae: 0.0073 - mse: 1.3263e-04 - val_loss: 2.0771e-04 - val_mae: 0.0078 - val_mse: 2.0771e-04\n",
      "Epoch 195/500\n",
      "2440/2440 [==============================] - 1s 314us/step - loss: 1.3188e-04 - mae: 0.0073 - mse: 1.3188e-04 - val_loss: 2.0773e-04 - val_mae: 0.0078 - val_mse: 2.0773e-04\n",
      "Epoch 196/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.3120e-04 - mae: 0.0072 - mse: 1.3120e-04 - val_loss: 2.0779e-04 - val_mae: 0.0078 - val_mse: 2.0779e-04\n",
      "Epoch 197/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.3057e-04 - mae: 0.0072 - mse: 1.3057e-04 - val_loss: 2.0786e-04 - val_mae: 0.0077 - val_mse: 2.0786e-04\n",
      "Epoch 198/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.2999e-04 - mae: 0.0072 - mse: 1.2999e-04 - val_loss: 2.0795e-04 - val_mae: 0.0077 - val_mse: 2.0795e-04\n",
      "Epoch 199/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.2948e-04 - mae: 0.0072 - mse: 1.2948e-04 - val_loss: 2.0810e-04 - val_mae: 0.0077 - val_mse: 2.0810e-04\n",
      "Epoch 200/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.2903e-04 - mae: 0.0072 - mse: 1.2903e-04 - val_loss: 2.0830e-04 - val_mae: 0.0077 - val_mse: 2.0830e-04\n",
      "Epoch 201/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.2864e-04 - mae: 0.0072 - mse: 1.2864e-04 - val_loss: 2.0859e-04 - val_mae: 0.0077 - val_mse: 2.0859e-04\n",
      "Epoch 202/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.2830e-04 - mae: 0.0072 - mse: 1.2830e-04 - val_loss: 2.0896e-04 - val_mae: 0.0077 - val_mse: 2.0896e-04\n",
      "Epoch 203/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.2801e-04 - mae: 0.0072 - mse: 1.2801e-04 - val_loss: 2.0941e-04 - val_mae: 0.0077 - val_mse: 2.0941e-04\n",
      "Epoch 204/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.2778e-04 - mae: 0.0072 - mse: 1.2778e-04 - val_loss: 2.0994e-04 - val_mae: 0.0077 - val_mse: 2.0994e-04\n",
      "Epoch 205/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.2760e-04 - mae: 0.0072 - mse: 1.2760e-04 - val_loss: 2.1055e-04 - val_mae: 0.0077 - val_mse: 2.1055e-04\n",
      "Epoch 206/500\n",
      "2440/2440 [==============================] - 1s 311us/step - loss: 1.2748e-04 - mae: 0.0072 - mse: 1.2748e-04 - val_loss: 2.1122e-04 - val_mae: 0.0077 - val_mse: 2.1122e-04\n",
      "Epoch 207/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.2741e-04 - mae: 0.0072 - mse: 1.2741e-04 - val_loss: 2.1196e-04 - val_mae: 0.0077 - val_mse: 2.1196e-04\n",
      "Epoch 208/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.2739e-04 - mae: 0.0072 - mse: 1.2739e-04 - val_loss: 2.1275e-04 - val_mae: 0.0077 - val_mse: 2.1275e-04\n",
      "Epoch 209/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.2741e-04 - mae: 0.0072 - mse: 1.2741e-04 - val_loss: 2.1359e-04 - val_mae: 0.0078 - val_mse: 2.1359e-04\n",
      "Epoch 210/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.2749e-04 - mae: 0.0072 - mse: 1.2749e-04 - val_loss: 2.1446e-04 - val_mae: 0.0078 - val_mse: 2.1446e-04\n",
      "Epoch 211/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.2761e-04 - mae: 0.0072 - mse: 1.2761e-04 - val_loss: 2.1535e-04 - val_mae: 0.0078 - val_mse: 2.1535e-04\n",
      "Epoch 212/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.2781e-04 - mae: 0.0072 - mse: 1.2781e-04 - val_loss: 2.1622e-04 - val_mae: 0.0079 - val_mse: 2.1622e-04\n",
      "Epoch 213/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.2810e-04 - mae: 0.0073 - mse: 1.2810e-04 - val_loss: 2.1702e-04 - val_mae: 0.0079 - val_mse: 2.1702e-04\n",
      "Epoch 214/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.2851e-04 - mae: 0.0073 - mse: 1.2851e-04 - val_loss: 2.1766e-04 - val_mae: 0.0079 - val_mse: 2.1766e-04\n",
      "Epoch 215/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.2907e-04 - mae: 0.0073 - mse: 1.2907e-04 - val_loss: 2.1802e-04 - val_mae: 0.0079 - val_mse: 2.1802e-04\n",
      "Epoch 216/500\n",
      "2440/2440 [==============================] - 1s 301us/step - loss: 1.2975e-04 - mae: 0.0074 - mse: 1.2975e-04 - val_loss: 2.1797e-04 - val_mae: 0.0079 - val_mse: 2.1797e-04\n",
      "Epoch 217/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.3049e-04 - mae: 0.0074 - mse: 1.3049e-04 - val_loss: 2.1736e-04 - val_mae: 0.0079 - val_mse: 2.1736e-04\n",
      "Epoch 218/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.3108e-04 - mae: 0.0075 - mse: 1.3108e-04 - val_loss: 2.1615e-04 - val_mae: 0.0078 - val_mse: 2.1615e-04\n",
      "Epoch 219/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.3123e-04 - mae: 0.0075 - mse: 1.3123e-04 - val_loss: 2.1448e-04 - val_mae: 0.0077 - val_mse: 2.1448e-04\n",
      "Epoch 220/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.3067e-04 - mae: 0.0074 - mse: 1.3067e-04 - val_loss: 2.1275e-04 - val_mae: 0.0077 - val_mse: 2.1275e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.2938e-04 - mae: 0.0073 - mse: 1.2938e-04 - val_loss: 2.1146e-04 - val_mae: 0.0076 - val_mse: 2.1146e-04\n",
      "Epoch 222/500\n",
      "2440/2440 [==============================] - 1s 270us/step - loss: 1.2762e-04 - mae: 0.0072 - mse: 1.2762e-04 - val_loss: 2.1086e-04 - val_mae: 0.0076 - val_mse: 2.1086e-04\n",
      "Epoch 223/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.2577e-04 - mae: 0.0071 - mse: 1.2577e-04 - val_loss: 2.1093e-04 - val_mae: 0.0077 - val_mse: 2.1093e-04\n",
      "Epoch 224/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.2404e-04 - mae: 0.0070 - mse: 1.2404e-04 - val_loss: 2.1153e-04 - val_mae: 0.0077 - val_mse: 2.1153e-04\n",
      "Epoch 225/500\n",
      "2440/2440 [==============================] - 1s 322us/step - loss: 1.2252e-04 - mae: 0.0069 - mse: 1.2252e-04 - val_loss: 2.1250e-04 - val_mae: 0.0077 - val_mse: 2.1250e-04\n",
      "Epoch 226/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 1.2122e-04 - mae: 0.0068 - mse: 1.2122e-04 - val_loss: 2.1374e-04 - val_mae: 0.0078 - val_mse: 2.1374e-04\n",
      "Epoch 227/500\n",
      "2440/2440 [==============================] - 1s 328us/step - loss: 1.2020e-04 - mae: 0.0067 - mse: 1.2020e-04 - val_loss: 2.1516e-04 - val_mae: 0.0078 - val_mse: 2.1516e-04\n",
      "Epoch 228/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.1944e-04 - mae: 0.0067 - mse: 1.1944e-04 - val_loss: 2.1668e-04 - val_mae: 0.0078 - val_mse: 2.1668e-04\n",
      "Epoch 229/500\n",
      "2440/2440 [==============================] - 1s 365us/step - loss: 1.1893e-04 - mae: 0.0066 - mse: 1.1893e-04 - val_loss: 2.1818e-04 - val_mae: 0.0079 - val_mse: 2.1818e-04\n",
      "Epoch 230/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 1.1863e-04 - mae: 0.0066 - mse: 1.1863e-04 - val_loss: 2.1959e-04 - val_mae: 0.0079 - val_mse: 2.1959e-04\n",
      "Epoch 231/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.1848e-04 - mae: 0.0066 - mse: 1.1848e-04 - val_loss: 2.2087e-04 - val_mae: 0.0079 - val_mse: 2.2087e-04\n",
      "Epoch 232/500\n",
      "2440/2440 [==============================] - 1s 343us/step - loss: 1.1845e-04 - mae: 0.0066 - mse: 1.1845e-04 - val_loss: 2.2204e-04 - val_mae: 0.0080 - val_mse: 2.2204e-04\n",
      "Epoch 233/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 1.1852e-04 - mae: 0.0066 - mse: 1.1852e-04 - val_loss: 2.2315e-04 - val_mae: 0.0080 - val_mse: 2.2315e-04\n",
      "Epoch 234/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.1865e-04 - mae: 0.0066 - mse: 1.1865e-04 - val_loss: 2.2429e-04 - val_mae: 0.0081 - val_mse: 2.2429e-04\n",
      "Epoch 235/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.1882e-04 - mae: 0.0066 - mse: 1.1882e-04 - val_loss: 2.2550e-04 - val_mae: 0.0082 - val_mse: 2.2550e-04\n",
      "Epoch 236/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 1.1900e-04 - mae: 0.0067 - mse: 1.1900e-04 - val_loss: 2.2688e-04 - val_mae: 0.0083 - val_mse: 2.2688e-04\n",
      "Epoch 237/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.1916e-04 - mae: 0.0067 - mse: 1.1916e-04 - val_loss: 2.2849e-04 - val_mae: 0.0085 - val_mse: 2.2849e-04\n",
      "Epoch 238/500\n",
      "2440/2440 [==============================] - 1s 308us/step - loss: 1.1930e-04 - mae: 0.0067 - mse: 1.1930e-04 - val_loss: 2.3037e-04 - val_mae: 0.0086 - val_mse: 2.3037e-04\n",
      "Epoch 239/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.1940e-04 - mae: 0.0067 - mse: 1.1940e-04 - val_loss: 2.3253e-04 - val_mae: 0.0088 - val_mse: 2.3253e-04\n",
      "Epoch 240/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1948e-04 - mae: 0.0067 - mse: 1.1948e-04 - val_loss: 2.3494e-04 - val_mae: 0.0089 - val_mse: 2.3494e-04\n",
      "Epoch 241/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.1958e-04 - mae: 0.0068 - mse: 1.1958e-04 - val_loss: 2.3743e-04 - val_mae: 0.0090 - val_mse: 2.3743e-04\n",
      "Epoch 242/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.1973e-04 - mae: 0.0068 - mse: 1.1973e-04 - val_loss: 2.3982e-04 - val_mae: 0.0091 - val_mse: 2.3982e-04\n",
      "Epoch 243/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.1998e-04 - mae: 0.0069 - mse: 1.1998e-04 - val_loss: 2.4175e-04 - val_mae: 0.0091 - val_mse: 2.4175e-04\n",
      "Epoch 244/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.2038e-04 - mae: 0.0069 - mse: 1.2038e-04 - val_loss: 2.4268e-04 - val_mae: 0.0091 - val_mse: 2.4268e-04\n",
      "Epoch 245/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.2099e-04 - mae: 0.0070 - mse: 1.2098e-04 - val_loss: 2.4194e-04 - val_mae: 0.0090 - val_mse: 2.4194e-04\n",
      "Epoch 246/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.2182e-04 - mae: 0.0071 - mse: 1.2182e-04 - val_loss: 2.3923e-04 - val_mae: 0.0087 - val_mse: 2.3923e-04\n",
      "Epoch 247/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.2293e-04 - mae: 0.0071 - mse: 1.2293e-04 - val_loss: 2.3491e-04 - val_mae: 0.0085 - val_mse: 2.3491e-04\n",
      "Epoch 248/500\n",
      "2440/2440 [==============================] - 1s 318us/step - loss: 1.2424e-04 - mae: 0.0072 - mse: 1.2424e-04 - val_loss: 2.2962e-04 - val_mae: 0.0082 - val_mse: 2.2962e-04\n",
      "Epoch 249/500\n",
      "2440/2440 [==============================] - 1s 318us/step - loss: 1.2527e-04 - mae: 0.0072 - mse: 1.2527e-04 - val_loss: 2.2389e-04 - val_mae: 0.0080 - val_mse: 2.2389e-04\n",
      "Epoch 250/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.2522e-04 - mae: 0.0072 - mse: 1.2522e-04 - val_loss: 2.1852e-04 - val_mae: 0.0079 - val_mse: 2.1852e-04\n",
      "Epoch 251/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.2369e-04 - mae: 0.0071 - mse: 1.2369e-04 - val_loss: 2.1520e-04 - val_mae: 0.0078 - val_mse: 2.1520e-04\n",
      "Epoch 252/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 1.2125e-04 - mae: 0.0069 - mse: 1.2125e-04 - val_loss: 2.1514e-04 - val_mae: 0.0078 - val_mse: 2.1514e-04\n",
      "Epoch 253/500\n",
      "2440/2440 [==============================] - ETA: 0s - loss: 1.1701e-04 - mae: 0.0067 - mse: 1.1701e-0 - 1s 285us/step - loss: 1.1915e-04 - mae: 0.0068 - mse: 1.1915e-04 - val_loss: 2.1720e-04 - val_mae: 0.0079 - val_mse: 2.1720e-04\n",
      "Epoch 254/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.1815e-04 - mae: 0.0068 - mse: 1.1815e-04 - val_loss: 2.1943e-04 - val_mae: 0.0080 - val_mse: 2.1943e-04\n",
      "Epoch 255/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.1784e-04 - mae: 0.0068 - mse: 1.1784e-04 - val_loss: 2.2181e-04 - val_mae: 0.0081 - val_mse: 2.2181e-04\n",
      "Epoch 256/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.1801e-04 - mae: 0.0068 - mse: 1.1801e-04 - val_loss: 2.2471e-04 - val_mae: 0.0081 - val_mse: 2.2471e-04\n",
      "Epoch 257/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.1891e-04 - mae: 0.0069 - mse: 1.1891e-04 - val_loss: 2.2728e-04 - val_mae: 0.0082 - val_mse: 2.2728e-04\n",
      "Epoch 258/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.2049e-04 - mae: 0.0070 - mse: 1.2049e-04 - val_loss: 2.2848e-04 - val_mae: 0.0082 - val_mse: 2.2848e-04\n",
      "Epoch 259/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.2232e-04 - mae: 0.0071 - mse: 1.2232e-04 - val_loss: 2.2798e-04 - val_mae: 0.0082 - val_mse: 2.2798e-04\n",
      "Epoch 260/500\n",
      "2440/2440 [==============================] - 1s 315us/step - loss: 1.2386e-04 - mae: 0.0072 - mse: 1.2386e-04 - val_loss: 2.2654e-04 - val_mae: 0.0081 - val_mse: 2.2654e-04\n",
      "Epoch 261/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.2468e-04 - mae: 0.0072 - mse: 1.2468e-04 - val_loss: 2.2555e-04 - val_mae: 0.0079 - val_mse: 2.2555e-04\n",
      "Epoch 262/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 1.2495e-04 - mae: 0.0072 - mse: 1.2495e-04 - val_loss: 2.2549e-04 - val_mae: 0.0079 - val_mse: 2.2549e-04\n",
      "Epoch 263/500\n",
      "2440/2440 [==============================] - 1s 320us/step - loss: 1.2511e-04 - mae: 0.0072 - mse: 1.2511e-04 - val_loss: 2.2575e-04 - val_mae: 0.0078 - val_mse: 2.2575e-04\n",
      "Epoch 264/500\n",
      "2440/2440 [==============================] - 1s 336us/step - loss: 1.2518e-04 - mae: 0.0072 - mse: 1.2518e-04 - val_loss: 2.2589e-04 - val_mae: 0.0078 - val_mse: 2.2589e-04\n",
      "Epoch 265/500\n",
      "2440/2440 [==============================] - 1s 292us/step - loss: 1.2499e-04 - mae: 0.0072 - mse: 1.2499e-04 - val_loss: 2.2533e-04 - val_mae: 0.0078 - val_mse: 2.2533e-04\n",
      "Epoch 266/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.2428e-04 - mae: 0.0072 - mse: 1.2428e-04 - val_loss: 2.2415e-04 - val_mae: 0.0078 - val_mse: 2.2415e-04\n",
      "Epoch 267/500\n",
      "2440/2440 [==============================] - ETA: 0s - loss: 1.2250e-04 - mae: 0.0070 - mse: 1.2250e-0 - 1s 324us/step - loss: 1.2302e-04 - mae: 0.0071 - mse: 1.2302e-04 - val_loss: 2.2300e-04 - val_mae: 0.0078 - val_mse: 2.2300e-04\n",
      "Epoch 268/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.2155e-04 - mae: 0.0070 - mse: 1.2155e-04 - val_loss: 2.2238e-04 - val_mae: 0.0078 - val_mse: 2.2238e-04\n",
      "Epoch 269/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.2007e-04 - mae: 0.0069 - mse: 1.2007e-04 - val_loss: 2.2239e-04 - val_mae: 0.0078 - val_mse: 2.2239e-04\n",
      "Epoch 270/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.1866e-04 - mae: 0.0068 - mse: 1.1866e-04 - val_loss: 2.2288e-04 - val_mae: 0.0079 - val_mse: 2.2288e-04\n",
      "Epoch 271/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.1736e-04 - mae: 0.0067 - mse: 1.1736e-04 - val_loss: 2.2370e-04 - val_mae: 0.0079 - val_mse: 2.2370e-04\n",
      "Epoch 272/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.1622e-04 - mae: 0.0066 - mse: 1.1622e-04 - val_loss: 2.2470e-04 - val_mae: 0.0080 - val_mse: 2.2470e-04\n",
      "Epoch 273/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.1527e-04 - mae: 0.0066 - mse: 1.1527e-04 - val_loss: 2.2576e-04 - val_mae: 0.0081 - val_mse: 2.2576e-04\n",
      "Epoch 274/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1459e-04 - mae: 0.0066 - mse: 1.1459e-04 - val_loss: 2.2674e-04 - val_mae: 0.0081 - val_mse: 2.2674e-04\n",
      "Epoch 275/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.1428e-04 - mae: 0.0066 - mse: 1.1428e-04 - val_loss: 2.2758e-04 - val_mae: 0.0081 - val_mse: 2.2758e-04\n",
      "Epoch 276/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.1435e-04 - mae: 0.0066 - mse: 1.1435e-04 - val_loss: 2.2827e-04 - val_mae: 0.0082 - val_mse: 2.2827e-04\n",
      "Epoch 277/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.1468e-04 - mae: 0.0066 - mse: 1.1468e-04 - val_loss: 2.2879e-04 - val_mae: 0.0082 - val_mse: 2.2879e-04\n",
      "Epoch 278/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.1505e-04 - mae: 0.0067 - mse: 1.1505e-04 - val_loss: 2.2903e-04 - val_mae: 0.0082 - val_mse: 2.2903e-04\n",
      "Epoch 279/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1535e-04 - mae: 0.0067 - mse: 1.1535e-04 - val_loss: 2.2887e-04 - val_mae: 0.0081 - val_mse: 2.2887e-04\n",
      "Epoch 280/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.1552e-04 - mae: 0.0067 - mse: 1.1552e-04 - val_loss: 2.2849e-04 - val_mae: 0.0080 - val_mse: 2.2849e-04\n",
      "Epoch 281/500\n",
      "2440/2440 [==============================] - 1s 308us/step - loss: 1.1545e-04 - mae: 0.0068 - mse: 1.1545e-04 - val_loss: 2.2823e-04 - val_mae: 0.0079 - val_mse: 2.2823e-04\n",
      "Epoch 282/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.1507e-04 - mae: 0.0068 - mse: 1.1507e-04 - val_loss: 2.2839e-04 - val_mae: 0.0079 - val_mse: 2.2839e-04\n",
      "Epoch 283/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.1433e-04 - mae: 0.0067 - mse: 1.1433e-04 - val_loss: 2.2909e-04 - val_mae: 0.0078 - val_mse: 2.2909e-04\n",
      "Epoch 284/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.1337e-04 - mae: 0.0067 - mse: 1.1337e-04 - val_loss: 2.3030e-04 - val_mae: 0.0078 - val_mse: 2.3030e-04\n",
      "Epoch 285/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.1234e-04 - mae: 0.0066 - mse: 1.1234e-04 - val_loss: 2.3184e-04 - val_mae: 0.0079 - val_mse: 2.3184e-04\n",
      "Epoch 286/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.1141e-04 - mae: 0.0066 - mse: 1.1141e-04 - val_loss: 2.3350e-04 - val_mae: 0.0079 - val_mse: 2.3350e-04\n",
      "Epoch 287/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.1063e-04 - mae: 0.0065 - mse: 1.1063e-04 - val_loss: 2.3515e-04 - val_mae: 0.0080 - val_mse: 2.3515e-04\n",
      "Epoch 288/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.0999e-04 - mae: 0.0065 - mse: 1.0999e-04 - val_loss: 2.3672e-04 - val_mae: 0.0080 - val_mse: 2.3672e-04\n",
      "Epoch 289/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.0943e-04 - mae: 0.0065 - mse: 1.0943e-04 - val_loss: 2.3822e-04 - val_mae: 0.0081 - val_mse: 2.3822e-04\n",
      "Epoch 290/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0892e-04 - mae: 0.0065 - mse: 1.0892e-04 - val_loss: 2.3972e-04 - val_mae: 0.0082 - val_mse: 2.3972e-04\n",
      "Epoch 291/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.0842e-04 - mae: 0.0064 - mse: 1.0842e-04 - val_loss: 2.4125e-04 - val_mae: 0.0082 - val_mse: 2.4125e-04\n",
      "Epoch 292/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.0794e-04 - mae: 0.0064 - mse: 1.0794e-04 - val_loss: 2.4282e-04 - val_mae: 0.0083 - val_mse: 2.4282e-04\n",
      "Epoch 293/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.0749e-04 - mae: 0.0064 - mse: 1.0749e-04 - val_loss: 2.4441e-04 - val_mae: 0.0084 - val_mse: 2.4441e-04\n",
      "Epoch 294/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0709e-04 - mae: 0.0064 - mse: 1.0709e-04 - val_loss: 2.4601e-04 - val_mae: 0.0085 - val_mse: 2.4601e-04\n",
      "Epoch 295/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0675e-04 - mae: 0.0064 - mse: 1.0675e-04 - val_loss: 2.4763e-04 - val_mae: 0.0085 - val_mse: 2.4763e-04\n",
      "Epoch 296/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.0648e-04 - mae: 0.0064 - mse: 1.0648e-04 - val_loss: 2.4934e-04 - val_mae: 0.0086 - val_mse: 2.4934e-04\n",
      "Epoch 297/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0629e-04 - mae: 0.0064 - mse: 1.0629e-04 - val_loss: 2.5124e-04 - val_mae: 0.0087 - val_mse: 2.5124e-04\n",
      "Epoch 298/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.0625e-04 - mae: 0.0064 - mse: 1.0625e-04 - val_loss: 2.5349e-04 - val_mae: 0.0089 - val_mse: 2.5349e-04\n",
      "Epoch 299/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.0645e-04 - mae: 0.0065 - mse: 1.0645e-04 - val_loss: 2.5628e-04 - val_mae: 0.0091 - val_mse: 2.5628e-04\n",
      "Epoch 300/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.0701e-04 - mae: 0.0066 - mse: 1.0701e-04 - val_loss: 2.5975e-04 - val_mae: 0.0093 - val_mse: 2.5975e-04\n",
      "Epoch 301/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0803e-04 - mae: 0.0067 - mse: 1.0803e-04 - val_loss: 2.6384e-04 - val_mae: 0.0095 - val_mse: 2.6384e-04\n",
      "Epoch 302/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0945e-04 - mae: 0.0068 - mse: 1.0945e-04 - val_loss: 2.6800e-04 - val_mae: 0.0097 - val_mse: 2.6800e-04\n",
      "Epoch 303/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.1091e-04 - mae: 0.0069 - mse: 1.1091e-04 - val_loss: 2.7134e-04 - val_mae: 0.0098 - val_mse: 2.7134e-04\n",
      "Epoch 304/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.1212e-04 - mae: 0.0070 - mse: 1.1212e-04 - val_loss: 2.7378e-04 - val_mae: 0.0099 - val_mse: 2.7378e-04\n",
      "Epoch 305/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.1321e-04 - mae: 0.0071 - mse: 1.1321e-04 - val_loss: 2.7610e-04 - val_mae: 0.0098 - val_mse: 2.7610e-04\n",
      "Epoch 306/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1505e-04 - mae: 0.0072 - mse: 1.1505e-04 - val_loss: 2.7653e-04 - val_mae: 0.0097 - val_mse: 2.7653e-04\n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.1711e-04 - mae: 0.0073 - mse: 1.1711e-04 - val_loss: 2.7475e-04 - val_mae: 0.0093 - val_mse: 2.7475e-04\n",
      "Epoch 308/500\n",
      "2440/2440 [==============================] - 1s 272us/step - loss: 1.1935e-04 - mae: 0.0074 - mse: 1.1935e-04 - val_loss: 2.6739e-04 - val_mae: 0.0089 - val_mse: 2.6739e-04\n",
      "Epoch 309/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.1334e-04 - mae: 0.0069 - mse: 1.1334e-04 - val_loss: 2.4911e-04 - val_mae: 0.0083 - val_mse: 2.4911e-04\n",
      "Epoch 310/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.1226e-04 - mae: 0.0068 - mse: 1.1226e-04 - val_loss: 2.3343e-04 - val_mae: 0.0084 - val_mse: 2.3343e-04\n",
      "Epoch 311/500\n",
      "2440/2440 [==============================] - 1s 272us/step - loss: 1.1090e-04 - mae: 0.0067 - mse: 1.1090e-04 - val_loss: 2.3534e-04 - val_mae: 0.0087 - val_mse: 2.3534e-04\n",
      "Epoch 312/500\n",
      "2440/2440 [==============================] - 1s 274us/step - loss: 1.0835e-04 - mae: 0.0066 - mse: 1.0835e-04 - val_loss: 2.4501e-04 - val_mae: 0.0093 - val_mse: 2.4501e-04\n",
      "Epoch 313/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0730e-04 - mae: 0.0066 - mse: 1.0730e-04 - val_loss: 2.4569e-04 - val_mae: 0.0091 - val_mse: 2.4569e-04\n",
      "Epoch 314/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.0647e-04 - mae: 0.0066 - mse: 1.0647e-04 - val_loss: 2.4800e-04 - val_mae: 0.0088 - val_mse: 2.4800e-04\n",
      "Epoch 315/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0504e-04 - mae: 0.0066 - mse: 1.0504e-04 - val_loss: 2.5505e-04 - val_mae: 0.0091 - val_mse: 2.5505e-04\n",
      "Epoch 316/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0498e-04 - mae: 0.0066 - mse: 1.0498e-04 - val_loss: 2.6246e-04 - val_mae: 0.0098 - val_mse: 2.6246e-04\n",
      "Epoch 317/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0490e-04 - mae: 0.0067 - mse: 1.0490e-04 - val_loss: 2.6925e-04 - val_mae: 0.0104 - val_mse: 2.6925e-04\n",
      "Epoch 318/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0495e-04 - mae: 0.0067 - mse: 1.0495e-04 - val_loss: 2.7631e-04 - val_mae: 0.0107 - val_mse: 2.7631e-04\n",
      "Epoch 319/500\n",
      "2440/2440 [==============================] - 1s 339us/step - loss: 1.0515e-04 - mae: 0.0067 - mse: 1.0515e-04 - val_loss: 2.8139e-04 - val_mae: 0.0109 - val_mse: 2.8139e-04\n",
      "Epoch 320/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0781e-04 - mae: 0.0069 - mse: 1.0781e-04 - val_loss: 2.8381e-04 - val_mae: 0.0109 - val_mse: 2.8381e-04\n",
      "Epoch 321/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.1295e-04 - mae: 0.0071 - mse: 1.1295e-04 - val_loss: 2.8712e-04 - val_mae: 0.0113 - val_mse: 2.8712e-04\n",
      "Epoch 322/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1344e-04 - mae: 0.0072 - mse: 1.1344e-04 - val_loss: 2.9329e-04 - val_mae: 0.0118 - val_mse: 2.9329e-04\n",
      "Epoch 323/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.1022e-04 - mae: 0.0071 - mse: 1.1022e-04 - val_loss: 2.9387e-04 - val_mae: 0.0120 - val_mse: 2.9387e-04\n",
      "Epoch 324/500\n",
      "2440/2440 [==============================] - 1s 274us/step - loss: 1.1199e-04 - mae: 0.0072 - mse: 1.1199e-04 - val_loss: 3.0827e-04 - val_mae: 0.0126 - val_mse: 3.0827e-04\n",
      "Epoch 325/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.1385e-04 - mae: 0.0073 - mse: 1.1385e-04 - val_loss: 3.1188e-04 - val_mae: 0.0127 - val_mse: 3.1188e-04\n",
      "Epoch 326/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.2395e-04 - mae: 0.0078 - mse: 1.2395e-04 - val_loss: 3.1723e-04 - val_mae: 0.0130 - val_mse: 3.1723e-04\n",
      "Epoch 327/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.3626e-04 - mae: 0.0084 - mse: 1.3626e-04 - val_loss: 2.8395e-04 - val_mae: 0.0116 - val_mse: 2.8395e-04\n",
      "Epoch 328/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.4456e-04 - mae: 0.0087 - mse: 1.4456e-04 - val_loss: 2.3994e-04 - val_mae: 0.0089 - val_mse: 2.3994e-04\n",
      "Epoch 329/500\n",
      "2440/2440 [==============================] - 1s 301us/step - loss: 1.4670e-04 - mae: 0.0085 - mse: 1.4670e-04 - val_loss: 2.4583e-04 - val_mae: 0.0091 - val_mse: 2.4583e-04\n",
      "Epoch 330/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.3623e-04 - mae: 0.0080 - mse: 1.3623e-04 - val_loss: 2.7453e-04 - val_mae: 0.0107 - val_mse: 2.7453e-04\n",
      "Epoch 331/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.2039e-04 - mae: 0.0073 - mse: 1.2039e-04 - val_loss: 2.7021e-04 - val_mae: 0.0100 - val_mse: 2.7021e-04\n",
      "Epoch 332/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.1162e-04 - mae: 0.0069 - mse: 1.1162e-04 - val_loss: 2.6282e-04 - val_mae: 0.0092 - val_mse: 2.6282e-04\n",
      "Epoch 333/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0773e-04 - mae: 0.0067 - mse: 1.0773e-04 - val_loss: 2.6307e-04 - val_mae: 0.0089 - val_mse: 2.6307e-04\n",
      "Epoch 334/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0570e-04 - mae: 0.0066 - mse: 1.0570e-04 - val_loss: 2.6376e-04 - val_mae: 0.0087 - val_mse: 2.6376e-04\n",
      "Epoch 335/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.0445e-04 - mae: 0.0065 - mse: 1.0445e-04 - val_loss: 2.6427e-04 - val_mae: 0.0086 - val_mse: 2.6427e-04\n",
      "Epoch 336/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.0359e-04 - mae: 0.0065 - mse: 1.0359e-04 - val_loss: 2.6490e-04 - val_mae: 0.0087 - val_mse: 2.6490e-04\n",
      "Epoch 337/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0294e-04 - mae: 0.0065 - mse: 1.0294e-04 - val_loss: 2.6584e-04 - val_mae: 0.0089 - val_mse: 2.6584e-04\n",
      "Epoch 338/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0252e-04 - mae: 0.0065 - mse: 1.0252e-04 - val_loss: 2.6723e-04 - val_mae: 0.0092 - val_mse: 2.6723e-04\n",
      "Epoch 339/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.0237e-04 - mae: 0.0066 - mse: 1.0237e-04 - val_loss: 2.6938e-04 - val_mae: 0.0095 - val_mse: 2.6938e-04\n",
      "Epoch 340/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0257e-04 - mae: 0.0066 - mse: 1.0257e-04 - val_loss: 2.7242e-04 - val_mae: 0.0099 - val_mse: 2.7242e-04\n",
      "Epoch 341/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0316e-04 - mae: 0.0067 - mse: 1.0316e-04 - val_loss: 2.7592e-04 - val_mae: 0.0103 - val_mse: 2.7592e-04\n",
      "Epoch 342/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.0408e-04 - mae: 0.0068 - mse: 1.0408e-04 - val_loss: 2.7882e-04 - val_mae: 0.0106 - val_mse: 2.7882e-04\n",
      "Epoch 343/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0505e-04 - mae: 0.0069 - mse: 1.0505e-04 - val_loss: 2.7969e-04 - val_mae: 0.0107 - val_mse: 2.7969e-04\n",
      "Epoch 344/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0573e-04 - mae: 0.0069 - mse: 1.0573e-04 - val_loss: 2.7796e-04 - val_mae: 0.0106 - val_mse: 2.7796e-04\n",
      "Epoch 345/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.0618e-04 - mae: 0.0069 - mse: 1.0618e-04 - val_loss: 2.7507e-04 - val_mae: 0.0103 - val_mse: 2.7507e-04\n",
      "Epoch 346/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0683e-04 - mae: 0.0069 - mse: 1.0683e-04 - val_loss: 2.7348e-04 - val_mae: 0.0099 - val_mse: 2.7348e-04\n",
      "Epoch 347/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.0732e-04 - mae: 0.0069 - mse: 1.0732e-04 - val_loss: 2.7428e-04 - val_mae: 0.0098 - val_mse: 2.7428e-04\n",
      "Epoch 348/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0724e-04 - mae: 0.0069 - mse: 1.0724e-04 - val_loss: 2.7848e-04 - val_mae: 0.0101 - val_mse: 2.7848e-04\n",
      "Epoch 349/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.0743e-04 - mae: 0.0070 - mse: 1.0743e-04 - val_loss: 2.8443e-04 - val_mae: 0.0106 - val_mse: 2.8443e-04\n",
      "Epoch 350/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.0780e-04 - mae: 0.0070 - mse: 1.0780e-04 - val_loss: 2.8986e-04 - val_mae: 0.0111 - val_mse: 2.8986e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.0776e-04 - mae: 0.0070 - mse: 1.0776e-04 - val_loss: 2.9247e-04 - val_mae: 0.0112 - val_mse: 2.9247e-04\n",
      "Epoch 352/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.0654e-04 - mae: 0.0070 - mse: 1.0654e-04 - val_loss: 2.8964e-04 - val_mae: 0.0110 - val_mse: 2.8964e-04\n",
      "Epoch 353/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.0504e-04 - mae: 0.0068 - mse: 1.0504e-04 - val_loss: 2.8622e-04 - val_mae: 0.0105 - val_mse: 2.8622e-04\n",
      "Epoch 354/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.0642e-04 - mae: 0.0068 - mse: 1.0642e-04 - val_loss: 2.8711e-04 - val_mae: 0.0099 - val_mse: 2.8711e-04\n",
      "Epoch 355/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.0692e-04 - mae: 0.0069 - mse: 1.0692e-04 - val_loss: 2.8345e-04 - val_mae: 0.0095 - val_mse: 2.8345e-04\n",
      "Epoch 356/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.0579e-04 - mae: 0.0068 - mse: 1.0579e-04 - val_loss: 2.9011e-04 - val_mae: 0.0104 - val_mse: 2.9011e-04\n",
      "Epoch 357/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.0370e-04 - mae: 0.0068 - mse: 1.0370e-04 - val_loss: 2.9411e-04 - val_mae: 0.0108 - val_mse: 2.9411e-04\n",
      "Epoch 358/500\n",
      "2440/2440 [==============================] - 1s 274us/step - loss: 1.0255e-04 - mae: 0.0068 - mse: 1.0255e-04 - val_loss: 2.9329e-04 - val_mae: 0.0108 - val_mse: 2.9329e-04\n",
      "Epoch 359/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.0125e-04 - mae: 0.0067 - mse: 1.0125e-04 - val_loss: 2.8970e-04 - val_mae: 0.0105 - val_mse: 2.8970e-04\n",
      "Epoch 360/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0139e-04 - mae: 0.0067 - mse: 1.0139e-04 - val_loss: 2.8874e-04 - val_mae: 0.0102 - val_mse: 2.8874e-04\n",
      "Epoch 361/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.0218e-04 - mae: 0.0067 - mse: 1.0218e-04 - val_loss: 2.8985e-04 - val_mae: 0.0100 - val_mse: 2.8985e-04\n",
      "Epoch 362/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.0205e-04 - mae: 0.0067 - mse: 1.0205e-04 - val_loss: 2.9064e-04 - val_mae: 0.0099 - val_mse: 2.9064e-04\n",
      "Epoch 363/500\n",
      "2440/2440 [==============================] - 1s 348us/step - loss: 1.0120e-04 - mae: 0.0067 - mse: 1.0120e-04 - val_loss: 2.9573e-04 - val_mae: 0.0101 - val_mse: 2.9573e-04\n",
      "Epoch 364/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0035e-04 - mae: 0.0066 - mse: 1.0035e-04 - val_loss: 2.9697e-04 - val_mae: 0.0103 - val_mse: 2.9697e-04\n",
      "Epoch 365/500\n",
      "2440/2440 [==============================] - 1s 272us/step - loss: 9.8941e-05 - mae: 0.0066 - mse: 9.8941e-05 - val_loss: 2.9783e-04 - val_mae: 0.0104 - val_mse: 2.9783e-04\n",
      "Epoch 366/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 9.9748e-05 - mae: 0.0066 - mse: 9.9748e-05 - val_loss: 2.9827e-04 - val_mae: 0.0106 - val_mse: 2.9827e-04\n",
      "Epoch 367/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 9.8703e-05 - mae: 0.0066 - mse: 9.8703e-05 - val_loss: 2.9876e-04 - val_mae: 0.0106 - val_mse: 2.9876e-04\n",
      "Epoch 368/500\n",
      "2440/2440 [==============================] - 1s 271us/step - loss: 9.8884e-05 - mae: 0.0066 - mse: 9.8884e-05 - val_loss: 3.0853e-04 - val_mae: 0.0112 - val_mse: 3.0853e-04\n",
      "Epoch 369/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 9.9650e-05 - mae: 0.0067 - mse: 9.9650e-05 - val_loss: 3.0872e-04 - val_mae: 0.0114 - val_mse: 3.0872e-04\n",
      "Epoch 370/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 9.9616e-05 - mae: 0.0067 - mse: 9.9616e-05 - val_loss: 3.1178e-04 - val_mae: 0.0113 - val_mse: 3.1178e-04\n",
      "Epoch 371/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 9.8426e-05 - mae: 0.0066 - mse: 9.8426e-05 - val_loss: 3.0929e-04 - val_mae: 0.0111 - val_mse: 3.0929e-04\n",
      "Epoch 372/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 9.7837e-05 - mae: 0.0065 - mse: 9.7837e-05 - val_loss: 3.0500e-04 - val_mae: 0.0110 - val_mse: 3.0500e-04\n",
      "Epoch 373/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 9.8050e-05 - mae: 0.0064 - mse: 9.8050e-05 - val_loss: 3.0014e-04 - val_mae: 0.0109 - val_mse: 3.0014e-04\n",
      "Epoch 374/500\n",
      "2440/2440 [==============================] - 1s 265us/step - loss: 9.7323e-05 - mae: 0.0065 - mse: 9.7323e-05 - val_loss: 3.0030e-04 - val_mae: 0.0109 - val_mse: 3.0030e-04\n",
      "Epoch 375/500\n",
      "2440/2440 [==============================] - 1s 268us/step - loss: 9.8149e-05 - mae: 0.0066 - mse: 9.8149e-05 - val_loss: 3.0296e-04 - val_mae: 0.0110 - val_mse: 3.0296e-04\n",
      "Epoch 376/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 9.8589e-05 - mae: 0.0067 - mse: 9.8589e-05 - val_loss: 3.0549e-04 - val_mae: 0.0111 - val_mse: 3.0549e-04\n",
      "Epoch 377/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 9.8742e-05 - mae: 0.0067 - mse: 9.8742e-05 - val_loss: 3.0816e-04 - val_mae: 0.0110 - val_mse: 3.0816e-04\n",
      "Epoch 378/500\n",
      "2440/2440 [==============================] - 1s 302us/step - loss: 9.8913e-05 - mae: 0.0067 - mse: 9.8913e-05 - val_loss: 3.1212e-04 - val_mae: 0.0110 - val_mse: 3.1212e-04\n",
      "Epoch 379/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.0424e-04 - mae: 0.0069 - mse: 1.0424e-04 - val_loss: 3.1474e-04 - val_mae: 0.0112 - val_mse: 3.1474e-04\n",
      "Epoch 380/500\n",
      "2440/2440 [==============================] - 1s 271us/step - loss: 1.1417e-04 - mae: 0.0072 - mse: 1.1417e-04 - val_loss: 3.3916e-04 - val_mae: 0.0113 - val_mse: 3.3916e-04\n",
      "Epoch 381/500\n",
      "2440/2440 [==============================] - 1s 264us/step - loss: 1.3496e-04 - mae: 0.0082 - mse: 1.3496e-04 - val_loss: 3.0380e-04 - val_mae: 0.0113 - val_mse: 3.0380e-04\n",
      "Epoch 382/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.2056e-04 - mae: 0.0073 - mse: 1.2056e-04 - val_loss: 2.7340e-04 - val_mae: 0.0092 - val_mse: 2.7340e-04\n",
      "Epoch 383/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.1508e-04 - mae: 0.0074 - mse: 1.1508e-04 - val_loss: 3.4217e-04 - val_mae: 0.0114 - val_mse: 3.4217e-04\n",
      "Epoch 384/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.1877e-04 - mae: 0.0077 - mse: 1.1877e-04 - val_loss: 3.1250e-04 - val_mae: 0.0105 - val_mse: 3.1250e-04\n",
      "Epoch 385/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.1229e-04 - mae: 0.0073 - mse: 1.1229e-04 - val_loss: 3.0224e-04 - val_mae: 0.0095 - val_mse: 3.0224e-04\n",
      "Epoch 386/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.0713e-04 - mae: 0.0072 - mse: 1.0713e-04 - val_loss: 3.0126e-04 - val_mae: 0.0098 - val_mse: 3.0126e-04\n",
      "Epoch 387/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 1.0828e-04 - mae: 0.0072 - mse: 1.0828e-04 - val_loss: 3.0368e-04 - val_mae: 0.0100 - val_mse: 3.0368e-04\n",
      "Epoch 388/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0499e-04 - mae: 0.0070 - mse: 1.0499e-04 - val_loss: 2.9740e-04 - val_mae: 0.0101 - val_mse: 2.9740e-04\n",
      "Epoch 389/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.0578e-04 - mae: 0.0071 - mse: 1.0578e-04 - val_loss: 2.9864e-04 - val_mae: 0.0098 - val_mse: 2.9864e-04\n",
      "Epoch 390/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.0274e-04 - mae: 0.0070 - mse: 1.0274e-04 - val_loss: 3.0229e-04 - val_mae: 0.0101 - val_mse: 3.0229e-04\n",
      "Epoch 391/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0361e-04 - mae: 0.0071 - mse: 1.0361e-04 - val_loss: 3.0646e-04 - val_mae: 0.0102 - val_mse: 3.0646e-04\n",
      "Epoch 392/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.0291e-04 - mae: 0.0070 - mse: 1.0291e-04 - val_loss: 3.0586e-04 - val_mae: 0.0101 - val_mse: 3.0586e-04\n",
      "Epoch 393/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.0135e-04 - mae: 0.0070 - mse: 1.0135e-04 - val_loss: 3.0555e-04 - val_mae: 0.0099 - val_mse: 3.0555e-04\n",
      "Epoch 394/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 9.9408e-05 - mae: 0.0069 - mse: 9.9408e-05 - val_loss: 3.0660e-04 - val_mae: 0.0099 - val_mse: 3.0660e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/500\n",
      "2440/2440 [==============================] - 1s 263us/step - loss: 9.8032e-05 - mae: 0.0068 - mse: 9.8032e-05 - val_loss: 3.0700e-04 - val_mae: 0.0097 - val_mse: 3.0700e-04\n",
      "Epoch 396/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 9.5973e-05 - mae: 0.0067 - mse: 9.5973e-05 - val_loss: 3.0652e-04 - val_mae: 0.0095 - val_mse: 3.0652e-04\n",
      "Epoch 397/500\n",
      "2440/2440 [==============================] - 1s 272us/step - loss: 9.4194e-05 - mae: 0.0066 - mse: 9.4194e-05 - val_loss: 3.0609e-04 - val_mae: 0.0093 - val_mse: 3.0609e-04\n",
      "Epoch 398/500\n",
      "2440/2440 [==============================] - 1s 268us/step - loss: 9.2076e-05 - mae: 0.0064 - mse: 9.2076e-05 - val_loss: 3.0584e-04 - val_mae: 0.0092 - val_mse: 3.0584e-04\n",
      "Epoch 399/500\n",
      "2440/2440 [==============================] - 1s 263us/step - loss: 9.0494e-05 - mae: 0.0063 - mse: 9.0494e-05 - val_loss: 3.0591e-04 - val_mae: 0.0092 - val_mse: 3.0591e-04\n",
      "Epoch 400/500\n",
      "2440/2440 [==============================] - 1s 264us/step - loss: 8.9274e-05 - mae: 0.0063 - mse: 8.9274e-05 - val_loss: 3.0673e-04 - val_mae: 0.0094 - val_mse: 3.0673e-04\n",
      "Epoch 401/500\n",
      "2440/2440 [==============================] - 1s 267us/step - loss: 8.9045e-05 - mae: 0.0063 - mse: 8.9045e-05 - val_loss: 3.0978e-04 - val_mae: 0.0096 - val_mse: 3.0978e-04\n",
      "Epoch 402/500\n",
      "2440/2440 [==============================] - 1s 261us/step - loss: 8.8663e-05 - mae: 0.0063 - mse: 8.8663e-05 - val_loss: 3.1506e-04 - val_mae: 0.0099 - val_mse: 3.1506e-04\n",
      "Epoch 403/500\n",
      "2440/2440 [==============================] - 1s 266us/step - loss: 8.9348e-05 - mae: 0.0064 - mse: 8.9348e-05 - val_loss: 3.3252e-04 - val_mae: 0.0103 - val_mse: 3.3252e-04\n",
      "Epoch 404/500\n",
      "2440/2440 [==============================] - 1s 265us/step - loss: 8.7274e-05 - mae: 0.0063 - mse: 8.7274e-05 - val_loss: 3.3191e-04 - val_mae: 0.0108 - val_mse: 3.3191e-04\n",
      "Epoch 405/500\n",
      "2440/2440 [==============================] - 1s 262us/step - loss: 9.4593e-05 - mae: 0.0066 - mse: 9.4593e-05 - val_loss: 3.1572e-04 - val_mae: 0.0099 - val_mse: 3.1572e-04\n",
      "Epoch 406/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.8958e-04 - mae: 0.0095 - mse: 1.8958e-04 - val_loss: 2.6623e-04 - val_mae: 0.0089 - val_mse: 2.6623e-04\n",
      "Epoch 407/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0841e-04 - mae: 0.0067 - mse: 1.0841e-04 - val_loss: 2.8487e-04 - val_mae: 0.0112 - val_mse: 2.8487e-04\n",
      "Epoch 408/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1312e-04 - mae: 0.0073 - mse: 1.1312e-04 - val_loss: 3.1248e-04 - val_mae: 0.0100 - val_mse: 3.1248e-04\n",
      "Epoch 409/500\n",
      "2440/2440 [==============================] - 1s 270us/step - loss: 1.0607e-04 - mae: 0.0071 - mse: 1.0607e-04 - val_loss: 2.9887e-04 - val_mae: 0.0097 - val_mse: 2.9887e-04\n",
      "Epoch 410/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 1.0528e-04 - mae: 0.0071 - mse: 1.0528e-04 - val_loss: 2.9698e-04 - val_mae: 0.0093 - val_mse: 2.9698e-04\n",
      "Epoch 411/500\n",
      "2440/2440 [==============================] - 1s 266us/step - loss: 9.2853e-05 - mae: 0.0066 - mse: 9.2853e-05 - val_loss: 3.1404e-04 - val_mae: 0.0101 - val_mse: 3.1404e-04\n",
      "Epoch 412/500\n",
      "2440/2440 [==============================] - 1s 271us/step - loss: 1.0201e-04 - mae: 0.0071 - mse: 1.0201e-04 - val_loss: 3.1871e-04 - val_mae: 0.0102 - val_mse: 3.1871e-04\n",
      "Epoch 413/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 9.8833e-05 - mae: 0.0070 - mse: 9.8833e-05 - val_loss: 3.2301e-04 - val_mae: 0.0101 - val_mse: 3.2301e-04\n",
      "Epoch 414/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.0052e-04 - mae: 0.0071 - mse: 1.0052e-04 - val_loss: 3.3450e-04 - val_mae: 0.0107 - val_mse: 3.3450e-04\n",
      "Epoch 415/500\n",
      "2440/2440 [==============================] - 1s 266us/step - loss: 1.0187e-04 - mae: 0.0071 - mse: 1.0187e-04 - val_loss: 3.3138e-04 - val_mae: 0.0105 - val_mse: 3.3138e-04\n",
      "Epoch 416/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 9.8840e-05 - mae: 0.0069 - mse: 9.8840e-05 - val_loss: 3.3615e-04 - val_mae: 0.0103 - val_mse: 3.3615e-04\n",
      "Epoch 417/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0092e-04 - mae: 0.0071 - mse: 1.0092e-04 - val_loss: 3.4784e-04 - val_mae: 0.0116 - val_mse: 3.4784e-04\n",
      "Epoch 418/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 9.9979e-05 - mae: 0.0070 - mse: 9.9979e-05 - val_loss: 3.1619e-04 - val_mae: 0.0095 - val_mse: 3.1619e-04\n",
      "Epoch 419/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 9.4722e-05 - mae: 0.0065 - mse: 9.4722e-05 - val_loss: 3.2443e-04 - val_mae: 0.0103 - val_mse: 3.2443e-04\n",
      "Epoch 420/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.0453e-04 - mae: 0.0070 - mse: 1.0453e-04 - val_loss: 3.7946e-04 - val_mae: 0.0139 - val_mse: 3.7946e-04\n",
      "Epoch 421/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0968e-04 - mae: 0.0074 - mse: 1.0968e-04 - val_loss: 2.9541e-04 - val_mae: 0.0091 - val_mse: 2.9541e-04\n",
      "Epoch 422/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 8.6053e-05 - mae: 0.0061 - mse: 8.6053e-05 - val_loss: 3.1029e-04 - val_mae: 0.0097 - val_mse: 3.1029e-04\n",
      "Epoch 423/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.7981e-05 - mae: 0.0063 - mse: 8.7981e-05 - val_loss: 3.2302e-04 - val_mae: 0.0097 - val_mse: 3.2302e-04\n",
      "Epoch 424/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 9.0452e-05 - mae: 0.0065 - mse: 9.0452e-05 - val_loss: 3.1922e-04 - val_mae: 0.0097 - val_mse: 3.1922e-04\n",
      "Epoch 425/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 9.0669e-05 - mae: 0.0064 - mse: 9.0669e-05 - val_loss: 3.2121e-04 - val_mae: 0.0099 - val_mse: 3.2121e-04\n",
      "Epoch 426/500\n",
      "2440/2440 [==============================] - 1s 271us/step - loss: 8.6024e-05 - mae: 0.0063 - mse: 8.6024e-05 - val_loss: 3.1925e-04 - val_mae: 0.0097 - val_mse: 3.1925e-04\n",
      "Epoch 427/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 1.0187e-04 - mae: 0.0070 - mse: 1.0187e-04 - val_loss: 3.3655e-04 - val_mae: 0.0124 - val_mse: 3.3655e-04\n",
      "Epoch 428/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.0962e-04 - mae: 0.0073 - mse: 1.0962e-04 - val_loss: 3.4150e-04 - val_mae: 0.0105 - val_mse: 3.4150e-04\n",
      "Epoch 429/500\n",
      "2440/2440 [==============================] - 1s 266us/step - loss: 9.4042e-05 - mae: 0.0066 - mse: 9.4042e-05 - val_loss: 3.2095e-04 - val_mae: 0.0098 - val_mse: 3.2095e-04\n",
      "Epoch 430/500\n",
      "2440/2440 [==============================] - 1s 265us/step - loss: 8.8162e-05 - mae: 0.0064 - mse: 8.8162e-05 - val_loss: 3.3512e-04 - val_mae: 0.0097 - val_mse: 3.3512e-04\n",
      "Epoch 431/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 8.5365e-05 - mae: 0.0062 - mse: 8.5365e-05 - val_loss: 3.2510e-04 - val_mae: 0.0096 - val_mse: 3.2510e-04\n",
      "Epoch 432/500\n",
      "2440/2440 [==============================] - 1s 267us/step - loss: 8.5610e-05 - mae: 0.0062 - mse: 8.5610e-05 - val_loss: 3.3921e-04 - val_mae: 0.0103 - val_mse: 3.3921e-04\n",
      "Epoch 433/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 8.8959e-05 - mae: 0.0065 - mse: 8.8959e-05 - val_loss: 3.6522e-04 - val_mae: 0.0119 - val_mse: 3.6522e-04\n",
      "Epoch 434/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.0370e-04 - mae: 0.0072 - mse: 1.0370e-04 - val_loss: 3.2361e-04 - val_mae: 0.0100 - val_mse: 3.2361e-04\n",
      "Epoch 435/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 1.0090e-04 - mae: 0.0068 - mse: 1.0090e-04 - val_loss: 3.7519e-04 - val_mae: 0.0140 - val_mse: 3.7519e-04\n",
      "Epoch 436/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 1.0643e-04 - mae: 0.0072 - mse: 1.0643e-04 - val_loss: 3.6452e-04 - val_mae: 0.0113 - val_mse: 3.6452e-04\n",
      "Epoch 437/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.0926e-04 - mae: 0.0073 - mse: 1.0926e-04 - val_loss: 3.0196e-04 - val_mae: 0.0095 - val_mse: 3.0196e-04\n",
      "Epoch 438/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 1.0295e-04 - mae: 0.0069 - mse: 1.0295e-04 - val_loss: 3.1769e-04 - val_mae: 0.0101 - val_mse: 3.1769e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 439/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 9.2455e-05 - mae: 0.0065 - mse: 9.2455e-05 - val_loss: 3.2848e-04 - val_mae: 0.0099 - val_mse: 3.2848e-04\n",
      "Epoch 440/500\n",
      "2440/2440 [==============================] - 1s 336us/step - loss: 9.3698e-05 - mae: 0.0066 - mse: 9.3698e-05 - val_loss: 3.2057e-04 - val_mae: 0.0098 - val_mse: 3.2057e-04\n",
      "Epoch 441/500\n",
      "2440/2440 [==============================] - 1s 294us/step - loss: 8.6396e-05 - mae: 0.0064 - mse: 8.6397e-05 - val_loss: 3.6014e-04 - val_mae: 0.0106 - val_mse: 3.6014e-04\n",
      "Epoch 442/500\n",
      "2440/2440 [==============================] - 1s 396us/step - loss: 9.6028e-05 - mae: 0.0069 - mse: 9.6028e-05 - val_loss: 3.4149e-04 - val_mae: 0.0108 - val_mse: 3.4149e-04\n",
      "Epoch 443/500\n",
      "2440/2440 [==============================] - 1s 329us/step - loss: 9.7392e-05 - mae: 0.0068 - mse: 9.7392e-05 - val_loss: 3.2756e-04 - val_mae: 0.0098 - val_mse: 3.2756e-04\n",
      "Epoch 444/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 8.4516e-05 - mae: 0.0063 - mse: 8.4516e-05 - val_loss: 3.3820e-04 - val_mae: 0.0103 - val_mse: 3.3820e-04\n",
      "Epoch 445/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 8.7394e-05 - mae: 0.0065 - mse: 8.7394e-05 - val_loss: 3.4104e-04 - val_mae: 0.0098 - val_mse: 3.4104e-04\n",
      "Epoch 446/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 8.4401e-05 - mae: 0.0063 - mse: 8.4401e-05 - val_loss: 3.3714e-04 - val_mae: 0.0098 - val_mse: 3.3714e-04\n",
      "Epoch 447/500\n",
      "2440/2440 [==============================] - 1s 309us/step - loss: 8.3060e-05 - mae: 0.0062 - mse: 8.3060e-05 - val_loss: 3.5634e-04 - val_mae: 0.0104 - val_mse: 3.5634e-04\n",
      "Epoch 448/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 8.0399e-05 - mae: 0.0061 - mse: 8.0399e-05 - val_loss: 3.4436e-04 - val_mae: 0.0100 - val_mse: 3.4436e-04\n",
      "Epoch 449/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 8.7105e-05 - mae: 0.0064 - mse: 8.7105e-05 - val_loss: 3.2402e-04 - val_mae: 0.0097 - val_mse: 3.2402e-04\n",
      "Epoch 450/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 8.8344e-05 - mae: 0.0064 - mse: 8.8344e-05 - val_loss: 3.4983e-04 - val_mae: 0.0107 - val_mse: 3.4983e-04\n",
      "Epoch 451/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 8.0268e-05 - mae: 0.0060 - mse: 8.0268e-05 - val_loss: 3.5031e-04 - val_mae: 0.0102 - val_mse: 3.5031e-04\n",
      "Epoch 452/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 8.3270e-05 - mae: 0.0062 - mse: 8.3270e-05 - val_loss: 3.6342e-04 - val_mae: 0.0104 - val_mse: 3.6342e-04\n",
      "Epoch 453/500\n",
      "2440/2440 [==============================] - 1s 301us/step - loss: 8.1695e-05 - mae: 0.0062 - mse: 8.1695e-05 - val_loss: 3.4067e-04 - val_mae: 0.0099 - val_mse: 3.4067e-04\n",
      "Epoch 454/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 8.5494e-05 - mae: 0.0063 - mse: 8.5494e-05 - val_loss: 3.4360e-04 - val_mae: 0.0102 - val_mse: 3.4360e-04\n",
      "Epoch 455/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 8.6492e-05 - mae: 0.0064 - mse: 8.6492e-05 - val_loss: 3.7506e-04 - val_mae: 0.0108 - val_mse: 3.7506e-04\n",
      "Epoch 456/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 9.3412e-05 - mae: 0.0069 - mse: 9.3412e-05 - val_loss: 3.5383e-04 - val_mae: 0.0104 - val_mse: 3.5383e-04\n",
      "Epoch 457/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 8.5227e-05 - mae: 0.0062 - mse: 8.5227e-05 - val_loss: 3.7033e-04 - val_mae: 0.0106 - val_mse: 3.7033e-04\n",
      "Epoch 458/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0048e-04 - mae: 0.0070 - mse: 1.0048e-04 - val_loss: 3.6126e-04 - val_mae: 0.0119 - val_mse: 3.6126e-04\n",
      "Epoch 459/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 9.3792e-05 - mae: 0.0067 - mse: 9.3792e-05 - val_loss: 3.7497e-04 - val_mae: 0.0109 - val_mse: 3.7497e-04\n",
      "Epoch 460/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.0258e-04 - mae: 0.0071 - mse: 1.0258e-04 - val_loss: 3.4540e-04 - val_mae: 0.0104 - val_mse: 3.4540e-04\n",
      "Epoch 461/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.9412e-05 - mae: 0.0065 - mse: 8.9412e-05 - val_loss: 3.7863e-04 - val_mae: 0.0109 - val_mse: 3.7863e-04\n",
      "Epoch 462/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.0240e-04 - mae: 0.0073 - mse: 1.0240e-04 - val_loss: 3.8303e-04 - val_mae: 0.0121 - val_mse: 3.8303e-04\n",
      "Epoch 463/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.1524e-04 - mae: 0.0077 - mse: 1.1524e-04 - val_loss: 3.7808e-04 - val_mae: 0.0113 - val_mse: 3.7808e-04\n",
      "Epoch 464/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.0019e-04 - mae: 0.0072 - mse: 1.0019e-04 - val_loss: 3.6873e-04 - val_mae: 0.0128 - val_mse: 3.6873e-04\n",
      "Epoch 465/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.0431e-04 - mae: 0.0072 - mse: 1.0431e-04 - val_loss: 3.1414e-04 - val_mae: 0.0098 - val_mse: 3.1414e-04\n",
      "Epoch 466/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 8.7681e-05 - mae: 0.0065 - mse: 8.7681e-05 - val_loss: 3.2373e-04 - val_mae: 0.0096 - val_mse: 3.2373e-04\n",
      "Epoch 467/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 8.6994e-05 - mae: 0.0065 - mse: 8.6994e-05 - val_loss: 3.2401e-04 - val_mae: 0.0095 - val_mse: 3.2401e-04\n",
      "Epoch 468/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.2868e-05 - mae: 0.0063 - mse: 8.2868e-05 - val_loss: 3.4815e-04 - val_mae: 0.0105 - val_mse: 3.4815e-04\n",
      "Epoch 469/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 8.2463e-05 - mae: 0.0064 - mse: 8.2463e-05 - val_loss: 3.5024e-04 - val_mae: 0.0104 - val_mse: 3.5024e-04\n",
      "Epoch 470/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 7.9581e-05 - mae: 0.0061 - mse: 7.9581e-05 - val_loss: 3.3110e-04 - val_mae: 0.0098 - val_mse: 3.3110e-04\n",
      "Epoch 471/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 8.0297e-05 - mae: 0.0061 - mse: 8.0297e-05 - val_loss: 3.5611e-04 - val_mae: 0.0103 - val_mse: 3.5611e-04\n",
      "Epoch 472/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 8.1905e-05 - mae: 0.0062 - mse: 8.1905e-05 - val_loss: 3.7079e-04 - val_mae: 0.0107 - val_mse: 3.7079e-04\n",
      "Epoch 473/500\n",
      "2440/2440 [==============================] - 1s 309us/step - loss: 8.4462e-05 - mae: 0.0062 - mse: 8.4462e-05 - val_loss: 3.6015e-04 - val_mae: 0.0110 - val_mse: 3.6015e-04\n",
      "Epoch 474/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 7.4708e-05 - mae: 0.0058 - mse: 7.4708e-05 - val_loss: 3.5892e-04 - val_mae: 0.0103 - val_mse: 3.5892e-04\n",
      "Epoch 475/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 8.7036e-05 - mae: 0.0063 - mse: 8.7036e-05 - val_loss: 3.4666e-04 - val_mae: 0.0105 - val_mse: 3.4666e-04\n",
      "Epoch 476/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 8.2759e-05 - mae: 0.0063 - mse: 8.2759e-05 - val_loss: 3.9316e-04 - val_mae: 0.0108 - val_mse: 3.9316e-04\n",
      "Epoch 477/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 8.1118e-05 - mae: 0.0064 - mse: 8.1118e-05 - val_loss: 3.8428e-04 - val_mae: 0.0106 - val_mse: 3.8428e-04\n",
      "Epoch 478/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 8.1839e-05 - mae: 0.0063 - mse: 8.1839e-05 - val_loss: 3.7808e-04 - val_mae: 0.0111 - val_mse: 3.7808e-04\n",
      "Epoch 479/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 7.6324e-05 - mae: 0.0061 - mse: 7.6324e-05 - val_loss: 3.9988e-04 - val_mae: 0.0116 - val_mse: 3.9988e-04\n",
      "Epoch 480/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 7.9956e-05 - mae: 0.0064 - mse: 7.9956e-05 - val_loss: 3.5380e-04 - val_mae: 0.0101 - val_mse: 3.5380e-04\n",
      "Epoch 481/500\n",
      "2440/2440 [==============================] - 1s 292us/step - loss: 8.6935e-05 - mae: 0.0064 - mse: 8.6935e-05 - val_loss: 3.9434e-04 - val_mae: 0.0110 - val_mse: 3.9434e-04\n",
      "Epoch 482/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 7.8094e-05 - mae: 0.0062 - mse: 7.8094e-05 - val_loss: 3.8708e-04 - val_mae: 0.0108 - val_mse: 3.8708e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 483/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 7.5337e-05 - mae: 0.0061 - mse: 7.5337e-05 - val_loss: 3.8486e-04 - val_mae: 0.0108 - val_mse: 3.8486e-04\n",
      "Epoch 484/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 7.6622e-05 - mae: 0.0061 - mse: 7.6622e-05 - val_loss: 4.0465e-04 - val_mae: 0.0112 - val_mse: 4.0465e-04\n",
      "Epoch 485/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 8.3597e-05 - mae: 0.0065 - mse: 8.3597e-05 - val_loss: 4.0550e-04 - val_mae: 0.0113 - val_mse: 4.0550e-04\n",
      "Epoch 486/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 9.0490e-05 - mae: 0.0068 - mse: 9.0490e-05 - val_loss: 4.5308e-04 - val_mae: 0.0116 - val_mse: 4.5308e-04\n",
      "Epoch 487/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 8.2534e-05 - mae: 0.0065 - mse: 8.2534e-05 - val_loss: 3.9571e-04 - val_mae: 0.0107 - val_mse: 3.9571e-04\n",
      "Epoch 488/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 9.3862e-05 - mae: 0.0068 - mse: 9.3862e-05 - val_loss: 3.0759e-04 - val_mae: 0.0097 - val_mse: 3.0759e-04\n",
      "Epoch 489/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 8.4868e-05 - mae: 0.0063 - mse: 8.4868e-05 - val_loss: 4.1959e-04 - val_mae: 0.0124 - val_mse: 4.1959e-04\n",
      "Epoch 490/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 7.9558e-05 - mae: 0.0063 - mse: 7.9558e-05 - val_loss: 3.8335e-04 - val_mae: 0.0107 - val_mse: 3.8335e-04\n",
      "Epoch 491/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 9.2131e-05 - mae: 0.0069 - mse: 9.2131e-05 - val_loss: 3.9333e-04 - val_mae: 0.0119 - val_mse: 3.9333e-04\n",
      "Epoch 492/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 8.9919e-05 - mae: 0.0067 - mse: 8.9919e-05 - val_loss: 4.1590e-04 - val_mae: 0.0117 - val_mse: 4.1590e-04\n",
      "Epoch 493/500\n",
      "2440/2440 [==============================] - 1s 321us/step - loss: 7.9306e-05 - mae: 0.0065 - mse: 7.9306e-05 - val_loss: 3.8598e-04 - val_mae: 0.0108 - val_mse: 3.8598e-04\n",
      "Epoch 494/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.0191e-04 - mae: 0.0072 - mse: 1.0191e-04 - val_loss: 3.6669e-04 - val_mae: 0.0115 - val_mse: 3.6669e-04\n",
      "Epoch 495/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.3860e-05 - mae: 0.0065 - mse: 8.3860e-05 - val_loss: 4.1808e-04 - val_mae: 0.0116 - val_mse: 4.1808e-04\n",
      "Epoch 496/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 9.1763e-05 - mae: 0.0070 - mse: 9.1763e-05 - val_loss: 3.6716e-04 - val_mae: 0.0104 - val_mse: 3.6716e-04\n",
      "Epoch 497/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.3840e-05 - mae: 0.0065 - mse: 8.3840e-05 - val_loss: 3.7311e-04 - val_mae: 0.0116 - val_mse: 3.7311e-04\n",
      "Epoch 498/500\n",
      "2440/2440 [==============================] - 1s 317us/step - loss: 7.9396e-05 - mae: 0.0063 - mse: 7.9396e-05 - val_loss: 4.1868e-04 - val_mae: 0.0119 - val_mse: 4.1868e-04\n",
      "Epoch 499/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 8.2509e-05 - mae: 0.0067 - mse: 8.2509e-05 - val_loss: 3.8194e-04 - val_mae: 0.0107 - val_mse: 3.8194e-04\n",
      "Epoch 500/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 8.6927e-05 - mae: 0.0067 - mse: 8.6927e-05 - val_loss: 4.0472e-04 - val_mae: 0.0117 - val_mse: 4.0472e-04\n"
     ]
    }
   ],
   "source": [
    "# Fit LSTM\n",
    "lstmhistory = lstm.fit(X_train_LSTM, y_train_LSTM,\n",
    "                       epochs=EPOCHS,\n",
    "                       validation_data=(X_test_LSTM, y_test_LSTM),\n",
    "                       verbose=1,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fb5a0e9b70>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph LSTM Error, mae and mse\n",
    "plt.plot(lstmhistory.history['loss'])\n",
    "plt.plot(lstmhistory.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Model WITHOUT Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4222 samples, validate on 1056 samples\n",
      "Epoch 1/500\n",
      "4222/4222 [==============================] - 1s 323us/step - loss: 0.0301 - mae: 0.1085 - mse: 0.0301 - val_loss: 0.0233 - val_mae: 0.0937 - val_mse: 0.0233\n",
      "Epoch 2/500\n",
      "4222/4222 [==============================] - 1s 253us/step - loss: 0.0224 - mae: 0.0810 - mse: 0.0224 - val_loss: 0.0199 - val_mae: 0.0780 - val_mse: 0.0199\n",
      "Epoch 3/500\n",
      "4222/4222 [==============================] - 1s 251us/step - loss: 0.0197 - mae: 0.0703 - mse: 0.0197 - val_loss: 0.0184 - val_mae: 0.0735 - val_mse: 0.0184\n",
      "Epoch 4/500\n",
      "4222/4222 [==============================] - 1s 246us/step - loss: 0.0183 - mae: 0.0664 - mse: 0.0183 - val_loss: 0.0170 - val_mae: 0.0720 - val_mse: 0.0170\n",
      "Epoch 5/500\n",
      "4222/4222 [==============================] - 1s 260us/step - loss: 0.0167 - mae: 0.0632 - mse: 0.0167 - val_loss: 0.0149 - val_mae: 0.0651 - val_mse: 0.0149\n",
      "Epoch 6/500\n",
      "4222/4222 [==============================] - 1s 246us/step - loss: 0.0156 - mae: 0.0620 - mse: 0.0156 - val_loss: 0.0135 - val_mae: 0.0595 - val_mse: 0.0135\n",
      "Epoch 7/500\n",
      "4222/4222 [==============================] - 1s 249us/step - loss: 0.0149 - mae: 0.0599 - mse: 0.0149 - val_loss: 0.0130 - val_mae: 0.0577 - val_mse: 0.0130\n",
      "Epoch 8/500\n",
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0147 - mae: 0.0592 - mse: 0.0147 - val_loss: 0.0126 - val_mae: 0.0579 - val_mse: 0.0126\n",
      "Epoch 9/500\n",
      "4222/4222 [==============================] - 1s 259us/step - loss: 0.0144 - mae: 0.0586 - mse: 0.0144 - val_loss: 0.0125 - val_mae: 0.0590 - val_mse: 0.0125\n",
      "Epoch 10/500\n",
      "4222/4222 [==============================] - 1s 256us/step - loss: 0.0140 - mae: 0.0578 - mse: 0.0140 - val_loss: 0.0122 - val_mae: 0.0590 - val_mse: 0.0122\n",
      "Epoch 11/500\n",
      "4222/4222 [==============================] - 1s 257us/step - loss: 0.0137 - mae: 0.0568 - mse: 0.0137 - val_loss: 0.0119 - val_mae: 0.0564 - val_mse: 0.0119\n",
      "Epoch 12/500\n",
      "4222/4222 [==============================] - 1s 261us/step - loss: 0.0134 - mae: 0.0557 - mse: 0.0134 - val_loss: 0.0115 - val_mae: 0.0528 - val_mse: 0.0115\n",
      "Epoch 13/500\n",
      "4222/4222 [==============================] - 1s 246us/step - loss: 0.0131 - mae: 0.0537 - mse: 0.0131 - val_loss: 0.0113 - val_mae: 0.0527 - val_mse: 0.0113\n",
      "Epoch 14/500\n",
      "4222/4222 [==============================] - 1s 260us/step - loss: 0.0129 - mae: 0.0536 - mse: 0.0129 - val_loss: 0.0111 - val_mae: 0.0513 - val_mse: 0.0111\n",
      "Epoch 15/500\n",
      "4222/4222 [==============================] - 1s 260us/step - loss: 0.0127 - mae: 0.0527 - mse: 0.0127 - val_loss: 0.0109 - val_mae: 0.0535 - val_mse: 0.0109\n",
      "Epoch 16/500\n",
      "4222/4222 [==============================] - 1s 255us/step - loss: 0.0126 - mae: 0.0519 - mse: 0.0126 - val_loss: 0.0107 - val_mae: 0.0512 - val_mse: 0.0107\n",
      "Epoch 17/500\n",
      "4222/4222 [==============================] - 1s 255us/step - loss: 0.0124 - mae: 0.0518 - mse: 0.0124 - val_loss: 0.0106 - val_mae: 0.0503 - val_mse: 0.0106\n",
      "Epoch 18/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0123 - mae: 0.0512 - mse: 0.0123 - val_loss: 0.0104 - val_mae: 0.0499 - val_mse: 0.0104\n",
      "Epoch 19/500\n",
      "4222/4222 [==============================] - 1s 274us/step - loss: 0.0121 - mae: 0.0507 - mse: 0.0121 - val_loss: 0.0103 - val_mae: 0.0504 - val_mse: 0.0103\n",
      "Epoch 20/500\n",
      "4222/4222 [==============================] - 1s 244us/step - loss: 0.0119 - mae: 0.0504 - mse: 0.0119 - val_loss: 0.0102 - val_mae: 0.0500 - val_mse: 0.0102\n",
      "Epoch 21/500\n",
      "4222/4222 [==============================] - 1s 248us/step - loss: 0.0118 - mae: 0.0492 - mse: 0.0118 - val_loss: 0.0101 - val_mae: 0.0495 - val_mse: 0.0101\n",
      "Epoch 22/500\n",
      "4222/4222 [==============================] - 1s 251us/step - loss: 0.0116 - mae: 0.0485 - mse: 0.0116 - val_loss: 0.0099 - val_mae: 0.0470 - val_mse: 0.0099\n",
      "Epoch 23/500\n",
      "4222/4222 [==============================] - 1s 247us/step - loss: 0.0114 - mae: 0.0486 - mse: 0.0114 - val_loss: 0.0097 - val_mae: 0.0470 - val_mse: 0.0097\n",
      "Epoch 24/500\n",
      "4222/4222 [==============================] - 1s 248us/step - loss: 0.0112 - mae: 0.0490 - mse: 0.0112 - val_loss: 0.0096 - val_mae: 0.0483 - val_mse: 0.0096\n",
      "Epoch 25/500\n",
      "4222/4222 [==============================] - 1s 271us/step - loss: 0.0111 - mae: 0.0488 - mse: 0.0111 - val_loss: 0.0094 - val_mae: 0.0465 - val_mse: 0.0094\n",
      "Epoch 26/500\n",
      "4222/4222 [==============================] - 1s 254us/step - loss: 0.0109 - mae: 0.0478 - mse: 0.0109 - val_loss: 0.0093 - val_mae: 0.0461 - val_mse: 0.0093\n",
      "Epoch 27/500\n",
      "4222/4222 [==============================] - 1s 247us/step - loss: 0.0107 - mae: 0.0472 - mse: 0.0107 - val_loss: 0.0091 - val_mae: 0.0461 - val_mse: 0.0091\n",
      "Epoch 28/500\n",
      "4222/4222 [==============================] - 1s 249us/step - loss: 0.0106 - mae: 0.0464 - mse: 0.0106 - val_loss: 0.0090 - val_mae: 0.0459 - val_mse: 0.0090\n",
      "Epoch 29/500\n",
      "4222/4222 [==============================] - 1s 259us/step - loss: 0.0103 - mae: 0.0455 - mse: 0.0103 - val_loss: 0.0088 - val_mae: 0.0455 - val_mse: 0.0088\n",
      "Epoch 30/500\n",
      "4222/4222 [==============================] - 1s 268us/step - loss: 0.0101 - mae: 0.0445 - mse: 0.0101 - val_loss: 0.0086 - val_mae: 0.0450 - val_mse: 0.0086\n",
      "Epoch 31/500\n",
      "4222/4222 [==============================] - 1s 277us/step - loss: 0.0099 - mae: 0.0441 - mse: 0.0099 - val_loss: 0.0085 - val_mae: 0.0445 - val_mse: 0.0085\n",
      "Epoch 32/500\n",
      "4222/4222 [==============================] - 1s 309us/step - loss: 0.0098 - mae: 0.0442 - mse: 0.0098 - val_loss: 0.0084 - val_mae: 0.0441 - val_mse: 0.0084\n",
      "Epoch 33/500\n",
      "4222/4222 [==============================] - 1s 297us/step - loss: 0.0096 - mae: 0.0441 - mse: 0.0096 - val_loss: 0.0083 - val_mae: 0.0436 - val_mse: 0.0083\n",
      "Epoch 34/500\n",
      "4222/4222 [==============================] - 1s 278us/step - loss: 0.0095 - mae: 0.0440 - mse: 0.0095 - val_loss: 0.0082 - val_mae: 0.0438 - val_mse: 0.0082\n",
      "Epoch 35/500\n",
      "4222/4222 [==============================] - 1s 278us/step - loss: 0.0093 - mae: 0.0439 - mse: 0.0093 - val_loss: 0.0082 - val_mae: 0.0446 - val_mse: 0.0082\n",
      "Epoch 36/500\n",
      "4222/4222 [==============================] - 1s 296us/step - loss: 0.0091 - mae: 0.0436 - mse: 0.0091 - val_loss: 0.0082 - val_mae: 0.0452 - val_mse: 0.0082\n",
      "Epoch 37/500\n",
      "4222/4222 [==============================] - 1s 310us/step - loss: 0.0089 - mae: 0.0432 - mse: 0.0089 - val_loss: 0.0082 - val_mae: 0.0466 - val_mse: 0.0082\n",
      "Epoch 38/500\n",
      "4222/4222 [==============================] - 1s 323us/step - loss: 0.0086 - mae: 0.0426 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0473 - val_mse: 0.0083\n",
      "Epoch 39/500\n",
      "4222/4222 [==============================] - 1s 304us/step - loss: 0.0082 - mae: 0.0422 - mse: 0.0082 - val_loss: 0.0083 - val_mae: 0.0475 - val_mse: 0.0083\n",
      "Epoch 40/500\n",
      "4222/4222 [==============================] - 1s 322us/step - loss: 0.0080 - mae: 0.0416 - mse: 0.0080 - val_loss: 0.0082 - val_mae: 0.0468 - val_mse: 0.0082\n",
      "Epoch 41/500\n",
      "4222/4222 [==============================] - 1s 302us/step - loss: 0.0077 - mae: 0.0406 - mse: 0.0077 - val_loss: 0.0079 - val_mae: 0.0443 - val_mse: 0.0079\n",
      "Epoch 42/500\n",
      "4222/4222 [==============================] - 1s 292us/step - loss: 0.0073 - mae: 0.0395 - mse: 0.0073 - val_loss: 0.0076 - val_mae: 0.0421 - val_mse: 0.0076\n",
      "Epoch 43/500\n",
      "4222/4222 [==============================] - 1s 311us/step - loss: 0.0069 - mae: 0.0378 - mse: 0.0069 - val_loss: 0.0072 - val_mae: 0.0405 - val_mse: 0.0072\n",
      "Epoch 44/500\n",
      "4222/4222 [==============================] - 1s 325us/step - loss: 0.0065 - mae: 0.0367 - mse: 0.0065 - val_loss: 0.0072 - val_mae: 0.0399 - val_mse: 0.0072\n",
      "Epoch 45/500\n",
      "4222/4222 [==============================] - 1s 305us/step - loss: 0.0063 - mae: 0.0359 - mse: 0.0063 - val_loss: 0.0071 - val_mae: 0.0390 - val_mse: 0.0071\n",
      "Epoch 46/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0060 - mae: 0.0351 - mse: 0.0060 - val_loss: 0.0072 - val_mae: 0.0392 - val_mse: 0.0072\n",
      "Epoch 47/500\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0057 - mae: 0.0343 - mse: 0.0057 - val_loss: 0.0075 - val_mae: 0.0404 - val_mse: 0.0075\n",
      "Epoch 48/500\n",
      "4222/4222 [==============================] - 1s 304us/step - loss: 0.0056 - mae: 0.0340 - mse: 0.0056 - val_loss: 0.0073 - val_mae: 0.0393 - val_mse: 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0052 - mae: 0.0329 - mse: 0.0052 - val_loss: 0.0080 - val_mae: 0.0418 - val_mse: 0.0080\n",
      "Epoch 50/500\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0052 - mae: 0.0329 - mse: 0.0052 - val_loss: 0.0074 - val_mae: 0.0399 - val_mse: 0.0074\n",
      "Epoch 51/500\n",
      "4222/4222 [==============================] - 1s 274us/step - loss: 0.0048 - mae: 0.0317 - mse: 0.0048 - val_loss: 0.0079 - val_mae: 0.0411 - val_mse: 0.0079\n",
      "Epoch 52/500\n",
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0047 - mae: 0.0313 - mse: 0.0047 - val_loss: 0.0077 - val_mae: 0.0400 - val_mse: 0.0077\n",
      "Epoch 53/500\n",
      "4222/4222 [==============================] - 1s 309us/step - loss: 0.0044 - mae: 0.0304 - mse: 0.0044 - val_loss: 0.0076 - val_mae: 0.0394 - val_mse: 0.0076\n",
      "Epoch 54/500\n",
      "4222/4222 [==============================] - 1s 279us/step - loss: 0.0042 - mae: 0.0297 - mse: 0.0042 - val_loss: 0.0073 - val_mae: 0.0383 - val_mse: 0.0073\n",
      "Epoch 55/500\n",
      "4222/4222 [==============================] - 1s 319us/step - loss: 0.0041 - mae: 0.0294 - mse: 0.0041 - val_loss: 0.0066 - val_mae: 0.0358 - val_mse: 0.0066\n",
      "Epoch 56/500\n",
      "4222/4222 [==============================] - 1s 311us/step - loss: 0.0041 - mae: 0.0295 - mse: 0.0041 - val_loss: 0.0053 - val_mae: 0.0321 - val_mse: 0.0053\n",
      "Epoch 57/500\n",
      "4222/4222 [==============================] - 1s 295us/step - loss: 0.0042 - mae: 0.0302 - mse: 0.0042 - val_loss: 0.0047 - val_mae: 0.0336 - val_mse: 0.0047\n",
      "Epoch 58/500\n",
      "4222/4222 [==============================] - 1s 285us/step - loss: 0.0050 - mae: 0.0334 - mse: 0.0050 - val_loss: 0.0049 - val_mae: 0.0333 - val_mse: 0.0049\n",
      "Epoch 59/500\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0040 - mae: 0.0290 - mse: 0.0040 - val_loss: 0.0047 - val_mae: 0.0339 - val_mse: 0.0047\n",
      "Epoch 60/500\n",
      "4222/4222 [==============================] - 1s 281us/step - loss: 0.0035 - mae: 0.0274 - mse: 0.0035 - val_loss: 0.0049 - val_mae: 0.0349 - val_mse: 0.0049\n",
      "Epoch 61/500\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0034 - mae: 0.0272 - mse: 0.0034 - val_loss: 0.0049 - val_mae: 0.0346 - val_mse: 0.0049\n",
      "Epoch 62/500\n",
      "4222/4222 [==============================] - 1s 274us/step - loss: 0.0033 - mae: 0.0267 - mse: 0.0033 - val_loss: 0.0048 - val_mae: 0.0340 - val_mse: 0.0048\n",
      "Epoch 63/500\n",
      "4222/4222 [==============================] - 1s 279us/step - loss: 0.0033 - mae: 0.0263 - mse: 0.0033 - val_loss: 0.0046 - val_mae: 0.0333 - val_mse: 0.0046\n",
      "Epoch 64/500\n",
      "4222/4222 [==============================] - 1s 279us/step - loss: 0.0032 - mae: 0.0258 - mse: 0.0032 - val_loss: 0.0045 - val_mae: 0.0328 - val_mse: 0.0045\n",
      "Epoch 65/500\n",
      "4222/4222 [==============================] - 1s 295us/step - loss: 0.0031 - mae: 0.0253 - mse: 0.0031 - val_loss: 0.0044 - val_mae: 0.0321 - val_mse: 0.0044\n",
      "Epoch 66/500\n",
      "4222/4222 [==============================] - 1s 297us/step - loss: 0.0031 - mae: 0.0250 - mse: 0.0031 - val_loss: 0.0043 - val_mae: 0.0315 - val_mse: 0.0043\n",
      "Epoch 67/500\n",
      "4222/4222 [==============================] - 1s 304us/step - loss: 0.0030 - mae: 0.0245 - mse: 0.0030 - val_loss: 0.0041 - val_mae: 0.0302 - val_mse: 0.0041\n",
      "Epoch 68/500\n",
      "4222/4222 [==============================] - 1s 307us/step - loss: 0.0029 - mae: 0.0243 - mse: 0.0029 - val_loss: 0.0040 - val_mae: 0.0293 - val_mse: 0.0040\n",
      "Epoch 69/500\n",
      "4222/4222 [==============================] - 1s 279us/step - loss: 0.0029 - mae: 0.0238 - mse: 0.0029 - val_loss: 0.0037 - val_mae: 0.0275 - val_mse: 0.0037\n",
      "Epoch 70/500\n",
      "4222/4222 [==============================] - 1s 281us/step - loss: 0.0029 - mae: 0.0243 - mse: 0.0029 - val_loss: 0.0038 - val_mae: 0.0274 - val_mse: 0.0038\n",
      "Epoch 71/500\n",
      "4222/4222 [==============================] - 1s 298us/step - loss: 0.0027 - mae: 0.0233 - mse: 0.0027 - val_loss: 0.0033 - val_mae: 0.0261 - val_mse: 0.0033\n",
      "Epoch 72/500\n",
      "4222/4222 [==============================] - 1s 299us/step - loss: 0.0029 - mae: 0.0242 - mse: 0.0029 - val_loss: 0.0036 - val_mae: 0.0267 - val_mse: 0.0036\n",
      "Epoch 73/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0027 - mae: 0.0228 - mse: 0.0027 - val_loss: 0.0030 - val_mae: 0.0259 - val_mse: 0.0030\n",
      "Epoch 74/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0027 - mae: 0.0237 - mse: 0.0027 - val_loss: 0.0034 - val_mae: 0.0267 - val_mse: 0.0034\n",
      "Epoch 75/500\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0026 - mae: 0.0223 - mse: 0.0026 - val_loss: 0.0029 - val_mae: 0.0261 - val_mse: 0.0029\n",
      "Epoch 76/500\n",
      "4222/4222 [==============================] - 1s 284us/step - loss: 0.0026 - mae: 0.0229 - mse: 0.0026 - val_loss: 0.0033 - val_mae: 0.0265 - val_mse: 0.0033\n",
      "Epoch 77/500\n",
      "4222/4222 [==============================] - 1s 295us/step - loss: 0.0025 - mae: 0.0218 - mse: 0.0025 - val_loss: 0.0029 - val_mae: 0.0257 - val_mse: 0.0029\n",
      "Epoch 78/500\n",
      "4222/4222 [==============================] - 1s 303us/step - loss: 0.0025 - mae: 0.0224 - mse: 0.0025 - val_loss: 0.0032 - val_mae: 0.0262 - val_mse: 0.0032\n",
      "Epoch 79/500\n",
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0024 - mae: 0.0213 - mse: 0.0024 - val_loss: 0.0028 - val_mae: 0.0252 - val_mse: 0.0028\n",
      "Epoch 80/500\n",
      "4222/4222 [==============================] - 1s 288us/step - loss: 0.0024 - mae: 0.0217 - mse: 0.0024 - val_loss: 0.0031 - val_mae: 0.0255 - val_mse: 0.0031\n",
      "Epoch 81/500\n",
      "4222/4222 [==============================] - 1s 302us/step - loss: 0.0023 - mae: 0.0207 - mse: 0.0023 - val_loss: 0.0028 - val_mae: 0.0247 - val_mse: 0.0028\n",
      "Epoch 82/500\n",
      "4222/4222 [==============================] - 1s 300us/step - loss: 0.0023 - mae: 0.0210 - mse: 0.0023 - val_loss: 0.0030 - val_mae: 0.0247 - val_mse: 0.0030\n",
      "Epoch 83/500\n",
      "4222/4222 [==============================] - 1s 296us/step - loss: 0.0022 - mae: 0.0202 - mse: 0.0022 - val_loss: 0.0027 - val_mae: 0.0249 - val_mse: 0.0027\n",
      "Epoch 84/500\n",
      "4222/4222 [==============================] - 1s 290us/step - loss: 0.0022 - mae: 0.0205 - mse: 0.0022 - val_loss: 0.0029 - val_mae: 0.0250 - val_mse: 0.0029\n",
      "Epoch 85/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0021 - mae: 0.0198 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0254 - val_mse: 0.0028\n",
      "Epoch 86/500\n",
      "4222/4222 [==============================] - 1s 301us/step - loss: 0.0021 - mae: 0.0202 - mse: 0.0021 - val_loss: 0.0029 - val_mae: 0.0257 - val_mse: 0.0029\n",
      "Epoch 87/500\n",
      "4222/4222 [==============================] - 1s 275us/step - loss: 0.0020 - mae: 0.0196 - mse: 0.0020 - val_loss: 0.0028 - val_mae: 0.0252 - val_mse: 0.0028\n",
      "Epoch 88/500\n",
      "4222/4222 [==============================] - 1s 280us/step - loss: 0.0020 - mae: 0.0198 - mse: 0.0020 - val_loss: 0.0029 - val_mae: 0.0256 - val_mse: 0.0029\n",
      "Epoch 89/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0020 - mae: 0.0194 - mse: 0.0020 - val_loss: 0.0028 - val_mae: 0.0252 - val_mse: 0.0028\n",
      "Epoch 90/500\n",
      "4222/4222 [==============================] - 1s 284us/step - loss: 0.0020 - mae: 0.0194 - mse: 0.0020 - val_loss: 0.0030 - val_mae: 0.0252 - val_mse: 0.0030\n",
      "Epoch 91/500\n",
      "4222/4222 [==============================] - 1s 299us/step - loss: 0.0019 - mae: 0.0190 - mse: 0.0019 - val_loss: 0.0029 - val_mae: 0.0250 - val_mse: 0.0029\n",
      "Epoch 92/500\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0020 - mae: 0.0191 - mse: 0.0020 - val_loss: 0.0030 - val_mae: 0.0249 - val_mse: 0.0030\n",
      "Epoch 93/500\n",
      "4222/4222 [==============================] - 1s 348us/step - loss: 0.0019 - mae: 0.0188 - mse: 0.0019 - val_loss: 0.0029 - val_mae: 0.0246 - val_mse: 0.0029\n",
      "Epoch 94/500\n",
      "4222/4222 [==============================] - 1s 281us/step - loss: 0.0019 - mae: 0.0190 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0245 - val_mse: 0.0030\n",
      "Epoch 95/500\n",
      "4222/4222 [==============================] - 2s 387us/step - loss: 0.0019 - mae: 0.0188 - mse: 0.0019 - val_loss: 0.0029 - val_mae: 0.0243 - val_mse: 0.0029\n",
      "Epoch 96/500\n",
      "4222/4222 [==============================] - 2s 433us/step - loss: 0.0019 - mae: 0.0189 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0244 - val_mse: 0.0030\n",
      "Epoch 97/500\n",
      "4222/4222 [==============================] - 3s 615us/step - loss: 0.0019 - mae: 0.0185 - mse: 0.0019 - val_loss: 0.0029 - val_mae: 0.0241 - val_mse: 0.0029\n",
      "Epoch 98/500\n",
      "4222/4222 [==============================] - 2s 513us/step - loss: 0.0019 - mae: 0.0187 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0244 - val_mse: 0.0030\n",
      "Epoch 99/500\n",
      "4222/4222 [==============================] - 2s 461us/step - loss: 0.0018 - mae: 0.0184 - mse: 0.0018 - val_loss: 0.0029 - val_mae: 0.0240 - val_mse: 0.0029\n",
      "Epoch 100/500\n",
      "4222/4222 [==============================] - 2s 382us/step - loss: 0.0018 - mae: 0.0186 - mse: 0.0018 - val_loss: 0.0031 - val_mae: 0.0246 - val_mse: 0.0031\n",
      "Epoch 101/500\n",
      "4222/4222 [==============================] - 1s 350us/step - loss: 0.0018 - mae: 0.0181 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0241 - val_mse: 0.0030\n",
      "Epoch 102/500\n",
      "4222/4222 [==============================] - 2s 358us/step - loss: 0.0018 - mae: 0.0188 - mse: 0.0018 - val_loss: 0.0031 - val_mae: 0.0245 - val_mse: 0.0031\n",
      "Epoch 103/500\n",
      "4222/4222 [==============================] - 2s 421us/step - loss: 0.0018 - mae: 0.0181 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0242 - val_mse: 0.0030\n",
      "Epoch 104/500\n",
      "4222/4222 [==============================] - 1s 350us/step - loss: 0.0018 - mae: 0.0183 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0245 - val_mse: 0.0030\n",
      "Epoch 105/500\n",
      "4222/4222 [==============================] - 2s 437us/step - loss: 0.0018 - mae: 0.0182 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0239 - val_mse: 0.0030\n",
      "Epoch 106/500\n",
      "4222/4222 [==============================] - 1s 314us/step - loss: 0.0018 - mae: 0.0187 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0233 - val_mse: 0.0030\n",
      "Epoch 107/500\n",
      "4222/4222 [==============================] - 1s 305us/step - loss: 0.0017 - mae: 0.0179 - mse: 0.0017 - val_loss: 0.0030 - val_mae: 0.0227 - val_mse: 0.0030\n",
      "Epoch 108/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0017 - mae: 0.0176 - mse: 0.0017 - val_loss: 0.0031 - val_mae: 0.0230 - val_mse: 0.0031\n",
      "Epoch 109/500\n",
      "4222/4222 [==============================] - 1s 302us/step - loss: 0.0015 - mae: 0.0173 - mse: 0.0015 - val_loss: 0.0030 - val_mae: 0.0228 - val_mse: 0.0030\n",
      "Epoch 110/500\n",
      "4222/4222 [==============================] - 1s 298us/step - loss: 0.0020 - mae: 0.0194 - mse: 0.0020 - val_loss: 0.0033 - val_mae: 0.0241 - val_mse: 0.0033\n",
      "Epoch 111/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0017 - mae: 0.0182 - mse: 0.0017 - val_loss: 0.0031 - val_mae: 0.0238 - val_mse: 0.0031\n",
      "Epoch 112/500\n",
      "4222/4222 [==============================] - 1s 284us/step - loss: 0.0016 - mae: 0.0174 - mse: 0.0016 - val_loss: 0.0031 - val_mae: 0.0234 - val_mse: 0.0031\n",
      "Epoch 113/500\n",
      "4222/4222 [==============================] - 1s 281us/step - loss: 0.0015 - mae: 0.0170 - mse: 0.0015 - val_loss: 0.0031 - val_mae: 0.0234 - val_mse: 0.0031\n",
      "Epoch 114/500\n",
      "4222/4222 [==============================] - 1s 282us/step - loss: 0.0019 - mae: 0.0192 - mse: 0.0019 - val_loss: 0.0031 - val_mae: 0.0224 - val_mse: 0.0031\n",
      "Epoch 115/500\n",
      "4222/4222 [==============================] - 1s 290us/step - loss: 0.0017 - mae: 0.0181 - mse: 0.0017 - val_loss: 0.0030 - val_mae: 0.0222 - val_mse: 0.0030\n",
      "Epoch 116/500\n",
      "4222/4222 [==============================] - 1s 293us/step - loss: 0.0020 - mae: 0.0187 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0228 - val_mse: 0.0026\n",
      "Epoch 117/500\n",
      "4222/4222 [==============================] - 1s 292us/step - loss: 0.0019 - mae: 0.0185 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0223 - val_mse: 0.0030\n",
      "Epoch 118/500\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0014 - mae: 0.0169 - mse: 0.0014 - val_loss: 0.0028 - val_mae: 0.0225 - val_mse: 0.0028\n",
      "Epoch 119/500\n",
      "4222/4222 [==============================] - 1s 306us/step - loss: 0.0020 - mae: 0.0189 - mse: 0.0020 - val_loss: 0.0030 - val_mae: 0.0216 - val_mse: 0.0030\n",
      "Epoch 120/500\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0014 - mae: 0.0164 - mse: 0.0014 - val_loss: 0.0030 - val_mae: 0.0227 - val_mse: 0.0030\n",
      "Epoch 121/500\n",
      "4222/4222 [==============================] - 1s 302us/step - loss: 0.0019 - mae: 0.0186 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0226 - val_mse: 0.0030\n",
      "Epoch 122/500\n",
      "4222/4222 [==============================] - 1s 282us/step - loss: 0.0017 - mae: 0.0189 - mse: 0.0017 - val_loss: 0.0032 - val_mae: 0.0222 - val_mse: 0.0032\n",
      "Epoch 123/500\n",
      "4222/4222 [==============================] - 1s 284us/step - loss: 0.0018 - mae: 0.0182 - mse: 0.0018 - val_loss: 0.0026 - val_mae: 0.0210 - val_mse: 0.0026\n",
      "Epoch 124/500\n",
      "4222/4222 [==============================] - 1s 337us/step - loss: 0.0015 - mae: 0.0163 - mse: 0.0015 - val_loss: 0.0030 - val_mae: 0.0218 - val_mse: 0.0030\n",
      "Epoch 125/500\n",
      "4222/4222 [==============================] - 1s 300us/step - loss: 0.0015 - mae: 0.0161 - mse: 0.0015 - val_loss: 0.0029 - val_mae: 0.0223 - val_mse: 0.0029\n",
      "Epoch 126/500\n",
      "4222/4222 [==============================] - 1s 301us/step - loss: 0.0015 - mae: 0.0167 - mse: 0.0015 - val_loss: 0.0031 - val_mae: 0.0222 - val_mse: 0.0031\n",
      "Epoch 127/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0016 - mae: 0.0166 - mse: 0.0016 - val_loss: 0.0029 - val_mae: 0.0223 - val_mse: 0.0029\n",
      "Epoch 128/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0014 - mae: 0.0161 - mse: 0.0014 - val_loss: 0.0031 - val_mae: 0.0224 - val_mse: 0.0031\n",
      "Epoch 129/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0015 - mae: 0.0161 - mse: 0.0015 - val_loss: 0.0031 - val_mae: 0.0226 - val_mse: 0.0031\n",
      "Epoch 130/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0013 - mae: 0.0158 - mse: 0.0013 - val_loss: 0.0031 - val_mae: 0.0219 - val_mse: 0.0031\n",
      "Epoch 131/500\n",
      "4222/4222 [==============================] - 1s 296us/step - loss: 0.0020 - mae: 0.0189 - mse: 0.0020 - val_loss: 0.0032 - val_mae: 0.0214 - val_mse: 0.0032\n",
      "Epoch 132/500\n",
      "4222/4222 [==============================] - 1s 305us/step - loss: 0.0013 - mae: 0.0161 - mse: 0.0013 - val_loss: 0.0030 - val_mae: 0.0218 - val_mse: 0.0030\n",
      "Epoch 133/500\n",
      "4222/4222 [==============================] - 1s 306us/step - loss: 0.0020 - mae: 0.0192 - mse: 0.0020 - val_loss: 0.0031 - val_mae: 0.0233 - val_mse: 0.0031\n",
      "Epoch 134/500\n",
      "4222/4222 [==============================] - 1s 290us/step - loss: 0.0015 - mae: 0.0173 - mse: 0.0015 - val_loss: 0.0028 - val_mae: 0.0210 - val_mse: 0.0028\n",
      "Epoch 135/500\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0026 - mae: 0.0226 - mse: 0.0026 - val_loss: 0.0026 - val_mae: 0.0236 - val_mse: 0.0026\n",
      "Epoch 136/500\n",
      "4222/4222 [==============================] - 1s 290us/step - loss: 0.0017 - mae: 0.0186 - mse: 0.0017 - val_loss: 0.0025 - val_mae: 0.0205 - val_mse: 0.0025\n",
      "Epoch 137/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0014 - mae: 0.0159 - mse: 0.0014 - val_loss: 0.0027 - val_mae: 0.0211 - val_mse: 0.0027\n",
      "Epoch 138/500\n",
      "4222/4222 [==============================] - 1s 296us/step - loss: 0.0014 - mae: 0.0157 - mse: 0.0014 - val_loss: 0.0028 - val_mae: 0.0206 - val_mse: 0.0028\n",
      "Epoch 139/500\n",
      "4222/4222 [==============================] - 1s 301us/step - loss: 0.0012 - mae: 0.0148 - mse: 0.0012 - val_loss: 0.0027 - val_mae: 0.0204 - val_mse: 0.0027\n",
      "Epoch 140/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0018 - mae: 0.0174 - mse: 0.0018 - val_loss: 0.0031 - val_mae: 0.0203 - val_mse: 0.0031\n",
      "Epoch 141/500\n",
      "4222/4222 [==============================] - 1s 294us/step - loss: 0.0013 - mae: 0.0155 - mse: 0.0013 - val_loss: 0.0026 - val_mae: 0.0200 - val_mse: 0.0026\n",
      "Epoch 142/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0020 - mae: 0.0187 - mse: 0.0020 - val_loss: 0.0032 - val_mae: 0.0204 - val_mse: 0.0032\n",
      "Epoch 143/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0013 - mae: 0.0153 - mse: 0.0013 - val_loss: 0.0027 - val_mae: 0.0205 - val_mse: 0.0027\n",
      "Epoch 144/500\n",
      "4222/4222 [==============================] - 1s 307us/step - loss: 0.0019 - mae: 0.0181 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0205 - val_mse: 0.0030\n",
      "Epoch 145/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0013 - mae: 0.0152 - mse: 0.0013 - val_loss: 0.0026 - val_mae: 0.0199 - val_mse: 0.0026\n",
      "Epoch 146/500\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0020 - mae: 0.0189 - mse: 0.0020 - val_loss: 0.0032 - val_mae: 0.0205 - val_mse: 0.0032\n",
      "Epoch 147/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0012 - mae: 0.0151 - mse: 0.0012 - val_loss: 0.0026 - val_mae: 0.0205 - val_mse: 0.0026\n",
      "Epoch 148/500\n",
      "4222/4222 [==============================] - 1s 333us/step - loss: 0.0019 - mae: 0.0184 - mse: 0.0019 - val_loss: 0.0027 - val_mae: 0.0201 - val_mse: 0.0027\n",
      "Epoch 149/500\n",
      "4222/4222 [==============================] - 1s 285us/step - loss: 0.0013 - mae: 0.0149 - mse: 0.0013 - val_loss: 0.0026 - val_mae: 0.0209 - val_mse: 0.0026\n",
      "Epoch 150/500\n",
      "4222/4222 [==============================] - 1s 299us/step - loss: 0.0019 - mae: 0.0187 - mse: 0.0019 - val_loss: 0.0032 - val_mae: 0.0206 - val_mse: 0.0032\n",
      "Epoch 151/500\n",
      "4222/4222 [==============================] - 1s 279us/step - loss: 0.0013 - mae: 0.0152 - mse: 0.0013 - val_loss: 0.0024 - val_mae: 0.0198 - val_mse: 0.0024\n",
      "Epoch 152/500\n",
      "4222/4222 [==============================] - 1s 301us/step - loss: 0.0020 - mae: 0.0188 - mse: 0.0020 - val_loss: 0.0029 - val_mae: 0.0204 - val_mse: 0.0029\n",
      "Epoch 153/500\n",
      "4222/4222 [==============================] - 1s 275us/step - loss: 0.0012 - mae: 0.0143 - mse: 0.0012 - val_loss: 0.0027 - val_mae: 0.0211 - val_mse: 0.0027\n",
      "Epoch 154/500\n",
      "4222/4222 [==============================] - 1s 312us/step - loss: 0.0019 - mae: 0.0183 - mse: 0.0019 - val_loss: 0.0028 - val_mae: 0.0205 - val_mse: 0.0028\n",
      "Epoch 155/500\n",
      "4222/4222 [==============================] - 1s 301us/step - loss: 0.0022 - mae: 0.0200 - mse: 0.0022 - val_loss: 0.0025 - val_mae: 0.0205 - val_mse: 0.0025\n",
      "Epoch 156/500\n",
      "4222/4222 [==============================] - 1s 326us/step - loss: 0.0026 - mae: 0.0221 - mse: 0.0026 - val_loss: 0.0024 - val_mae: 0.0206 - val_mse: 0.0024\n",
      "Epoch 157/500\n",
      "4222/4222 [==============================] - 1s 330us/step - loss: 0.0015 - mae: 0.0166 - mse: 0.0015 - val_loss: 0.0026 - val_mae: 0.0202 - val_mse: 0.0026\n",
      "Epoch 158/500\n",
      "4222/4222 [==============================] - 1s 306us/step - loss: 0.0019 - mae: 0.0178 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0208 - val_mse: 0.0030\n",
      "Epoch 159/500\n",
      "4222/4222 [==============================] - 1s 325us/step - loss: 0.0013 - mae: 0.0152 - mse: 0.0013 - val_loss: 0.0025 - val_mae: 0.0201 - val_mse: 0.0025\n",
      "Epoch 160/500\n",
      "4222/4222 [==============================] - 1s 299us/step - loss: 0.0019 - mae: 0.0179 - mse: 0.0019 - val_loss: 0.0031 - val_mae: 0.0209 - val_mse: 0.0031\n",
      "Epoch 161/500\n",
      "4222/4222 [==============================] - 1s 331us/step - loss: 0.0013 - mae: 0.0152 - mse: 0.0013 - val_loss: 0.0024 - val_mae: 0.0194 - val_mse: 0.0024\n",
      "Epoch 162/500\n",
      "4222/4222 [==============================] - 1s 313us/step - loss: 0.0020 - mae: 0.0187 - mse: 0.0020 - val_loss: 0.0028 - val_mae: 0.0199 - val_mse: 0.0028\n",
      "Epoch 163/500\n",
      "4222/4222 [==============================] - 1s 298us/step - loss: 0.0013 - mae: 0.0151 - mse: 0.0013 - val_loss: 0.0022 - val_mae: 0.0190 - val_mse: 0.0022\n",
      "Epoch 164/500\n",
      "4222/4222 [==============================] - 1s 302us/step - loss: 0.0019 - mae: 0.0180 - mse: 0.0019 - val_loss: 0.0024 - val_mae: 0.0195 - val_mse: 0.0024\n",
      "Epoch 165/500\n",
      "4222/4222 [==============================] - 1s 297us/step - loss: 0.0012 - mae: 0.0147 - mse: 0.0012 - val_loss: 0.0024 - val_mae: 0.0190 - val_mse: 0.0024\n",
      "Epoch 166/500\n",
      "4222/4222 [==============================] - 1s 292us/step - loss: 0.0019 - mae: 0.0184 - mse: 0.0019 - val_loss: 0.0026 - val_mae: 0.0196 - val_mse: 0.0026\n",
      "Epoch 167/500\n",
      "4222/4222 [==============================] - 1s 309us/step - loss: 0.0013 - mae: 0.0151 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0186 - val_mse: 0.0020\n",
      "Epoch 168/500\n",
      "4222/4222 [==============================] - 1s 298us/step - loss: 0.0018 - mae: 0.0178 - mse: 0.0018 - val_loss: 0.0024 - val_mae: 0.0199 - val_mse: 0.0024\n",
      "Epoch 169/500\n",
      "4222/4222 [==============================] - 1s 307us/step - loss: 0.0013 - mae: 0.0153 - mse: 0.0013 - val_loss: 0.0023 - val_mae: 0.0191 - val_mse: 0.0023\n",
      "Epoch 170/500\n",
      "4222/4222 [==============================] - 1s 300us/step - loss: 0.0019 - mae: 0.0186 - mse: 0.0019 - val_loss: 0.0027 - val_mae: 0.0201 - val_mse: 0.0027\n",
      "Epoch 171/500\n",
      "4222/4222 [==============================] - 1s 318us/step - loss: 0.0013 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0188 - val_mse: 0.0020\n",
      "Epoch 172/500\n",
      "4222/4222 [==============================] - 1s 341us/step - loss: 0.0018 - mae: 0.0177 - mse: 0.0018 - val_loss: 0.0023 - val_mae: 0.0193 - val_mse: 0.0023\n",
      "Epoch 173/500\n",
      "4222/4222 [==============================] - 1s 323us/step - loss: 0.0012 - mae: 0.0149 - mse: 0.0012 - val_loss: 0.0023 - val_mae: 0.0190 - val_mse: 0.0023\n",
      "Epoch 174/500\n",
      "3680/4222 [=========================>....] - ETA: 0s - loss: 0.0021 - mae: 0.0195 - mse: 0.0021"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-8da166ed9ccf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_LSTM_nomob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_LSTM_nomob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                        \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                        shuffle=False)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_LSTM_nomob, y_LSTM_nomob = get_LSTM_states_dataset(state_cases_df, n_steps=N_STEPS)\n",
    "X_train_LSTM_nomob, X_test_LSTM_nomob, y_train_LSTM_nomob, y_test_LSTM_nomob = train_test_split(X_LSTM_nomob, y_LSTM_nomob, test_size=SPLIT, shuffle=False)\n",
    "\n",
    "# Set up model\n",
    "n_lstm_features = X_train_LSTM_nomob.shape[2]\n",
    "lstm_nomob = get_lstm(N_STEPS, n_lstm_features)\n",
    "\n",
    "# Fit LSTM\n",
    "lstm_nomob_history = lstm_nomob.fit(X_train_LSTM_nomob, y_train_LSTM_nomob,\n",
    "                                     epochs=EPOCHS,\n",
    "                                     validation_data=(X_test_LSTM_nomob, y_test_LSTM_nomob),\n",
    "                                     verbose=1,\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_nomob_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-4ecad2a12268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Graph LSTM Error, mae and mse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_nomob_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_nomob_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lstm_nomob_history' is not defined"
     ]
    }
   ],
   "source": [
    "# Graph LSTM Error, mae and mse\n",
    "plt.plot(lstm_nomob_history.history['loss'])\n",
    "plt.plot(lstm_nomob_history.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STACKED LSTM Model with Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Stacked LSTM, Boo's model\n",
    "def get_stacked_lstm(n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    model.add(LSTM(128, input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "n_lstm_features = X_train_LSTM.shape[2]\n",
    "slstm = get_stacked_lstm(N_STEPS, n_lstm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2440 samples, validate on 610 samples\n",
      "Epoch 1/200\n",
      "2440/2440 [==============================] - 2s 785us/step - loss: 0.0174 - mae: 0.0798 - mse: 0.0174 - val_loss: 0.0022 - val_mae: 0.0349 - val_mse: 0.0022\n",
      "Epoch 2/200\n",
      "2440/2440 [==============================] - 1s 543us/step - loss: 0.0022 - mae: 0.0343 - mse: 0.0022 - val_loss: 0.0024 - val_mae: 0.0412 - val_mse: 0.0024\n",
      "Epoch 3/200\n",
      "2440/2440 [==============================] - 1s 549us/step - loss: 0.0017 - mae: 0.0310 - mse: 0.0017 - val_loss: 0.0015 - val_mae: 0.0299 - val_mse: 0.0015\n",
      "Epoch 4/200\n",
      "2440/2440 [==============================] - 1s 562us/step - loss: 0.0015 - mae: 0.0279 - mse: 0.0015 - val_loss: 0.0013 - val_mae: 0.0278 - val_mse: 0.0013\n",
      "Epoch 5/200\n",
      "2440/2440 [==============================] - 1s 537us/step - loss: 0.0014 - mae: 0.0272 - mse: 0.0014 - val_loss: 0.0015 - val_mae: 0.0328 - val_mse: 0.0015\n",
      "Epoch 6/200\n",
      "2440/2440 [==============================] - 1s 538us/step - loss: 0.0013 - mae: 0.0256 - mse: 0.0013 - val_loss: 0.0014 - val_mae: 0.0322 - val_mse: 0.0014\n",
      "Epoch 7/200\n",
      "2440/2440 [==============================] - 1s 536us/step - loss: 0.0012 - mae: 0.0245 - mse: 0.0012 - val_loss: 0.0013 - val_mae: 0.0297 - val_mse: 0.0013\n",
      "Epoch 8/200\n",
      "2440/2440 [==============================] - 1s 554us/step - loss: 0.0011 - mae: 0.0238 - mse: 0.0011 - val_loss: 0.0011 - val_mae: 0.0262 - val_mse: 0.0011\n",
      "Epoch 9/200\n",
      "2440/2440 [==============================] - 1s 539us/step - loss: 0.0011 - mae: 0.0234 - mse: 0.0011 - val_loss: 8.5337e-04 - val_mae: 0.0215 - val_mse: 8.5337e-04\n",
      "Epoch 10/200\n",
      "2440/2440 [==============================] - 1s 549us/step - loss: 0.0010 - mae: 0.0230 - mse: 0.0010 - val_loss: 7.2569e-04 - val_mae: 0.0169 - val_mse: 7.2569e-04\n",
      "Epoch 11/200\n",
      "2440/2440 [==============================] - ETA: 0s - loss: 9.9261e-04 - mae: 0.0230 - mse: 9.9261e-0 - 1s 544us/step - loss: 9.9252e-04 - mae: 0.0230 - mse: 9.9252e-04 - val_loss: 7.3569e-04 - val_mae: 0.0186 - val_mse: 7.3569e-04\n",
      "Epoch 12/200\n",
      "2440/2440 [==============================] - 1s 544us/step - loss: 9.9194e-04 - mae: 0.0238 - mse: 9.9194e-04 - val_loss: 7.7677e-04 - val_mae: 0.0206 - val_mse: 7.7677e-04\n",
      "Epoch 13/200\n",
      "2440/2440 [==============================] - 1s 543us/step - loss: 9.3702e-04 - mae: 0.0234 - mse: 9.3702e-04 - val_loss: 6.2246e-04 - val_mae: 0.0161 - val_mse: 6.2246e-04\n",
      "Epoch 14/200\n",
      "2440/2440 [==============================] - 1s 559us/step - loss: 7.8055e-04 - mae: 0.0204 - mse: 7.8055e-04 - val_loss: 6.0066e-04 - val_mae: 0.0174 - val_mse: 6.0066e-04\n",
      "Epoch 15/200\n",
      "2440/2440 [==============================] - 1s 547us/step - loss: 7.0310e-04 - mae: 0.0192 - mse: 7.0310e-04 - val_loss: 5.5662e-04 - val_mae: 0.0166 - val_mse: 5.5662e-04\n",
      "Epoch 16/200\n",
      "2440/2440 [==============================] - 1s 535us/step - loss: 6.5138e-04 - mae: 0.0183 - mse: 6.5138e-04 - val_loss: 5.2970e-04 - val_mae: 0.0164 - val_mse: 5.2970e-04\n",
      "Epoch 17/200\n",
      "2440/2440 [==============================] - 1s 545us/step - loss: 6.0327e-04 - mae: 0.0175 - mse: 6.0327e-04 - val_loss: 5.0735e-04 - val_mae: 0.0162 - val_mse: 5.0735e-04\n",
      "Epoch 18/200\n",
      "2440/2440 [==============================] - 1s 540us/step - loss: 5.6004e-04 - mae: 0.0167 - mse: 5.6004e-04 - val_loss: 4.8954e-04 - val_mae: 0.0161 - val_mse: 4.8954e-04\n",
      "Epoch 19/200\n",
      "2440/2440 [==============================] - 1s 575us/step - loss: 5.2100e-04 - mae: 0.0160 - mse: 5.2100e-04 - val_loss: 4.7384e-04 - val_mae: 0.0160 - val_mse: 4.7384e-04\n",
      "Epoch 20/200\n",
      "2440/2440 [==============================] - 1s 553us/step - loss: 4.8647e-04 - mae: 0.0154 - mse: 4.8647e-04 - val_loss: 4.5874e-04 - val_mae: 0.0159 - val_mse: 4.5874e-04\n",
      "Epoch 21/200\n",
      "2440/2440 [==============================] - 1s 541us/step - loss: 4.5666e-04 - mae: 0.0148 - mse: 4.5666e-04 - val_loss: 4.4327e-04 - val_mae: 0.0157 - val_mse: 4.4327e-04\n",
      "Epoch 22/200\n",
      "2440/2440 [==============================] - 1s 608us/step - loss: 4.3159e-04 - mae: 0.0144 - mse: 4.3159e-04 - val_loss: 4.2706e-04 - val_mae: 0.0154 - val_mse: 4.2706e-04\n",
      "Epoch 23/200\n",
      "2440/2440 [==============================] - 2s 622us/step - loss: 4.1115e-04 - mae: 0.0140 - mse: 4.1115e-04 - val_loss: 4.1012e-04 - val_mae: 0.0150 - val_mse: 4.1012e-04\n",
      "Epoch 24/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 3.9503e-04 - mae: 0.0138 - mse: 3.9503e-04 - val_loss: 3.9268e-04 - val_mae: 0.0146 - val_mse: 3.9268e-04\n",
      "Epoch 25/200\n",
      "2440/2440 [==============================] - 2s 630us/step - loss: 3.8265e-04 - mae: 0.0136 - mse: 3.8265e-04 - val_loss: 3.7516e-04 - val_mae: 0.0141 - val_mse: 3.7516e-04\n",
      "Epoch 26/200\n",
      "2440/2440 [==============================] - 1s 614us/step - loss: 3.7304e-04 - mae: 0.0135 - mse: 3.7304e-04 - val_loss: 3.5832e-04 - val_mae: 0.0136 - val_mse: 3.5832e-04\n",
      "Epoch 27/200\n",
      "2440/2440 [==============================] - 2s 652us/step - loss: 3.6480e-04 - mae: 0.0134 - mse: 3.6480e-04 - val_loss: 3.4311e-04 - val_mae: 0.0131 - val_mse: 3.4311e-04\n",
      "Epoch 28/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 3.5625e-04 - mae: 0.0133 - mse: 3.5625e-04 - val_loss: 3.3030e-04 - val_mae: 0.0126 - val_mse: 3.3030e-04\n",
      "Epoch 29/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 3.4595e-04 - mae: 0.0131 - mse: 3.4595e-04 - val_loss: 3.1994e-04 - val_mae: 0.0122 - val_mse: 3.1994e-04\n",
      "Epoch 30/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 3.3348e-04 - mae: 0.0129 - mse: 3.3348e-04 - val_loss: 3.1119e-04 - val_mae: 0.0118 - val_mse: 3.1119e-04\n",
      "Epoch 31/200\n",
      "2440/2440 [==============================] - 2s 641us/step - loss: 3.1954e-04 - mae: 0.0126 - mse: 3.1954e-04 - val_loss: 3.0309e-04 - val_mae: 0.0115 - val_mse: 3.0309e-04\n",
      "Epoch 32/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 3.0543e-04 - mae: 0.0123 - mse: 3.0543e-04 - val_loss: 2.9521e-04 - val_mae: 0.0111 - val_mse: 2.9521e-04\n",
      "Epoch 33/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 2.9220e-04 - mae: 0.0120 - mse: 2.9220e-04 - val_loss: 2.8765e-04 - val_mae: 0.0108 - val_mse: 2.8765e-04\n",
      "Epoch 34/200\n",
      "2440/2440 [==============================] - 2s 635us/step - loss: 2.8037e-04 - mae: 0.0117 - mse: 2.8037e-04 - val_loss: 2.8070e-04 - val_mae: 0.0105 - val_mse: 2.8070e-04\n",
      "Epoch 35/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 2.7004e-04 - mae: 0.0114 - mse: 2.7004e-04 - val_loss: 2.7459e-04 - val_mae: 0.0102 - val_mse: 2.7459e-04\n",
      "Epoch 36/200\n",
      "2440/2440 [==============================] - 1s 608us/step - loss: 2.6110e-04 - mae: 0.0112 - mse: 2.6110e-04 - val_loss: 2.6937e-04 - val_mae: 0.0100 - val_mse: 2.6937e-04\n",
      "Epoch 37/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 2.5337e-04 - mae: 0.0110 - mse: 2.5337e-04 - val_loss: 2.6503e-04 - val_mae: 0.0099 - val_mse: 2.6503e-04\n",
      "Epoch 38/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 2.4665e-04 - mae: 0.0109 - mse: 2.4665e-04 - val_loss: 2.6148e-04 - val_mae: 0.0098 - val_mse: 2.6148e-04\n",
      "Epoch 39/200\n",
      "2440/2440 [==============================] - 2s 635us/step - loss: 2.4079e-04 - mae: 0.0107 - mse: 2.4079e-04 - val_loss: 2.5861e-04 - val_mae: 0.0097 - val_mse: 2.5861e-04\n",
      "Epoch 40/200\n",
      "2440/2440 [==============================] - 1s 602us/step - loss: 2.3566e-04 - mae: 0.0106 - mse: 2.3566e-04 - val_loss: 2.5635e-04 - val_mae: 0.0097 - val_mse: 2.5635e-04\n",
      "Epoch 41/200\n",
      "2440/2440 [==============================] - 1s 607us/step - loss: 2.3115e-04 - mae: 0.0105 - mse: 2.3115e-04 - val_loss: 2.5459e-04 - val_mae: 0.0096 - val_mse: 2.5459e-04\n",
      "Epoch 42/200\n",
      "2440/2440 [==============================] - 2s 616us/step - loss: 2.2720e-04 - mae: 0.0104 - mse: 2.2720e-04 - val_loss: 2.5326e-04 - val_mae: 0.0097 - val_mse: 2.5326e-04\n",
      "Epoch 43/200\n",
      "2440/2440 [==============================] - 2s 678us/step - loss: 2.2372e-04 - mae: 0.0103 - mse: 2.2372e-04 - val_loss: 2.5230e-04 - val_mae: 0.0097 - val_mse: 2.5230e-04\n",
      "Epoch 44/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 2.2066e-04 - mae: 0.0102 - mse: 2.2066e-04 - val_loss: 2.5162e-04 - val_mae: 0.0097 - val_mse: 2.5162e-04\n",
      "Epoch 45/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 2.1795e-04 - mae: 0.0102 - mse: 2.1795e-04 - val_loss: 2.5118e-04 - val_mae: 0.0098 - val_mse: 2.5118e-04\n",
      "Epoch 46/200\n",
      "2440/2440 [==============================] - 2s 636us/step - loss: 2.1554e-04 - mae: 0.0101 - mse: 2.1554e-04 - val_loss: 2.5092e-04 - val_mae: 0.0098 - val_mse: 2.5092e-04\n",
      "Epoch 47/200\n",
      "2440/2440 [==============================] - 2s 635us/step - loss: 2.1337e-04 - mae: 0.0100 - mse: 2.1337e-04 - val_loss: 2.5079e-04 - val_mae: 0.0099 - val_mse: 2.5079e-04\n",
      "Epoch 48/200\n",
      "2440/2440 [==============================] - 2s 670us/step - loss: 2.1138e-04 - mae: 0.0100 - mse: 2.1138e-04 - val_loss: 2.5073e-04 - val_mae: 0.0099 - val_mse: 2.5073e-04\n",
      "Epoch 49/200\n",
      "2440/2440 [==============================] - 2s 618us/step - loss: 2.0952e-04 - mae: 0.0099 - mse: 2.0952e-04 - val_loss: 2.5070e-04 - val_mae: 0.0100 - val_mse: 2.5070e-04\n",
      "Epoch 50/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 2.0773e-04 - mae: 0.0099 - mse: 2.0773e-04 - val_loss: 2.5066e-04 - val_mae: 0.0100 - val_mse: 2.5066e-04\n",
      "Epoch 51/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 2.0598e-04 - mae: 0.0098 - mse: 2.0598e-04 - val_loss: 2.5055e-04 - val_mae: 0.0101 - val_mse: 2.5055e-04\n",
      "Epoch 52/200\n",
      "2440/2440 [==============================] - 2s 631us/step - loss: 2.0423e-04 - mae: 0.0097 - mse: 2.0423e-04 - val_loss: 2.5035e-04 - val_mae: 0.0101 - val_mse: 2.5035e-04\n",
      "Epoch 53/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 2.0244e-04 - mae: 0.0097 - mse: 2.0244e-04 - val_loss: 2.5000e-04 - val_mae: 0.0101 - val_mse: 2.5000e-04\n",
      "Epoch 54/200\n",
      "2440/2440 [==============================] - 1s 607us/step - loss: 2.0061e-04 - mae: 0.0096 - mse: 2.0061e-04 - val_loss: 2.4946e-04 - val_mae: 0.0101 - val_mse: 2.4946e-04\n",
      "Epoch 55/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.9871e-04 - mae: 0.0096 - mse: 1.9871e-04 - val_loss: 2.4869e-04 - val_mae: 0.0101 - val_mse: 2.4869e-04\n",
      "Epoch 56/200\n",
      "2440/2440 [==============================] - 2s 616us/step - loss: 1.9673e-04 - mae: 0.0095 - mse: 1.9673e-04 - val_loss: 2.4762e-04 - val_mae: 0.0101 - val_mse: 2.4762e-04\n",
      "Epoch 57/200\n",
      "2440/2440 [==============================] - 2s 633us/step - loss: 1.9470e-04 - mae: 0.0094 - mse: 1.9470e-04 - val_loss: 2.4620e-04 - val_mae: 0.0101 - val_mse: 2.4620e-04\n",
      "Epoch 58/200\n",
      "2440/2440 [==============================] - 2s 678us/step - loss: 1.9261e-04 - mae: 0.0094 - mse: 1.9261e-04 - val_loss: 2.4438e-04 - val_mae: 0.0100 - val_mse: 2.4438e-04\n",
      "Epoch 59/200\n",
      "2440/2440 [==============================] - 1s 614us/step - loss: 1.9050e-04 - mae: 0.0093 - mse: 1.9050e-04 - val_loss: 2.4211e-04 - val_mae: 0.0099 - val_mse: 2.4211e-04\n",
      "Epoch 60/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.8841e-04 - mae: 0.0092 - mse: 1.8841e-04 - val_loss: 2.3939e-04 - val_mae: 0.0097 - val_mse: 2.3939e-04\n",
      "Epoch 61/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.8639e-04 - mae: 0.0092 - mse: 1.8639e-04 - val_loss: 2.3623e-04 - val_mae: 0.0096 - val_mse: 2.3623e-04\n",
      "Epoch 62/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.8449e-04 - mae: 0.0091 - mse: 1.8449e-04 - val_loss: 2.3272e-04 - val_mae: 0.0094 - val_mse: 2.3272e-04\n",
      "Epoch 63/200\n",
      "2440/2440 [==============================] - 2s 635us/step - loss: 1.8276e-04 - mae: 0.0090 - mse: 1.8276e-04 - val_loss: 2.2899e-04 - val_mae: 0.0091 - val_mse: 2.2899e-04\n",
      "Epoch 64/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 1.8124e-04 - mae: 0.0090 - mse: 1.8124e-04 - val_loss: 2.2522e-04 - val_mae: 0.0089 - val_mse: 2.2522e-04\n",
      "Epoch 65/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 1.7999e-04 - mae: 0.0089 - mse: 1.7999e-04 - val_loss: 2.2162e-04 - val_mae: 0.0086 - val_mse: 2.2162e-04\n",
      "Epoch 66/200\n",
      "2440/2440 [==============================] - 2s 626us/step - loss: 1.7905e-04 - mae: 0.0089 - mse: 1.7905e-04 - val_loss: 2.1843e-04 - val_mae: 0.0084 - val_mse: 2.1843e-04\n",
      "Epoch 67/200\n",
      "2440/2440 [==============================] - 1s 607us/step - loss: 1.7847e-04 - mae: 0.0088 - mse: 1.7847e-04 - val_loss: 2.1586e-04 - val_mae: 0.0082 - val_mse: 2.1586e-04\n",
      "Epoch 68/200\n",
      "2440/2440 [==============================] - 2s 673us/step - loss: 1.7838e-04 - mae: 0.0088 - mse: 1.7838e-04 - val_loss: 2.1413e-04 - val_mae: 0.0080 - val_mse: 2.1413e-04\n",
      "Epoch 69/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.7894e-04 - mae: 0.0089 - mse: 1.7894e-04 - val_loss: 2.1339e-04 - val_mae: 0.0079 - val_mse: 2.1339e-04\n",
      "Epoch 70/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 1.8041e-04 - mae: 0.0089 - mse: 1.8041e-04 - val_loss: 2.1370e-04 - val_mae: 0.0078 - val_mse: 2.1370e-04\n",
      "Epoch 71/200\n",
      "2440/2440 [==============================] - 1s 610us/step - loss: 1.8294e-04 - mae: 0.0090 - mse: 1.8294e-04 - val_loss: 2.1498e-04 - val_mae: 0.0079 - val_mse: 2.1498e-04\n",
      "Epoch 72/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.8638e-04 - mae: 0.0092 - mse: 1.8638e-04 - val_loss: 2.1666e-04 - val_mae: 0.0080 - val_mse: 2.1666e-04\n",
      "Epoch 73/200\n",
      "2440/2440 [==============================] - 2s 657us/step - loss: 1.8971e-04 - mae: 0.0093 - mse: 1.8971e-04 - val_loss: 2.1766e-04 - val_mae: 0.0081 - val_mse: 2.1766e-04\n",
      "Epoch 74/200\n",
      "2440/2440 [==============================] - 2s 641us/step - loss: 1.9119e-04 - mae: 0.0094 - mse: 1.9119e-04 - val_loss: 2.1721e-04 - val_mae: 0.0081 - val_mse: 2.1721e-04\n",
      "Epoch 75/200\n",
      "2440/2440 [==============================] - 2s 618us/step - loss: 1.8978e-04 - mae: 0.0093 - mse: 1.8978e-04 - val_loss: 2.1578e-04 - val_mae: 0.0081 - val_mse: 2.1578e-04\n",
      "Epoch 76/200\n",
      "2440/2440 [==============================] - 2s 618us/step - loss: 1.8657e-04 - mae: 0.0092 - mse: 1.8657e-04 - val_loss: 2.1437e-04 - val_mae: 0.0080 - val_mse: 2.1437e-04\n",
      "Epoch 77/200\n",
      "2440/2440 [==============================] - 2s 624us/step - loss: 1.8357e-04 - mae: 0.0091 - mse: 1.8357e-04 - val_loss: 2.1346e-04 - val_mae: 0.0079 - val_mse: 2.1346e-04\n",
      "Epoch 78/200\n",
      "2440/2440 [==============================] - 2s 638us/step - loss: 1.8185e-04 - mae: 0.0090 - mse: 1.8185e-04 - val_loss: 2.1322e-04 - val_mae: 0.0079 - val_mse: 2.1322e-04\n",
      "Epoch 79/200\n",
      "2440/2440 [==============================] - 2s 646us/step - loss: 1.8152e-04 - mae: 0.0090 - mse: 1.8152e-04 - val_loss: 2.1360e-04 - val_mae: 0.0079 - val_mse: 2.1360e-04\n",
      "Epoch 80/200\n",
      "2440/2440 [==============================] - 2s 647us/step - loss: 1.8224e-04 - mae: 0.0090 - mse: 1.8224e-04 - val_loss: 2.1431e-04 - val_mae: 0.0079 - val_mse: 2.1431e-04\n",
      "Epoch 81/200\n",
      "2440/2440 [==============================] - 2s 639us/step - loss: 1.8334e-04 - mae: 0.0091 - mse: 1.8334e-04 - val_loss: 2.1479e-04 - val_mae: 0.0080 - val_mse: 2.1479e-04\n",
      "Epoch 82/200\n",
      "2440/2440 [==============================] - 2s 647us/step - loss: 1.8392e-04 - mae: 0.0091 - mse: 1.8392e-04 - val_loss: 2.1460e-04 - val_mae: 0.0080 - val_mse: 2.1460e-04\n",
      "Epoch 83/200\n",
      "2440/2440 [==============================] - 2s 616us/step - loss: 1.8335e-04 - mae: 0.0091 - mse: 1.8335e-04 - val_loss: 2.1380e-04 - val_mae: 0.0080 - val_mse: 2.1380e-04\n",
      "Epoch 84/200\n",
      "2440/2440 [==============================] - 2s 634us/step - loss: 1.8180e-04 - mae: 0.0090 - mse: 1.8180e-04 - val_loss: 2.1284e-04 - val_mae: 0.0079 - val_mse: 2.1284e-04\n",
      "Epoch 85/200\n",
      "2440/2440 [==============================] - 2s 653us/step - loss: 1.8002e-04 - mae: 0.0089 - mse: 1.8002e-04 - val_loss: 2.1212e-04 - val_mae: 0.0079 - val_mse: 2.1212e-04\n",
      "Epoch 86/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.7869e-04 - mae: 0.0089 - mse: 1.7869e-04 - val_loss: 2.1185e-04 - val_mae: 0.0078 - val_mse: 2.1185e-04\n",
      "Epoch 87/200\n",
      "2440/2440 [==============================] - 2s 647us/step - loss: 1.7810e-04 - mae: 0.0089 - mse: 1.7810e-04 - val_loss: 2.1204e-04 - val_mae: 0.0079 - val_mse: 2.1204e-04\n",
      "Epoch 88/200\n",
      "2440/2440 [==============================] - 2s 616us/step - loss: 1.7819e-04 - mae: 0.0089 - mse: 1.7819e-04 - val_loss: 2.1255e-04 - val_mae: 0.0079 - val_mse: 2.1255e-04\n",
      "Epoch 89/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2440/2440 [==============================] - 1s 597us/step - loss: 1.7867e-04 - mae: 0.0089 - mse: 1.7867e-04 - val_loss: 2.1307e-04 - val_mae: 0.0079 - val_mse: 2.1307e-04\n",
      "Epoch 90/200\n",
      "2440/2440 [==============================] - 1s 596us/step - loss: 1.7908e-04 - mae: 0.0089 - mse: 1.7908e-04 - val_loss: 2.1328e-04 - val_mae: 0.0080 - val_mse: 2.1328e-04\n",
      "Epoch 91/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 1.7892e-04 - mae: 0.0089 - mse: 1.7892e-04 - val_loss: 2.1301e-04 - val_mae: 0.0080 - val_mse: 2.1301e-04\n",
      "Epoch 92/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 1.7805e-04 - mae: 0.0089 - mse: 1.7805e-04 - val_loss: 2.1243e-04 - val_mae: 0.0080 - val_mse: 2.1243e-04\n",
      "Epoch 93/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 1.7671e-04 - mae: 0.0088 - mse: 1.7671e-04 - val_loss: 2.1182e-04 - val_mae: 0.0080 - val_mse: 2.1182e-04\n",
      "Epoch 94/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 1.7536e-04 - mae: 0.0087 - mse: 1.7536e-04 - val_loss: 2.1142e-04 - val_mae: 0.0080 - val_mse: 2.1142e-04\n",
      "Epoch 95/200\n",
      "2440/2440 [==============================] - 2s 619us/step - loss: 1.7433e-04 - mae: 0.0087 - mse: 1.7433e-04 - val_loss: 2.1140e-04 - val_mae: 0.0080 - val_mse: 2.1140e-04\n",
      "Epoch 96/200\n",
      "2440/2440 [==============================] - 2s 631us/step - loss: 1.7377e-04 - mae: 0.0086 - mse: 1.7377e-04 - val_loss: 2.1183e-04 - val_mae: 0.0080 - val_mse: 2.1183e-04\n",
      "Epoch 97/200\n",
      "2440/2440 [==============================] - 2s 646us/step - loss: 1.7371e-04 - mae: 0.0086 - mse: 1.7371e-04 - val_loss: 2.1269e-04 - val_mae: 0.0081 - val_mse: 2.1269e-04\n",
      "Epoch 98/200\n",
      "2440/2440 [==============================] - 2s 653us/step - loss: 1.7404e-04 - mae: 0.0087 - mse: 1.7404e-04 - val_loss: 2.1380e-04 - val_mae: 0.0082 - val_mse: 2.1380e-04\n",
      "Epoch 99/200\n",
      "2440/2440 [==============================] - 2s 630us/step - loss: 1.7453e-04 - mae: 0.0087 - mse: 1.7453e-04 - val_loss: 2.1481e-04 - val_mae: 0.0083 - val_mse: 2.1481e-04\n",
      "Epoch 100/200\n",
      "2440/2440 [==============================] - 2s 623us/step - loss: 1.7473e-04 - mae: 0.0087 - mse: 1.7473e-04 - val_loss: 2.1530e-04 - val_mae: 0.0084 - val_mse: 2.1530e-04\n",
      "Epoch 101/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.7421e-04 - mae: 0.0086 - mse: 1.7421e-04 - val_loss: 2.1500e-04 - val_mae: 0.0084 - val_mse: 2.1500e-04\n",
      "Epoch 102/200\n",
      "2440/2440 [==============================] - 2s 650us/step - loss: 1.7286e-04 - mae: 0.0086 - mse: 1.7286e-04 - val_loss: 2.1392e-04 - val_mae: 0.0084 - val_mse: 2.1392e-04\n",
      "Epoch 103/200\n",
      "2440/2440 [==============================] - 2s 646us/step - loss: 1.7108e-04 - mae: 0.0085 - mse: 1.7108e-04 - val_loss: 2.1227e-04 - val_mae: 0.0083 - val_mse: 2.1227e-04\n",
      "Epoch 104/200\n",
      "2440/2440 [==============================] - 2s 615us/step - loss: 1.6947e-04 - mae: 0.0084 - mse: 1.6947e-04 - val_loss: 2.1041e-04 - val_mae: 0.0081 - val_mse: 2.1041e-04\n",
      "Epoch 105/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.6856e-04 - mae: 0.0084 - mse: 1.6856e-04 - val_loss: 2.0880e-04 - val_mae: 0.0079 - val_mse: 2.0880e-04\n",
      "Epoch 106/200\n",
      "2440/2440 [==============================] - 1s 610us/step - loss: 1.6847e-04 - mae: 0.0084 - mse: 1.6847e-04 - val_loss: 2.0778e-04 - val_mae: 0.0078 - val_mse: 2.0778e-04\n",
      "Epoch 107/200\n",
      "2440/2440 [==============================] - 2s 639us/step - loss: 1.6853e-04 - mae: 0.0084 - mse: 1.6853e-04 - val_loss: 2.0715e-04 - val_mae: 0.0077 - val_mse: 2.0715e-04\n",
      "Epoch 108/200\n",
      "2440/2440 [==============================] - 2s 642us/step - loss: 1.6781e-04 - mae: 0.0084 - mse: 1.6781e-04 - val_loss: 2.0682e-04 - val_mae: 0.0077 - val_mse: 2.0682e-04\n",
      "Epoch 109/200\n",
      "2440/2440 [==============================] - 2s 623us/step - loss: 1.6663e-04 - mae: 0.0084 - mse: 1.6663e-04 - val_loss: 2.0771e-04 - val_mae: 0.0078 - val_mse: 2.0771e-04\n",
      "Epoch 110/200\n",
      "2440/2440 [==============================] - 2s 636us/step - loss: 1.6652e-04 - mae: 0.0083 - mse: 1.6652e-04 - val_loss: 2.1069e-04 - val_mae: 0.0080 - val_mse: 2.1069e-04\n",
      "Epoch 111/200\n",
      "2440/2440 [==============================] - 2s 635us/step - loss: 1.6856e-04 - mae: 0.0084 - mse: 1.6856e-04 - val_loss: 2.1570e-04 - val_mae: 0.0084 - val_mse: 2.1570e-04\n",
      "Epoch 112/200\n",
      "2440/2440 [==============================] - 2s 702us/step - loss: 1.7247e-04 - mae: 0.0086 - mse: 1.7247e-04 - val_loss: 2.2105e-04 - val_mae: 0.0088 - val_mse: 2.2105e-04\n",
      "Epoch 113/200\n",
      "2440/2440 [==============================] - 2s 672us/step - loss: 1.7593e-04 - mae: 0.0087 - mse: 1.7593e-04 - val_loss: 2.2329e-04 - val_mae: 0.0090 - val_mse: 2.2329e-04\n",
      "Epoch 114/200\n",
      "2440/2440 [==============================] - 2s 660us/step - loss: 1.7576e-04 - mae: 0.0087 - mse: 1.7576e-04 - val_loss: 2.2167e-04 - val_mae: 0.0090 - val_mse: 2.2167e-04\n",
      "Epoch 115/200\n",
      "2440/2440 [==============================] - 2s 678us/step - loss: 1.7411e-04 - mae: 0.0087 - mse: 1.7411e-04 - val_loss: 2.2041e-04 - val_mae: 0.0090 - val_mse: 2.2041e-04\n",
      "Epoch 116/200\n",
      "2440/2440 [==============================] - 2s 631us/step - loss: 1.7623e-04 - mae: 0.0089 - mse: 1.7623e-04 - val_loss: 2.2307e-04 - val_mae: 0.0091 - val_mse: 2.2307e-04\n",
      "Epoch 117/200\n",
      "2440/2440 [==============================] - 2s 633us/step - loss: 1.7719e-04 - mae: 0.0090 - mse: 1.7719e-04 - val_loss: 2.2640e-04 - val_mae: 0.0094 - val_mse: 2.2640e-04\n",
      "Epoch 118/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 1.7411e-04 - mae: 0.0089 - mse: 1.7411e-04 - val_loss: 2.3245e-04 - val_mae: 0.0098 - val_mse: 2.3245e-04\n",
      "Epoch 119/200\n",
      "2440/2440 [==============================] - 2s 618us/step - loss: 1.7459e-04 - mae: 0.0088 - mse: 1.7459e-04 - val_loss: 2.4088e-04 - val_mae: 0.0102 - val_mse: 2.4088e-04\n",
      "Epoch 120/200\n",
      "2440/2440 [==============================] - 1s 597us/step - loss: 1.7730e-04 - mae: 0.0089 - mse: 1.7730e-04 - val_loss: 2.4940e-04 - val_mae: 0.0107 - val_mse: 2.4940e-04\n",
      "Epoch 121/200\n",
      "2440/2440 [==============================] - 1s 602us/step - loss: 1.7991e-04 - mae: 0.0090 - mse: 1.7991e-04 - val_loss: 2.5886e-04 - val_mae: 0.0111 - val_mse: 2.5886e-04\n",
      "Epoch 122/200\n",
      "2440/2440 [==============================] - 2s 676us/step - loss: 1.8254e-04 - mae: 0.0091 - mse: 1.8254e-04 - val_loss: 2.7037e-04 - val_mae: 0.0116 - val_mse: 2.7037e-04\n",
      "Epoch 123/200\n",
      "2440/2440 [==============================] - 2s 726us/step - loss: 1.8600e-04 - mae: 0.0092 - mse: 1.8600e-04 - val_loss: 2.8423e-04 - val_mae: 0.0122 - val_mse: 2.8423e-04\n",
      "Epoch 124/200\n",
      "2440/2440 [==============================] - 2s 669us/step - loss: 1.9109e-04 - mae: 0.0094 - mse: 1.9109e-04 - val_loss: 2.9919e-04 - val_mae: 0.0128 - val_mse: 2.9919e-04\n",
      "Epoch 125/200\n",
      "2440/2440 [==============================] - 2s 709us/step - loss: 1.9792e-04 - mae: 0.0096 - mse: 1.9792e-04 - val_loss: 3.1152e-04 - val_mae: 0.0132 - val_mse: 3.1152e-04\n",
      "Epoch 126/200\n",
      "2440/2440 [==============================] - 2s 696us/step - loss: 2.0729e-04 - mae: 0.0100 - mse: 2.0729e-04 - val_loss: 3.1875e-04 - val_mae: 0.0134 - val_mse: 3.1875e-04\n",
      "Epoch 127/200\n",
      "2440/2440 [==============================] - 2s 710us/step - loss: 2.1927e-04 - mae: 0.0104 - mse: 2.1927e-04 - val_loss: 3.0646e-04 - val_mae: 0.0130 - val_mse: 3.0646e-04\n",
      "Epoch 128/200\n",
      "2440/2440 [==============================] - 2s 617us/step - loss: 2.2328e-04 - mae: 0.0106 - mse: 2.2328e-04 - val_loss: 2.6319e-04 - val_mae: 0.0114 - val_mse: 2.6319e-04\n",
      "Epoch 129/200\n",
      "2440/2440 [==============================] - 2s 675us/step - loss: 2.0703e-04 - mae: 0.0102 - mse: 2.0703e-04 - val_loss: 2.2257e-04 - val_mae: 0.0091 - val_mse: 2.2257e-04\n",
      "Epoch 130/200\n",
      "2440/2440 [==============================] - 2s 621us/step - loss: 1.8852e-04 - mae: 0.0096 - mse: 1.8852e-04 - val_loss: 2.0893e-04 - val_mae: 0.0078 - val_mse: 2.0893e-04\n",
      "Epoch 131/200\n",
      "2440/2440 [==============================] - 2s 627us/step - loss: 1.7974e-04 - mae: 0.0092 - mse: 1.7974e-04 - val_loss: 2.0693e-04 - val_mae: 0.0076 - val_mse: 2.0693e-04\n",
      "Epoch 132/200\n",
      "2440/2440 [==============================] - 2s 680us/step - loss: 1.7811e-04 - mae: 0.0091 - mse: 1.7811e-04 - val_loss: 2.0691e-04 - val_mae: 0.0076 - val_mse: 2.0691e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/200\n",
      "2440/2440 [==============================] - 2s 656us/step - loss: 1.7691e-04 - mae: 0.0090 - mse: 1.7691e-04 - val_loss: 2.0746e-04 - val_mae: 0.0076 - val_mse: 2.0746e-04\n",
      "Epoch 134/200\n",
      "2440/2440 [==============================] - 2s 682us/step - loss: 1.7496e-04 - mae: 0.0090 - mse: 1.7496e-04 - val_loss: 2.0820e-04 - val_mae: 0.0076 - val_mse: 2.0820e-04\n",
      "Epoch 135/200\n",
      "2440/2440 [==============================] - 1s 600us/step - loss: 1.7299e-04 - mae: 0.0089 - mse: 1.7299e-04 - val_loss: 2.0894e-04 - val_mae: 0.0076 - val_mse: 2.0894e-04\n",
      "Epoch 136/200\n",
      "2440/2440 [==============================] - 1s 603us/step - loss: 1.7137e-04 - mae: 0.0088 - mse: 1.7137e-04 - val_loss: 2.0958e-04 - val_mae: 0.0077 - val_mse: 2.0958e-04\n",
      "Epoch 137/200\n",
      "2440/2440 [==============================] - 1s 596us/step - loss: 1.7013e-04 - mae: 0.0088 - mse: 1.7013e-04 - val_loss: 2.1011e-04 - val_mae: 0.0077 - val_mse: 2.1011e-04\n",
      "Epoch 138/200\n",
      "2440/2440 [==============================] - 1s 593us/step - loss: 1.6916e-04 - mae: 0.0087 - mse: 1.6916e-04 - val_loss: 2.1055e-04 - val_mae: 0.0077 - val_mse: 2.1055e-04\n",
      "Epoch 139/200\n",
      "2440/2440 [==============================] - 2s 633us/step - loss: 1.6840e-04 - mae: 0.0087 - mse: 1.6840e-04 - val_loss: 2.1091e-04 - val_mae: 0.0077 - val_mse: 2.1091e-04\n",
      "Epoch 140/200\n",
      "2440/2440 [==============================] - 1s 590us/step - loss: 1.6782e-04 - mae: 0.0086 - mse: 1.6782e-04 - val_loss: 2.1120e-04 - val_mae: 0.0077 - val_mse: 2.1120e-04\n",
      "Epoch 141/200\n",
      "2440/2440 [==============================] - 2s 712us/step - loss: 1.6739e-04 - mae: 0.0086 - mse: 1.6739e-04 - val_loss: 2.1145e-04 - val_mae: 0.0078 - val_mse: 2.1145e-04\n",
      "Epoch 142/200\n",
      "2440/2440 [==============================] - 2s 615us/step - loss: 1.6710e-04 - mae: 0.0086 - mse: 1.6710e-04 - val_loss: 2.1167e-04 - val_mae: 0.0078 - val_mse: 2.1167e-04\n",
      "Epoch 143/200\n",
      "2440/2440 [==============================] - 2s 630us/step - loss: 1.6690e-04 - mae: 0.0086 - mse: 1.6690e-04 - val_loss: 2.1190e-04 - val_mae: 0.0078 - val_mse: 2.1190e-04\n",
      "Epoch 144/200\n",
      "2440/2440 [==============================] - 2s 655us/step - loss: 1.6674e-04 - mae: 0.0086 - mse: 1.6674e-04 - val_loss: 2.1215e-04 - val_mae: 0.0078 - val_mse: 2.1215e-04\n",
      "Epoch 145/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 1.6657e-04 - mae: 0.0086 - mse: 1.6657e-04 - val_loss: 2.1240e-04 - val_mae: 0.0078 - val_mse: 2.1240e-04\n",
      "Epoch 146/200\n",
      "2440/2440 [==============================] - 1s 601us/step - loss: 1.6639e-04 - mae: 0.0086 - mse: 1.6639e-04 - val_loss: 2.1265e-04 - val_mae: 0.0078 - val_mse: 2.1265e-04\n",
      "Epoch 147/200\n",
      "2440/2440 [==============================] - 1s 589us/step - loss: 1.6621e-04 - mae: 0.0086 - mse: 1.6621e-04 - val_loss: 2.1286e-04 - val_mae: 0.0078 - val_mse: 2.1286e-04\n",
      "Epoch 148/200\n",
      "2440/2440 [==============================] - 2s 641us/step - loss: 1.6605e-04 - mae: 0.0086 - mse: 1.6605e-04 - val_loss: 2.1302e-04 - val_mae: 0.0078 - val_mse: 2.1302e-04\n",
      "Epoch 149/200\n",
      "2440/2440 [==============================] - 2s 676us/step - loss: 1.6593e-04 - mae: 0.0085 - mse: 1.6593e-04 - val_loss: 2.1315e-04 - val_mae: 0.0078 - val_mse: 2.1315e-04\n",
      "Epoch 150/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 1.6583e-04 - mae: 0.0085 - mse: 1.6583e-04 - val_loss: 2.1324e-04 - val_mae: 0.0078 - val_mse: 2.1324e-04\n",
      "Epoch 151/200\n",
      "2440/2440 [==============================] - 2s 644us/step - loss: 1.6575e-04 - mae: 0.0085 - mse: 1.6575e-04 - val_loss: 2.1333e-04 - val_mae: 0.0078 - val_mse: 2.1333e-04\n",
      "Epoch 152/200\n",
      "2440/2440 [==============================] - 2s 716us/step - loss: 1.6565e-04 - mae: 0.0085 - mse: 1.6565e-04 - val_loss: 2.1349e-04 - val_mae: 0.0078 - val_mse: 2.1349e-04\n",
      "Epoch 153/200\n",
      "2440/2440 [==============================] - 2s 642us/step - loss: 1.6551e-04 - mae: 0.0085 - mse: 1.6551e-04 - val_loss: 2.1378e-04 - val_mae: 0.0078 - val_mse: 2.1378e-04\n",
      "Epoch 154/200\n",
      "2440/2440 [==============================] - 2s 634us/step - loss: 1.6534e-04 - mae: 0.0085 - mse: 1.6534e-04 - val_loss: 2.1421e-04 - val_mae: 0.0078 - val_mse: 2.1421e-04\n",
      "Epoch 155/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.6517e-04 - mae: 0.0085 - mse: 1.6517e-04 - val_loss: 2.1467e-04 - val_mae: 0.0078 - val_mse: 2.1467e-04\n",
      "Epoch 156/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 1.6502e-04 - mae: 0.0085 - mse: 1.6502e-04 - val_loss: 2.1503e-04 - val_mae: 0.0078 - val_mse: 2.1503e-04\n",
      "Epoch 157/200\n",
      "2440/2440 [==============================] - 2s 667us/step - loss: 1.6492e-04 - mae: 0.0085 - mse: 1.6492e-04 - val_loss: 2.1518e-04 - val_mae: 0.0078 - val_mse: 2.1518e-04\n",
      "Epoch 158/200\n",
      "2440/2440 [==============================] - 2s 621us/step - loss: 1.6487e-04 - mae: 0.0085 - mse: 1.6487e-04 - val_loss: 2.1517e-04 - val_mae: 0.0078 - val_mse: 2.1517e-04\n",
      "Epoch 159/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 1.6486e-04 - mae: 0.0085 - mse: 1.6486e-04 - val_loss: 2.1514e-04 - val_mae: 0.0078 - val_mse: 2.1514e-04\n",
      "Epoch 160/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 1.6484e-04 - mae: 0.0085 - mse: 1.6484e-04 - val_loss: 2.1534e-04 - val_mae: 0.0078 - val_mse: 2.1534e-04\n",
      "Epoch 161/200\n",
      "2440/2440 [==============================] - 2s 643us/step - loss: 1.6474e-04 - mae: 0.0085 - mse: 1.6474e-04 - val_loss: 2.1604e-04 - val_mae: 0.0078 - val_mse: 2.1604e-04\n",
      "Epoch 162/200\n",
      "2440/2440 [==============================] - 2s 624us/step - loss: 1.6447e-04 - mae: 0.0084 - mse: 1.6447e-04 - val_loss: 2.1726e-04 - val_mae: 0.0079 - val_mse: 2.1726e-04\n",
      "Epoch 163/200\n",
      "2440/2440 [==============================] - 2s 617us/step - loss: 1.6408e-04 - mae: 0.0084 - mse: 1.6408e-04 - val_loss: 2.1852e-04 - val_mae: 0.0079 - val_mse: 2.1852e-04\n",
      "Epoch 164/200\n",
      "2440/2440 [==============================] - 2s 620us/step - loss: 1.6368e-04 - mae: 0.0084 - mse: 1.6368e-04 - val_loss: 2.1930e-04 - val_mae: 0.0080 - val_mse: 2.1930e-04\n",
      "Epoch 165/200\n",
      "2440/2440 [==============================] - 2s 621us/step - loss: 1.6326e-04 - mae: 0.0084 - mse: 1.6326e-04 - val_loss: 2.1939e-04 - val_mae: 0.0079 - val_mse: 2.1939e-04\n",
      "Epoch 166/200\n",
      "2440/2440 [==============================] - 2s 626us/step - loss: 1.6278e-04 - mae: 0.0084 - mse: 1.6278e-04 - val_loss: 2.1893e-04 - val_mae: 0.0079 - val_mse: 2.1893e-04\n",
      "Epoch 167/200\n",
      "2440/2440 [==============================] - 2s 636us/step - loss: 1.6226e-04 - mae: 0.0084 - mse: 1.6226e-04 - val_loss: 2.1820e-04 - val_mae: 0.0079 - val_mse: 2.1820e-04\n",
      "Epoch 168/200\n",
      "2440/2440 [==============================] - 2s 647us/step - loss: 1.6182e-04 - mae: 0.0083 - mse: 1.6182e-04 - val_loss: 2.1763e-04 - val_mae: 0.0079 - val_mse: 2.1763e-04\n",
      "Epoch 169/200\n",
      "2440/2440 [==============================] - 2s 649us/step - loss: 1.6152e-04 - mae: 0.0083 - mse: 1.6152e-04 - val_loss: 2.1761e-04 - val_mae: 0.0079 - val_mse: 2.1761e-04\n",
      "Epoch 170/200\n",
      "2440/2440 [==============================] - 2s 670us/step - loss: 1.6135e-04 - mae: 0.0083 - mse: 1.6135e-04 - val_loss: 2.1814e-04 - val_mae: 0.0080 - val_mse: 2.1814e-04\n",
      "Epoch 171/200\n",
      "2440/2440 [==============================] - 2s 657us/step - loss: 1.6123e-04 - mae: 0.0083 - mse: 1.6123e-04 - val_loss: 2.1871e-04 - val_mae: 0.0080 - val_mse: 2.1871e-04\n",
      "Epoch 172/200\n",
      "2440/2440 [==============================] - 2s 631us/step - loss: 1.6115e-04 - mae: 0.0083 - mse: 1.6115e-04 - val_loss: 2.1888e-04 - val_mae: 0.0080 - val_mse: 2.1888e-04\n",
      "Epoch 173/200\n",
      "2440/2440 [==============================] - 1s 601us/step - loss: 1.6109e-04 - mae: 0.0083 - mse: 1.6109e-04 - val_loss: 2.1869e-04 - val_mae: 0.0080 - val_mse: 2.1869e-04\n",
      "Epoch 174/200\n",
      "2440/2440 [==============================] - 2s 617us/step - loss: 1.6099e-04 - mae: 0.0083 - mse: 1.6099e-04 - val_loss: 2.1846e-04 - val_mae: 0.0080 - val_mse: 2.1846e-04\n",
      "Epoch 175/200\n",
      "2440/2440 [==============================] - 2s 643us/step - loss: 1.6082e-04 - mae: 0.0083 - mse: 1.6082e-04 - val_loss: 2.1849e-04 - val_mae: 0.0080 - val_mse: 2.1849e-04\n",
      "Epoch 176/200\n",
      "2440/2440 [==============================] - 2s 671us/step - loss: 1.6058e-04 - mae: 0.0083 - mse: 1.6058e-04 - val_loss: 2.1890e-04 - val_mae: 0.0080 - val_mse: 2.1890e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200\n",
      "2440/2440 [==============================] - 1s 610us/step - loss: 1.6032e-04 - mae: 0.0083 - mse: 1.6032e-04 - val_loss: 2.1951e-04 - val_mae: 0.0080 - val_mse: 2.1951e-04\n",
      "Epoch 178/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 1.6007e-04 - mae: 0.0083 - mse: 1.6007e-04 - val_loss: 2.1976e-04 - val_mae: 0.0080 - val_mse: 2.1976e-04\n",
      "Epoch 179/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 1.5979e-04 - mae: 0.0082 - mse: 1.5979e-04 - val_loss: 2.1935e-04 - val_mae: 0.0080 - val_mse: 2.1935e-04\n",
      "Epoch 180/200\n",
      "2440/2440 [==============================] - 2s 674us/step - loss: 1.5937e-04 - mae: 0.0082 - mse: 1.5937e-04 - val_loss: 2.1880e-04 - val_mae: 0.0080 - val_mse: 2.1880e-04\n",
      "Epoch 181/200\n",
      "2440/2440 [==============================] - 1s 602us/step - loss: 1.5886e-04 - mae: 0.0082 - mse: 1.5886e-04 - val_loss: 2.1872e-04 - val_mae: 0.0080 - val_mse: 2.1872e-04\n",
      "Epoch 182/200\n",
      "2440/2440 [==============================] - 1s 599us/step - loss: 1.5835e-04 - mae: 0.0082 - mse: 1.5835e-04 - val_loss: 2.1894e-04 - val_mae: 0.0080 - val_mse: 2.1894e-04\n",
      "Epoch 183/200\n",
      "2440/2440 [==============================] - 1s 597us/step - loss: 1.5799e-04 - mae: 0.0082 - mse: 1.5799e-04 - val_loss: 2.1887e-04 - val_mae: 0.0080 - val_mse: 2.1887e-04\n",
      "Epoch 184/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.5791e-04 - mae: 0.0081 - mse: 1.5791e-04 - val_loss: 2.1831e-04 - val_mae: 0.0080 - val_mse: 2.1831e-04\n",
      "Epoch 185/200\n",
      "2440/2440 [==============================] - 2s 636us/step - loss: 1.5799e-04 - mae: 0.0081 - mse: 1.5799e-04 - val_loss: 2.1777e-04 - val_mae: 0.0080 - val_mse: 2.1777e-04\n",
      "Epoch 186/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.5804e-04 - mae: 0.0081 - mse: 1.5804e-04 - val_loss: 2.1785e-04 - val_mae: 0.0080 - val_mse: 2.1785e-04\n",
      "Epoch 187/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.5803e-04 - mae: 0.0081 - mse: 1.5803e-04 - val_loss: 2.1876e-04 - val_mae: 0.0080 - val_mse: 2.1876e-04\n",
      "Epoch 188/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 1.5813e-04 - mae: 0.0081 - mse: 1.5813e-04 - val_loss: 2.2036e-04 - val_mae: 0.0081 - val_mse: 2.2036e-04\n",
      "Epoch 189/200\n",
      "2440/2440 [==============================] - 1s 604us/step - loss: 1.5844e-04 - mae: 0.0082 - mse: 1.5844e-04 - val_loss: 2.2215e-04 - val_mae: 0.0082 - val_mse: 2.2215e-04\n",
      "Epoch 190/200\n",
      "2440/2440 [==============================] - 2s 624us/step - loss: 1.5886e-04 - mae: 0.0082 - mse: 1.5886e-04 - val_loss: 2.2380e-04 - val_mae: 0.0083 - val_mse: 2.2380e-04\n",
      "Epoch 191/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.5935e-04 - mae: 0.0082 - mse: 1.5935e-04 - val_loss: 2.2547e-04 - val_mae: 0.0084 - val_mse: 2.2547e-04\n",
      "Epoch 192/200\n",
      "2440/2440 [==============================] - 1s 598us/step - loss: 1.5991e-04 - mae: 0.0082 - mse: 1.5991e-04 - val_loss: 2.2753e-04 - val_mae: 0.0085 - val_mse: 2.2753e-04\n",
      "Epoch 193/200\n",
      "2440/2440 [==============================] - 1s 602us/step - loss: 1.6039e-04 - mae: 0.0083 - mse: 1.6039e-04 - val_loss: 2.3033e-04 - val_mae: 0.0086 - val_mse: 2.3033e-04\n",
      "Epoch 194/200\n",
      "2440/2440 [==============================] - 1s 600us/step - loss: 1.6081e-04 - mae: 0.0083 - mse: 1.6081e-04 - val_loss: 2.3420e-04 - val_mae: 0.0088 - val_mse: 2.3420e-04\n",
      "Epoch 195/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 1.6132e-04 - mae: 0.0083 - mse: 1.6132e-04 - val_loss: 2.3922e-04 - val_mae: 0.0091 - val_mse: 2.3922e-04\n",
      "Epoch 196/200\n",
      "2440/2440 [==============================] - 2s 626us/step - loss: 1.6221e-04 - mae: 0.0084 - mse: 1.6221e-04 - val_loss: 2.4487e-04 - val_mae: 0.0093 - val_mse: 2.4487e-04\n",
      "Epoch 197/200\n",
      "2440/2440 [==============================] - 2s 651us/step - loss: 1.6351e-04 - mae: 0.0084 - mse: 1.6351e-04 - val_loss: 2.5076e-04 - val_mae: 0.0096 - val_mse: 2.5076e-04\n",
      "Epoch 198/200\n",
      "2440/2440 [==============================] - 1s 600us/step - loss: 1.6487e-04 - mae: 0.0085 - mse: 1.6487e-04 - val_loss: 2.5756e-04 - val_mae: 0.0100 - val_mse: 2.5756e-04\n",
      "Epoch 199/200\n",
      "2440/2440 [==============================] - 1s 601us/step - loss: 1.6605e-04 - mae: 0.0085 - mse: 1.6605e-04 - val_loss: 2.6632e-04 - val_mae: 0.0104 - val_mse: 2.6632e-04\n",
      "Epoch 200/200\n",
      "2440/2440 [==============================] - 1s 604us/step - loss: 1.6750e-04 - mae: 0.0086 - mse: 1.6750e-04 - val_loss: 2.7706e-04 - val_mae: 0.0109 - val_mse: 2.7706e-04\n"
     ]
    }
   ],
   "source": [
    "# Use same data from earlier to fit the model\n",
    "slstmhistory = slstm.fit(X_train_LSTM, y_train_LSTM,\n",
    "                       epochs=EPOCHS,\n",
    "                       validation_data=(X_test_LSTM, y_test_LSTM),\n",
    "                       verbose=1,\n",
    "                       shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacked LSTM Model WITHOUT Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4222 samples, validate on 1056 samples\n",
      "Epoch 1/200\n",
      "4222/4222 [==============================] - 1s 328us/step - loss: 0.0335 - mae: 0.1160 - mse: 0.0335 - val_loss: 0.0243 - val_mae: 0.0976 - val_mse: 0.0243\n",
      "Epoch 2/200\n",
      "4222/4222 [==============================] - 1s 248us/step - loss: 0.0229 - mae: 0.0836 - mse: 0.0229 - val_loss: 0.0203 - val_mae: 0.0802 - val_mse: 0.0203\n",
      "Epoch 3/200\n",
      "4222/4222 [==============================] - 1s 243us/step - loss: 0.0197 - mae: 0.0712 - mse: 0.0197 - val_loss: 0.0183 - val_mae: 0.0719 - val_mse: 0.0183\n",
      "Epoch 4/200\n",
      "4222/4222 [==============================] - 1s 243us/step - loss: 0.0183 - mae: 0.0670 - mse: 0.0183 - val_loss: 0.0166 - val_mae: 0.0696 - val_mse: 0.0166\n",
      "Epoch 5/200\n",
      "4222/4222 [==============================] - 1s 250us/step - loss: 0.0168 - mae: 0.0640 - mse: 0.0168 - val_loss: 0.0150 - val_mae: 0.0691 - val_mse: 0.0150\n",
      "Epoch 6/200\n",
      "4222/4222 [==============================] - 1s 262us/step - loss: 0.0158 - mae: 0.0616 - mse: 0.0158 - val_loss: 0.0139 - val_mae: 0.0635 - val_mse: 0.0139\n",
      "Epoch 7/200\n",
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0152 - mae: 0.0600 - mse: 0.0152 - val_loss: 0.0134 - val_mae: 0.0610 - val_mse: 0.0134\n",
      "Epoch 8/200\n",
      "4222/4222 [==============================] - 1s 244us/step - loss: 0.0149 - mae: 0.0595 - mse: 0.0149 - val_loss: 0.0131 - val_mae: 0.0615 - val_mse: 0.0131\n",
      "Epoch 9/200\n",
      "4222/4222 [==============================] - 1s 250us/step - loss: 0.0147 - mae: 0.0592 - mse: 0.0147 - val_loss: 0.0129 - val_mae: 0.0630 - val_mse: 0.0129\n",
      "Epoch 10/200\n",
      "4222/4222 [==============================] - 1s 254us/step - loss: 0.0144 - mae: 0.0586 - mse: 0.0144 - val_loss: 0.0128 - val_mae: 0.0632 - val_mse: 0.0128\n",
      "Epoch 11/200\n",
      "4222/4222 [==============================] - 1s 266us/step - loss: 0.0141 - mae: 0.0578 - mse: 0.0141 - val_loss: 0.0126 - val_mae: 0.0625 - val_mse: 0.0126\n",
      "Epoch 12/200\n",
      "4222/4222 [==============================] - 1s 240us/step - loss: 0.0139 - mae: 0.0571 - mse: 0.0139 - val_loss: 0.0124 - val_mae: 0.0619 - val_mse: 0.0124\n",
      "Epoch 13/200\n",
      "4222/4222 [==============================] - 1s 253us/step - loss: 0.0137 - mae: 0.0566 - mse: 0.0137 - val_loss: 0.0122 - val_mae: 0.0611 - val_mse: 0.0122\n",
      "Epoch 14/200\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0135 - mae: 0.0561 - mse: 0.0135 - val_loss: 0.0119 - val_mae: 0.0594 - val_mse: 0.0119\n",
      "Epoch 15/200\n",
      "4222/4222 [==============================] - 1s 256us/step - loss: 0.0133 - mae: 0.0553 - mse: 0.0133 - val_loss: 0.0115 - val_mae: 0.0563 - val_mse: 0.0115\n",
      "Epoch 16/200\n",
      "4222/4222 [==============================] - 1s 245us/step - loss: 0.0130 - mae: 0.0544 - mse: 0.0130 - val_loss: 0.0112 - val_mae: 0.0533 - val_mse: 0.0112\n",
      "Epoch 17/200\n",
      "4222/4222 [==============================] - 1s 249us/step - loss: 0.0128 - mae: 0.0533 - mse: 0.0128 - val_loss: 0.0110 - val_mae: 0.0514 - val_mse: 0.0110\n",
      "Epoch 18/200\n",
      "4222/4222 [==============================] - 1s 264us/step - loss: 0.0126 - mae: 0.0524 - mse: 0.0126 - val_loss: 0.0108 - val_mae: 0.0505 - val_mse: 0.0108\n",
      "Epoch 19/200\n",
      "4222/4222 [==============================] - 1s 262us/step - loss: 0.0124 - mae: 0.0515 - mse: 0.0124 - val_loss: 0.0106 - val_mae: 0.0503 - val_mse: 0.0106\n",
      "Epoch 20/200\n",
      "4222/4222 [==============================] - 1s 248us/step - loss: 0.0122 - mae: 0.0509 - mse: 0.0122 - val_loss: 0.0104 - val_mae: 0.0509 - val_mse: 0.0104\n",
      "Epoch 21/200\n",
      "4222/4222 [==============================] - 1s 259us/step - loss: 0.0119 - mae: 0.0503 - mse: 0.0119 - val_loss: 0.0103 - val_mae: 0.0517 - val_mse: 0.0103\n",
      "Epoch 22/200\n",
      "4222/4222 [==============================] - 1s 244us/step - loss: 0.0117 - mae: 0.0496 - mse: 0.0117 - val_loss: 0.0101 - val_mae: 0.0516 - val_mse: 0.0101\n",
      "Epoch 23/200\n",
      "4222/4222 [==============================] - 1s 247us/step - loss: 0.0115 - mae: 0.0487 - mse: 0.0115 - val_loss: 0.0099 - val_mae: 0.0504 - val_mse: 0.0099\n",
      "Epoch 24/200\n",
      "4222/4222 [==============================] - 1s 245us/step - loss: 0.0113 - mae: 0.0479 - mse: 0.0113 - val_loss: 0.0096 - val_mae: 0.0490 - val_mse: 0.0096\n",
      "Epoch 25/200\n",
      "4222/4222 [==============================] - 1s 243us/step - loss: 0.0110 - mae: 0.0471 - mse: 0.0110 - val_loss: 0.0094 - val_mae: 0.0476 - val_mse: 0.0094\n",
      "Epoch 26/200\n",
      "4222/4222 [==============================] - 1s 245us/step - loss: 0.0107 - mae: 0.0463 - mse: 0.0107 - val_loss: 0.0091 - val_mae: 0.0466 - val_mse: 0.0091\n",
      "Epoch 27/200\n",
      "4222/4222 [==============================] - 1s 251us/step - loss: 0.0105 - mae: 0.0453 - mse: 0.0105 - val_loss: 0.0089 - val_mae: 0.0464 - val_mse: 0.0089\n",
      "Epoch 28/200\n",
      "4222/4222 [==============================] - 1s 280us/step - loss: 0.0103 - mae: 0.0449 - mse: 0.0103 - val_loss: 0.0088 - val_mae: 0.0463 - val_mse: 0.0088\n",
      "Epoch 29/200\n",
      "4222/4222 [==============================] - 1s 269us/step - loss: 0.0102 - mae: 0.0448 - mse: 0.0102 - val_loss: 0.0086 - val_mae: 0.0461 - val_mse: 0.0086\n",
      "Epoch 30/200\n",
      "4222/4222 [==============================] - 1s 274us/step - loss: 0.0100 - mae: 0.0447 - mse: 0.0100 - val_loss: 0.0085 - val_mae: 0.0459 - val_mse: 0.0085\n",
      "Epoch 31/200\n",
      "4222/4222 [==============================] - 1s 274us/step - loss: 0.0099 - mae: 0.0445 - mse: 0.0099 - val_loss: 0.0085 - val_mae: 0.0457 - val_mse: 0.0085\n",
      "Epoch 32/200\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0098 - mae: 0.0442 - mse: 0.0098 - val_loss: 0.0084 - val_mae: 0.0455 - val_mse: 0.0084\n",
      "Epoch 33/200\n",
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0096 - mae: 0.0441 - mse: 0.0096 - val_loss: 0.0083 - val_mae: 0.0453 - val_mse: 0.0083\n",
      "Epoch 34/200\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0095 - mae: 0.0439 - mse: 0.0095 - val_loss: 0.0081 - val_mae: 0.0450 - val_mse: 0.0081\n",
      "Epoch 35/200\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0094 - mae: 0.0437 - mse: 0.0094 - val_loss: 0.0080 - val_mae: 0.0446 - val_mse: 0.0080\n",
      "Epoch 36/200\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0092 - mae: 0.0434 - mse: 0.0092 - val_loss: 0.0079 - val_mae: 0.0440 - val_mse: 0.0079\n",
      "Epoch 37/200\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0090 - mae: 0.0429 - mse: 0.0090 - val_loss: 0.0079 - val_mae: 0.0456 - val_mse: 0.0079\n",
      "Epoch 38/200\n",
      "4222/4222 [==============================] - 1s 292us/step - loss: 0.0086 - mae: 0.0425 - mse: 0.0086 - val_loss: 0.0078 - val_mae: 0.0484 - val_mse: 0.0078\n",
      "Epoch 39/200\n",
      "4222/4222 [==============================] - 1s 288us/step - loss: 0.0083 - mae: 0.0421 - mse: 0.0083 - val_loss: 0.0078 - val_mae: 0.0489 - val_mse: 0.0078\n",
      "Epoch 40/200\n",
      "4222/4222 [==============================] - 1s 320us/step - loss: 0.0079 - mae: 0.0411 - mse: 0.0079 - val_loss: 0.0077 - val_mae: 0.0461 - val_mse: 0.0077\n",
      "Epoch 41/200\n",
      "4222/4222 [==============================] - 1s 298us/step - loss: 0.0076 - mae: 0.0400 - mse: 0.0076 - val_loss: 0.0074 - val_mae: 0.0448 - val_mse: 0.0074\n",
      "Epoch 42/200\n",
      "4222/4222 [==============================] - 1s 276us/step - loss: 0.0073 - mae: 0.0392 - mse: 0.0073 - val_loss: 0.0072 - val_mae: 0.0444 - val_mse: 0.0072\n",
      "Epoch 43/200\n",
      "4222/4222 [==============================] - 1s 275us/step - loss: 0.0070 - mae: 0.0383 - mse: 0.0070 - val_loss: 0.0070 - val_mae: 0.0435 - val_mse: 0.0070\n",
      "Epoch 44/200\n",
      "4222/4222 [==============================] - 1s 288us/step - loss: 0.0067 - mae: 0.0378 - mse: 0.0067 - val_loss: 0.0067 - val_mae: 0.0416 - val_mse: 0.0067\n",
      "Epoch 45/200\n",
      "4222/4222 [==============================] - 1s 278us/step - loss: 0.0063 - mae: 0.0370 - mse: 0.0063 - val_loss: 0.0063 - val_mae: 0.0392 - val_mse: 0.0063\n",
      "Epoch 46/200\n",
      "4222/4222 [==============================] - 1s 285us/step - loss: 0.0061 - mae: 0.0362 - mse: 0.0061 - val_loss: 0.0062 - val_mae: 0.0377 - val_mse: 0.0062\n",
      "Epoch 47/200\n",
      "4222/4222 [==============================] - 1s 282us/step - loss: 0.0057 - mae: 0.0346 - mse: 0.0057 - val_loss: 0.0059 - val_mae: 0.0380 - val_mse: 0.0059\n",
      "Epoch 48/200\n",
      "4222/4222 [==============================] - 1s 288us/step - loss: 0.0055 - mae: 0.0342 - mse: 0.0055 - val_loss: 0.0058 - val_mae: 0.0373 - val_mse: 0.0058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0051 - mae: 0.0328 - mse: 0.0051 - val_loss: 0.0059 - val_mae: 0.0398 - val_mse: 0.0059\n",
      "Epoch 50/200\n",
      "1088/4222 [======>.......................] - ETA: 0s - loss: 0.0069 - mae: 0.0346 - mse: 0.0069    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-d1be7cc70567>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                      \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_LSTM_nomob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_LSTM_nomob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                                      shuffle=False)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reuse No-mobility Data\n",
    "# X_LSTM_nomob, y_LSTM_nomob = get_LSTM_states_dataset(state_cases_df, n_steps=N_STEPS)\n",
    "# X_train_LSTM_nomob, X_test_LSTM_nomob, y_train_LSTM_nomob, y_test_LSTM_nomob = train_test_split(X_LSTM_nomob, y_LSTM_nomob, test_size=SPLIT, shuffle=False)\n",
    "\n",
    "# Set up model\n",
    "n_lstm_features = X_train_LSTM_nomob.shape[2]\n",
    "slstm_nomob = get_lstm(N_STEPS, n_lstm_features)\n",
    "\n",
    "# Fit LSTM\n",
    "slstm_nomob_history = slstm_nomob.fit(X_train_LSTM_nomob, y_train_LSTM_nomob,\n",
    "                                     epochs=EPOCHS,\n",
    "                                     validation_data=(X_test_LSTM_nomob, y_test_LSTM_nomob),\n",
    "                                     verbose=1,\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Model with Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CNN, Boo's model\n",
    "def get_CNN_model(n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, \n",
    "                     activation='relu', \n",
    "                     input_shape=(n_steps, n_features)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "n_CNN_features = X_train_CNN.shape[2]\n",
    "cnn = get_CNN_model(N_STEPS, n_CNN_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2440 samples, validate on 610 samples\n",
      "Epoch 1/200\n",
      "2440/2440 [==============================] - 0s 108us/step - loss: 0.0267 - mae: 0.1135 - mse: 0.0267 - val_loss: 0.0038 - val_mae: 0.0474 - val_mse: 0.0038\n",
      "Epoch 2/200\n",
      "2440/2440 [==============================] - 0s 65us/step - loss: 0.0034 - mae: 0.0444 - mse: 0.0034 - val_loss: 0.0024 - val_mae: 0.0364 - val_mse: 0.0024\n",
      "Epoch 3/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 0.0024 - mae: 0.0375 - mse: 0.0024 - val_loss: 0.0020 - val_mae: 0.0317 - val_mse: 0.0020\n",
      "Epoch 4/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 0.0020 - mae: 0.0337 - mse: 0.0020 - val_loss: 0.0018 - val_mae: 0.0304 - val_mse: 0.0018\n",
      "Epoch 5/200\n",
      "2440/2440 [==============================] - 0s 66us/step - loss: 0.0016 - mae: 0.0303 - mse: 0.0016 - val_loss: 0.0016 - val_mae: 0.0290 - val_mse: 0.0016\n",
      "Epoch 6/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 0.0015 - mae: 0.0286 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0386 - val_mse: 0.0021\n",
      "Epoch 7/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 0.0014 - mae: 0.0277 - mse: 0.0014 - val_loss: 0.0014 - val_mae: 0.0279 - val_mse: 0.0014\n",
      "Epoch 8/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 0.0013 - mae: 0.0268 - mse: 0.0013 - val_loss: 0.0012 - val_mae: 0.0250 - val_mse: 0.0012\n",
      "Epoch 9/200\n",
      "2440/2440 [==============================] - 0s 66us/step - loss: 0.0012 - mae: 0.0258 - mse: 0.0012 - val_loss: 0.0012 - val_mae: 0.0251 - val_mse: 0.0012\n",
      "Epoch 10/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 0.0011 - mae: 0.0244 - mse: 0.0011 - val_loss: 0.0011 - val_mae: 0.0244 - val_mse: 0.0011\n",
      "Epoch 11/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 0.0010 - mae: 0.0233 - mse: 0.0010 - val_loss: 0.0011 - val_mae: 0.0248 - val_mse: 0.0011\n",
      "Epoch 12/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 9.4084e-04 - mae: 0.0223 - mse: 9.4084e-04 - val_loss: 0.0011 - val_mae: 0.0244 - val_mse: 0.0011\n",
      "Epoch 13/200\n",
      "2440/2440 [==============================] - 0s 61us/step - loss: 8.8684e-04 - mae: 0.0216 - mse: 8.8684e-04 - val_loss: 0.0011 - val_mae: 0.0248 - val_mse: 0.0011\n",
      "Epoch 14/200\n",
      "2440/2440 [==============================] - 0s 63us/step - loss: 8.4703e-04 - mae: 0.0211 - mse: 8.4703e-04 - val_loss: 0.0011 - val_mae: 0.0245 - val_mse: 0.0011\n",
      "Epoch 15/200\n",
      "2440/2440 [==============================] - 0s 61us/step - loss: 8.0035e-04 - mae: 0.0205 - mse: 8.0035e-04 - val_loss: 0.0011 - val_mae: 0.0243 - val_mse: 0.0011\n",
      "Epoch 16/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 7.6493e-04 - mae: 0.0200 - mse: 7.6493e-04 - val_loss: 0.0010 - val_mae: 0.0242 - val_mse: 0.0010\n",
      "Epoch 17/200\n",
      "2440/2440 [==============================] - 0s 65us/step - loss: 7.3219e-04 - mae: 0.0195 - mse: 7.3219e-04 - val_loss: 0.0010 - val_mae: 0.0245 - val_mse: 0.0010\n",
      "Epoch 18/200\n",
      "2440/2440 [==============================] - 0s 61us/step - loss: 7.0408e-04 - mae: 0.0191 - mse: 7.0408e-04 - val_loss: 0.0010 - val_mae: 0.0244 - val_mse: 0.0010\n",
      "Epoch 19/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 6.7420e-04 - mae: 0.0187 - mse: 6.7420e-04 - val_loss: 0.0011 - val_mae: 0.0262 - val_mse: 0.0011\n",
      "Epoch 20/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 6.3678e-04 - mae: 0.0181 - mse: 6.3678e-04 - val_loss: 0.0011 - val_mae: 0.0259 - val_mse: 0.0011\n",
      "Epoch 21/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 6.1025e-04 - mae: 0.0176 - mse: 6.1025e-04 - val_loss: 0.0011 - val_mae: 0.0263 - val_mse: 0.0011\n",
      "Epoch 22/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 5.8902e-04 - mae: 0.0173 - mse: 5.8902e-04 - val_loss: 0.0011 - val_mae: 0.0262 - val_mse: 0.0011\n",
      "Epoch 23/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 5.7471e-04 - mae: 0.0171 - mse: 5.7471e-04 - val_loss: 0.0011 - val_mae: 0.0260 - val_mse: 0.0011\n",
      "Epoch 24/200\n",
      "2440/2440 [==============================] - 0s 76us/step - loss: 5.5774e-04 - mae: 0.0169 - mse: 5.5774e-04 - val_loss: 0.0010 - val_mae: 0.0255 - val_mse: 0.0010\n",
      "Epoch 25/200\n",
      "2440/2440 [==============================] - 0s 63us/step - loss: 5.4313e-04 - mae: 0.0166 - mse: 5.4313e-04 - val_loss: 0.0010 - val_mae: 0.0252 - val_mse: 0.0010\n",
      "Epoch 26/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 5.3249e-04 - mae: 0.0165 - mse: 5.3249e-04 - val_loss: 0.0010 - val_mae: 0.0259 - val_mse: 0.0010\n",
      "Epoch 27/200\n",
      "2440/2440 [==============================] - 0s 65us/step - loss: 5.2420e-04 - mae: 0.0165 - mse: 5.2420e-04 - val_loss: 0.0010 - val_mae: 0.0253 - val_mse: 0.0010\n",
      "Epoch 28/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 5.0742e-04 - mae: 0.0162 - mse: 5.0742e-04 - val_loss: 0.0010 - val_mae: 0.0253 - val_mse: 0.0010\n",
      "Epoch 29/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 4.9815e-04 - mae: 0.0161 - mse: 4.9815e-04 - val_loss: 0.0010 - val_mae: 0.0254 - val_mse: 0.0010\n",
      "Epoch 30/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 4.8366e-04 - mae: 0.0158 - mse: 4.8366e-04 - val_loss: 9.4475e-04 - val_mae: 0.0243 - val_mse: 9.4475e-04\n",
      "Epoch 31/200\n",
      "2440/2440 [==============================] - 0s 51us/step - loss: 4.6283e-04 - mae: 0.0154 - mse: 4.6283e-04 - val_loss: 9.3035e-04 - val_mae: 0.0241 - val_mse: 9.3035e-04\n",
      "Epoch 32/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 4.4971e-04 - mae: 0.0151 - mse: 4.4971e-04 - val_loss: 8.9420e-04 - val_mae: 0.0233 - val_mse: 8.9420e-04\n",
      "Epoch 33/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 4.3644e-04 - mae: 0.0148 - mse: 4.3644e-04 - val_loss: 8.4224e-04 - val_mae: 0.0224 - val_mse: 8.4224e-04\n",
      "Epoch 34/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 4.2672e-04 - mae: 0.0146 - mse: 4.2672e-04 - val_loss: 8.7650e-04 - val_mae: 0.0230 - val_mse: 8.7650e-04\n",
      "Epoch 35/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 4.2419e-04 - mae: 0.0146 - mse: 4.2419e-04 - val_loss: 8.4441e-04 - val_mae: 0.0223 - val_mse: 8.4441e-04\n",
      "Epoch 36/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 4.0706e-04 - mae: 0.0143 - mse: 4.0706e-04 - val_loss: 8.5440e-04 - val_mae: 0.0226 - val_mse: 8.5440e-04\n",
      "Epoch 37/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 4.0627e-04 - mae: 0.0144 - mse: 4.0627e-04 - val_loss: 8.3399e-04 - val_mae: 0.0221 - val_mse: 8.3399e-04\n",
      "Epoch 38/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 3.9579e-04 - mae: 0.0142 - mse: 3.9579e-04 - val_loss: 7.8542e-04 - val_mae: 0.0206 - val_mse: 7.8542e-04\n",
      "Epoch 39/200\n",
      "2440/2440 [==============================] - 0s 51us/step - loss: 3.9944e-04 - mae: 0.0142 - mse: 3.9944e-04 - val_loss: 8.3385e-04 - val_mae: 0.0220 - val_mse: 8.3385e-04\n",
      "Epoch 40/200\n",
      "2440/2440 [==============================] - 0s 50us/step - loss: 3.7382e-04 - mae: 0.0139 - mse: 3.7382e-04 - val_loss: 8.3130e-04 - val_mae: 0.0222 - val_mse: 8.3130e-04\n",
      "Epoch 41/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 3.7333e-04 - mae: 0.0139 - mse: 3.7333e-04 - val_loss: 8.3000e-04 - val_mae: 0.0218 - val_mse: 8.3000e-04\n",
      "Epoch 42/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 3.6585e-04 - mae: 0.0137 - mse: 3.6585e-04 - val_loss: 8.1186e-04 - val_mae: 0.0214 - val_mse: 8.1186e-04\n",
      "Epoch 43/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.5910e-04 - mae: 0.0136 - mse: 3.5910e-04 - val_loss: 8.0275e-04 - val_mae: 0.0212 - val_mse: 8.0275e-04\n",
      "Epoch 44/200\n",
      "2440/2440 [==============================] - 0s 60us/step - loss: 3.5779e-04 - mae: 0.0136 - mse: 3.5779e-04 - val_loss: 8.0697e-04 - val_mae: 0.0216 - val_mse: 8.0697e-04\n",
      "Epoch 45/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 3.5519e-04 - mae: 0.0136 - mse: 3.5519e-04 - val_loss: 8.1694e-04 - val_mae: 0.0216 - val_mse: 8.1694e-04\n",
      "Epoch 46/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 3.5244e-04 - mae: 0.0136 - mse: 3.5244e-04 - val_loss: 8.1217e-04 - val_mae: 0.0213 - val_mse: 8.1217e-04\n",
      "Epoch 47/200\n",
      "2440/2440 [==============================] - 0s 49us/step - loss: 3.4364e-04 - mae: 0.0134 - mse: 3.4364e-04 - val_loss: 7.9681e-04 - val_mae: 0.0212 - val_mse: 7.9681e-04\n",
      "Epoch 48/200\n",
      "2440/2440 [==============================] - 0s 51us/step - loss: 3.2977e-04 - mae: 0.0131 - mse: 3.2977e-04 - val_loss: 7.7412e-04 - val_mae: 0.0206 - val_mse: 7.7412e-04\n",
      "Epoch 49/200\n",
      "2440/2440 [==============================] - 0s 63us/step - loss: 3.3023e-04 - mae: 0.0131 - mse: 3.3023e-04 - val_loss: 8.2141e-04 - val_mae: 0.0216 - val_mse: 8.2141e-04\n",
      "Epoch 50/200\n",
      "2440/2440 [==============================] - 0s 49us/step - loss: 3.2016e-04 - mae: 0.0129 - mse: 3.2016e-04 - val_loss: 7.8168e-04 - val_mae: 0.0206 - val_mse: 7.8168e-04\n",
      "Epoch 51/200\n",
      "2440/2440 [==============================] - 0s 50us/step - loss: 3.1616e-04 - mae: 0.0126 - mse: 3.1616e-04 - val_loss: 7.4501e-04 - val_mae: 0.0194 - val_mse: 7.4501e-04\n",
      "Epoch 52/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 3.0873e-04 - mae: 0.0124 - mse: 3.0873e-04 - val_loss: 7.7168e-04 - val_mae: 0.0199 - val_mse: 7.7168e-04\n",
      "Epoch 53/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 3.0468e-04 - mae: 0.0124 - mse: 3.0468e-04 - val_loss: 7.5457e-04 - val_mae: 0.0193 - val_mse: 7.5457e-04\n",
      "Epoch 54/200\n",
      "2440/2440 [==============================] - 0s 51us/step - loss: 2.8914e-04 - mae: 0.0119 - mse: 2.8914e-04 - val_loss: 7.3500e-04 - val_mae: 0.0189 - val_mse: 7.3500e-04\n",
      "Epoch 55/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 2.9267e-04 - mae: 0.0120 - mse: 2.9267e-04 - val_loss: 7.2984e-04 - val_mae: 0.0182 - val_mse: 7.2984e-04\n",
      "Epoch 56/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.8550e-04 - mae: 0.0118 - mse: 2.8550e-04 - val_loss: 7.3713e-04 - val_mae: 0.0186 - val_mse: 7.3713e-04\n",
      "Epoch 57/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.8631e-04 - mae: 0.0119 - mse: 2.8631e-04 - val_loss: 7.8664e-04 - val_mae: 0.0203 - val_mse: 7.8664e-04\n",
      "Epoch 58/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 2.7877e-04 - mae: 0.0118 - mse: 2.7877e-04 - val_loss: 7.7783e-04 - val_mae: 0.0199 - val_mse: 7.7783e-04\n",
      "Epoch 59/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.7946e-04 - mae: 0.0119 - mse: 2.7946e-04 - val_loss: 7.5885e-04 - val_mae: 0.0189 - val_mse: 7.5885e-04\n",
      "Epoch 60/200\n",
      "2440/2440 [==============================] - 0s 84us/step - loss: 2.7370e-04 - mae: 0.0117 - mse: 2.7370e-04 - val_loss: 7.6886e-04 - val_mae: 0.0193 - val_mse: 7.6886e-04\n",
      "Epoch 61/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 2.6798e-04 - mae: 0.0114 - mse: 2.6798e-04 - val_loss: 7.5597e-04 - val_mae: 0.0188 - val_mse: 7.5597e-04\n",
      "Epoch 62/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 2.7310e-04 - mae: 0.0117 - mse: 2.7310e-04 - val_loss: 7.6212e-04 - val_mae: 0.0189 - val_mse: 7.6212e-04\n",
      "Epoch 63/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.6606e-04 - mae: 0.0115 - mse: 2.6606e-04 - val_loss: 7.6325e-04 - val_mae: 0.0184 - val_mse: 7.6325e-04\n",
      "Epoch 64/200\n",
      "2440/2440 [==============================] - 0s 67us/step - loss: 2.5862e-04 - mae: 0.0114 - mse: 2.5862e-04 - val_loss: 7.5988e-04 - val_mae: 0.0182 - val_mse: 7.5988e-04\n",
      "Epoch 65/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 2.5659e-04 - mae: 0.0113 - mse: 2.5659e-04 - val_loss: 7.6556e-04 - val_mae: 0.0185 - val_mse: 7.6556e-04\n",
      "Epoch 66/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.5704e-04 - mae: 0.0114 - mse: 2.5704e-04 - val_loss: 8.1102e-04 - val_mae: 0.0194 - val_mse: 8.1102e-04\n",
      "Epoch 67/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.5968e-04 - mae: 0.0116 - mse: 2.5968e-04 - val_loss: 7.8391e-04 - val_mae: 0.0186 - val_mse: 7.8391e-04\n",
      "Epoch 68/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.6710e-04 - mae: 0.0119 - mse: 2.6710e-04 - val_loss: 8.8375e-04 - val_mae: 0.0205 - val_mse: 8.8375e-04\n",
      "Epoch 69/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.9693e-04 - mae: 0.0129 - mse: 2.9693e-04 - val_loss: 8.6822e-04 - val_mae: 0.0199 - val_mse: 8.6822e-04\n",
      "Epoch 70/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.9695e-04 - mae: 0.0126 - mse: 2.9695e-04 - val_loss: 9.1900e-04 - val_mae: 0.0199 - val_mse: 9.1900e-04\n",
      "Epoch 71/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 3.2127e-04 - mae: 0.0134 - mse: 3.2127e-04 - val_loss: 8.6895e-04 - val_mae: 0.0191 - val_mse: 8.6895e-04\n",
      "Epoch 72/200\n",
      "2440/2440 [==============================] - 0s 70us/step - loss: 3.0102e-04 - mae: 0.0130 - mse: 3.0102e-04 - val_loss: 9.4502e-04 - val_mae: 0.0204 - val_mse: 9.4502e-04\n",
      "Epoch 73/200\n",
      "2440/2440 [==============================] - 0s 65us/step - loss: 3.3820e-04 - mae: 0.0142 - mse: 3.3820e-04 - val_loss: 8.5479e-04 - val_mae: 0.0201 - val_mse: 8.5479e-04\n",
      "Epoch 74/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 3.4536e-04 - mae: 0.0144 - mse: 3.4536e-04 - val_loss: 9.2662e-04 - val_mae: 0.0210 - val_mse: 9.2662e-04\n",
      "Epoch 75/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.0426e-04 - mae: 0.0133 - mse: 3.0426e-04 - val_loss: 9.4682e-04 - val_mae: 0.0214 - val_mse: 9.4682e-04\n",
      "Epoch 76/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 3.7767e-04 - mae: 0.0148 - mse: 3.7767e-04 - val_loss: 9.1638e-04 - val_mae: 0.0213 - val_mse: 9.1638e-04\n",
      "Epoch 77/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 3.4355e-04 - mae: 0.0138 - mse: 3.4355e-04 - val_loss: 8.7488e-04 - val_mae: 0.0202 - val_mse: 8.7488e-04\n",
      "Epoch 78/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 3.0467e-04 - mae: 0.0128 - mse: 3.0467e-04 - val_loss: 9.7714e-04 - val_mae: 0.0214 - val_mse: 9.7714e-04\n",
      "Epoch 79/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.5543e-04 - mae: 0.0138 - mse: 3.5543e-04 - val_loss: 9.4860e-04 - val_mae: 0.0216 - val_mse: 9.4860e-04\n",
      "Epoch 80/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 4.1092e-04 - mae: 0.0149 - mse: 4.1092e-04 - val_loss: 8.3177e-04 - val_mae: 0.0212 - val_mse: 8.3177e-04\n",
      "Epoch 81/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.9630e-04 - mae: 0.0147 - mse: 3.9630e-04 - val_loss: 7.8114e-04 - val_mae: 0.0200 - val_mse: 7.8114e-04\n",
      "Epoch 82/200\n",
      "2440/2440 [==============================] - 0s 63us/step - loss: 2.8087e-04 - mae: 0.0123 - mse: 2.8087e-04 - val_loss: 8.6724e-04 - val_mae: 0.0197 - val_mse: 8.6724e-04\n",
      "Epoch 83/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.4842e-04 - mae: 0.0113 - mse: 2.4842e-04 - val_loss: 8.1754e-04 - val_mae: 0.0201 - val_mse: 8.1754e-04\n",
      "Epoch 84/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.2779e-04 - mae: 0.0109 - mse: 2.2779e-04 - val_loss: 8.4810e-04 - val_mae: 0.0203 - val_mse: 8.4810e-04\n",
      "Epoch 85/200\n",
      "2440/2440 [==============================] - 0s 60us/step - loss: 2.2221e-04 - mae: 0.0109 - mse: 2.2221e-04 - val_loss: 8.3266e-04 - val_mae: 0.0204 - val_mse: 8.3266e-04\n",
      "Epoch 86/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.1705e-04 - mae: 0.0107 - mse: 2.1705e-04 - val_loss: 8.5085e-04 - val_mae: 0.0206 - val_mse: 8.5085e-04\n",
      "Epoch 87/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.1684e-04 - mae: 0.0108 - mse: 2.1684e-04 - val_loss: 8.4865e-04 - val_mae: 0.0207 - val_mse: 8.4865e-04\n",
      "Epoch 88/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.1162e-04 - mae: 0.0105 - mse: 2.1162e-04 - val_loss: 8.4454e-04 - val_mae: 0.0206 - val_mse: 8.4454e-04\n",
      "Epoch 89/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.0815e-04 - mae: 0.0104 - mse: 2.0815e-04 - val_loss: 8.4074e-04 - val_mae: 0.0209 - val_mse: 8.4074e-04\n",
      "Epoch 90/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.1775e-04 - mae: 0.0106 - mse: 2.1775e-04 - val_loss: 8.6006e-04 - val_mae: 0.0211 - val_mse: 8.6006e-04\n",
      "Epoch 91/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.0336e-04 - mae: 0.0103 - mse: 2.0336e-04 - val_loss: 8.6102e-04 - val_mae: 0.0211 - val_mse: 8.6102e-04\n",
      "Epoch 92/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 1.9837e-04 - mae: 0.0102 - mse: 1.9837e-04 - val_loss: 8.9843e-04 - val_mae: 0.0214 - val_mse: 8.9843e-04\n",
      "Epoch 93/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 2.0191e-04 - mae: 0.0106 - mse: 2.0191e-04 - val_loss: 9.3765e-04 - val_mae: 0.0216 - val_mse: 9.3765e-04\n",
      "Epoch 94/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.2001e-04 - mae: 0.0111 - mse: 2.2001e-04 - val_loss: 9.5735e-04 - val_mae: 0.0214 - val_mse: 9.5735e-04\n",
      "Epoch 95/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.3568e-04 - mae: 0.0115 - mse: 2.3568e-04 - val_loss: 9.5373e-04 - val_mae: 0.0208 - val_mse: 9.5373e-04\n",
      "Epoch 96/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.2753e-04 - mae: 0.0111 - mse: 2.2753e-04 - val_loss: 8.9649e-04 - val_mae: 0.0203 - val_mse: 8.9649e-04\n",
      "Epoch 97/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.1908e-04 - mae: 0.0107 - mse: 2.1908e-04 - val_loss: 8.8824e-04 - val_mae: 0.0205 - val_mse: 8.8824e-04\n",
      "Epoch 98/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.2011e-04 - mae: 0.0108 - mse: 2.2011e-04 - val_loss: 9.1316e-04 - val_mae: 0.0212 - val_mse: 9.1316e-04\n",
      "Epoch 99/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.7516e-04 - mae: 0.0117 - mse: 2.7516e-04 - val_loss: 9.2384e-04 - val_mae: 0.0226 - val_mse: 9.2384e-04\n",
      "Epoch 100/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 3.4489e-04 - mae: 0.0131 - mse: 3.4489e-04 - val_loss: 0.0010 - val_mae: 0.0248 - val_mse: 0.0010\n",
      "Epoch 101/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 3.4406e-04 - mae: 0.0129 - mse: 3.4406e-04 - val_loss: 8.7981e-04 - val_mae: 0.0213 - val_mse: 8.7981e-04\n",
      "Epoch 102/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.6769e-04 - mae: 0.0116 - mse: 2.6769e-04 - val_loss: 8.3324e-04 - val_mae: 0.0212 - val_mse: 8.3324e-04\n",
      "Epoch 103/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 2.5229e-04 - mae: 0.0116 - mse: 2.5229e-04 - val_loss: 9.0333e-04 - val_mae: 0.0227 - val_mse: 9.0333e-04\n",
      "Epoch 104/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.3513e-04 - mae: 0.0113 - mse: 2.3513e-04 - val_loss: 9.0073e-04 - val_mae: 0.0224 - val_mse: 9.0073e-04\n",
      "Epoch 105/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.3112e-04 - mae: 0.0111 - mse: 2.3112e-04 - val_loss: 9.2169e-04 - val_mae: 0.0229 - val_mse: 9.2169e-04\n",
      "Epoch 106/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 2.5486e-04 - mae: 0.0114 - mse: 2.5486e-04 - val_loss: 0.0011 - val_mae: 0.0258 - val_mse: 0.0011\n",
      "Epoch 107/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 2.4815e-04 - mae: 0.0112 - mse: 2.4815e-04 - val_loss: 9.2668e-04 - val_mae: 0.0225 - val_mse: 9.2668e-04\n",
      "Epoch 108/200\n",
      "2440/2440 [==============================] - 0s 60us/step - loss: 2.3799e-04 - mae: 0.0110 - mse: 2.3799e-04 - val_loss: 9.5905e-04 - val_mae: 0.0235 - val_mse: 9.5905e-04\n",
      "Epoch 109/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.3933e-04 - mae: 0.0109 - mse: 2.3933e-04 - val_loss: 9.3617e-04 - val_mae: 0.0228 - val_mse: 9.3617e-04\n",
      "Epoch 110/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.6650e-04 - mae: 0.0116 - mse: 2.6650e-04 - val_loss: 0.0011 - val_mae: 0.0255 - val_mse: 0.0011\n",
      "Epoch 111/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.6363e-04 - mae: 0.0115 - mse: 2.6363e-04 - val_loss: 0.0010 - val_mae: 0.0246 - val_mse: 0.0010\n",
      "Epoch 112/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.9574e-04 - mae: 0.0123 - mse: 2.9574e-04 - val_loss: 0.0012 - val_mae: 0.0274 - val_mse: 0.0012\n",
      "Epoch 113/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 3.8895e-04 - mae: 0.0140 - mse: 3.8895e-04 - val_loss: 0.0011 - val_mae: 0.0253 - val_mse: 0.0011\n",
      "Epoch 114/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 4.4144e-04 - mae: 0.0150 - mse: 4.4144e-04 - val_loss: 0.0010 - val_mae: 0.0237 - val_mse: 0.0010\n",
      "Epoch 115/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 6.0990e-04 - mae: 0.0178 - mse: 6.0990e-04 - val_loss: 0.0011 - val_mae: 0.0220 - val_mse: 0.0011\n",
      "Epoch 116/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 6.8770e-04 - mae: 0.0182 - mse: 6.8770e-04 - val_loss: 0.0012 - val_mae: 0.0227 - val_mse: 0.0012\n",
      "Epoch 117/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 5.7256e-04 - mae: 0.0170 - mse: 5.7256e-04 - val_loss: 0.0013 - val_mae: 0.0251 - val_mse: 0.0013\n",
      "Epoch 118/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 5.4921e-04 - mae: 0.0164 - mse: 5.4921e-04 - val_loss: 0.0014 - val_mae: 0.0246 - val_mse: 0.0014\n",
      "Epoch 119/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 5.0497e-04 - mae: 0.0151 - mse: 5.0497e-04 - val_loss: 0.0015 - val_mae: 0.0251 - val_mse: 0.0015\n",
      "Epoch 120/200\n",
      "2440/2440 [==============================] - 0s 64us/step - loss: 5.0077e-04 - mae: 0.0148 - mse: 5.0077e-04 - val_loss: 0.0015 - val_mae: 0.0260 - val_mse: 0.0015\n",
      "Epoch 121/200\n",
      "2440/2440 [==============================] - 0s 65us/step - loss: 4.7977e-04 - mae: 0.0147 - mse: 4.7977e-04 - val_loss: 0.0017 - val_mae: 0.0273 - val_mse: 0.0017\n",
      "Epoch 122/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 5.1515e-04 - mae: 0.0152 - mse: 5.1515e-04 - val_loss: 0.0021 - val_mae: 0.0303 - val_mse: 0.0021\n",
      "Epoch 123/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 6.1368e-04 - mae: 0.0165 - mse: 6.1368e-04 - val_loss: 0.0022 - val_mae: 0.0305 - val_mse: 0.0022\n",
      "Epoch 124/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 7.5485e-04 - mae: 0.0181 - mse: 7.5485e-04 - val_loss: 0.0021 - val_mae: 0.0295 - val_mse: 0.0021\n",
      "Epoch 125/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 9.1566e-04 - mae: 0.0198 - mse: 9.1566e-04 - val_loss: 0.0020 - val_mae: 0.0291 - val_mse: 0.0020\n",
      "Epoch 126/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 0.0010 - mae: 0.0208 - mse: 0.0010 - val_loss: 0.0013 - val_mae: 0.0247 - val_mse: 0.0013\n",
      "Epoch 127/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 9.6475e-04 - mae: 0.0203 - mse: 9.6475e-04 - val_loss: 9.3671e-04 - val_mae: 0.0205 - val_mse: 9.3671e-04\n",
      "Epoch 128/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 7.3801e-04 - mae: 0.0179 - mse: 7.3801e-04 - val_loss: 8.3754e-04 - val_mae: 0.0193 - val_mse: 8.3754e-04\n",
      "Epoch 129/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 5.2297e-04 - mae: 0.0152 - mse: 5.2297e-04 - val_loss: 8.2588e-04 - val_mae: 0.0189 - val_mse: 8.2588e-04\n",
      "Epoch 130/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 4.2914e-04 - mae: 0.0138 - mse: 4.2914e-04 - val_loss: 7.7192e-04 - val_mae: 0.0185 - val_mse: 7.7192e-04\n",
      "Epoch 131/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 3.7042e-04 - mae: 0.0129 - mse: 3.7042e-04 - val_loss: 7.9256e-04 - val_mae: 0.0190 - val_mse: 7.9256e-04\n",
      "Epoch 132/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 3.3153e-04 - mae: 0.0122 - mse: 3.3153e-04 - val_loss: 7.5317e-04 - val_mae: 0.0184 - val_mse: 7.5317e-04\n",
      "Epoch 133/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 3.0101e-04 - mae: 0.0116 - mse: 3.0101e-04 - val_loss: 7.7992e-04 - val_mae: 0.0189 - val_mse: 7.7992e-04\n",
      "Epoch 134/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.7890e-04 - mae: 0.0112 - mse: 2.7890e-04 - val_loss: 7.6441e-04 - val_mae: 0.0190 - val_mse: 7.6441e-04\n",
      "Epoch 135/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2440/2440 [==============================] - 0s 61us/step - loss: 2.6295e-04 - mae: 0.0109 - mse: 2.6295e-04 - val_loss: 7.8051e-04 - val_mae: 0.0194 - val_mse: 7.8051e-04\n",
      "Epoch 136/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 2.5333e-04 - mae: 0.0107 - mse: 2.5333e-04 - val_loss: 7.9060e-04 - val_mae: 0.0196 - val_mse: 7.9060e-04\n",
      "Epoch 137/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.4360e-04 - mae: 0.0105 - mse: 2.4360e-04 - val_loss: 8.0171e-04 - val_mae: 0.0197 - val_mse: 8.0171e-04\n",
      "Epoch 138/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.3937e-04 - mae: 0.0104 - mse: 2.3937e-04 - val_loss: 8.1011e-04 - val_mae: 0.0197 - val_mse: 8.1011e-04\n",
      "Epoch 139/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.3558e-04 - mae: 0.0103 - mse: 2.3558e-04 - val_loss: 8.2549e-04 - val_mae: 0.0199 - val_mse: 8.2549e-04\n",
      "Epoch 140/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.2913e-04 - mae: 0.0102 - mse: 2.2913e-04 - val_loss: 8.3040e-04 - val_mae: 0.0198 - val_mse: 8.3040e-04\n",
      "Epoch 141/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.3044e-04 - mae: 0.0103 - mse: 2.3044e-04 - val_loss: 8.6694e-04 - val_mae: 0.0196 - val_mse: 8.6694e-04\n",
      "Epoch 142/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.3233e-04 - mae: 0.0103 - mse: 2.3233e-04 - val_loss: 8.8761e-04 - val_mae: 0.0203 - val_mse: 8.8761e-04\n",
      "Epoch 143/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.2777e-04 - mae: 0.0102 - mse: 2.2777e-04 - val_loss: 9.0471e-04 - val_mae: 0.0204 - val_mse: 9.0471e-04\n",
      "Epoch 144/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.2666e-04 - mae: 0.0101 - mse: 2.2666e-04 - val_loss: 9.4678e-04 - val_mae: 0.0206 - val_mse: 9.4678e-04\n",
      "Epoch 145/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.3043e-04 - mae: 0.0103 - mse: 2.3043e-04 - val_loss: 9.4324e-04 - val_mae: 0.0209 - val_mse: 9.4324e-04\n",
      "Epoch 146/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.3084e-04 - mae: 0.0103 - mse: 2.3084e-04 - val_loss: 9.7733e-04 - val_mae: 0.0206 - val_mse: 9.7733e-04\n",
      "Epoch 147/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.3475e-04 - mae: 0.0103 - mse: 2.3475e-04 - val_loss: 9.9134e-04 - val_mae: 0.0207 - val_mse: 9.9135e-04\n",
      "Epoch 148/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.3670e-04 - mae: 0.0103 - mse: 2.3670e-04 - val_loss: 0.0010 - val_mae: 0.0210 - val_mse: 0.0010\n",
      "Epoch 149/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.4025e-04 - mae: 0.0105 - mse: 2.4025e-04 - val_loss: 0.0011 - val_mae: 0.0210 - val_mse: 0.0011\n",
      "Epoch 150/200\n",
      "2440/2440 [==============================] - 0s 60us/step - loss: 2.4389e-04 - mae: 0.0106 - mse: 2.4389e-04 - val_loss: 0.0010 - val_mae: 0.0209 - val_mse: 0.0010\n",
      "Epoch 151/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.4474e-04 - mae: 0.0106 - mse: 2.4474e-04 - val_loss: 0.0011 - val_mae: 0.0212 - val_mse: 0.0011\n",
      "Epoch 152/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.5459e-04 - mae: 0.0107 - mse: 2.5459e-04 - val_loss: 0.0011 - val_mae: 0.0213 - val_mse: 0.0011\n",
      "Epoch 153/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.5889e-04 - mae: 0.0108 - mse: 2.5889e-04 - val_loss: 0.0011 - val_mae: 0.0215 - val_mse: 0.0011\n",
      "Epoch 154/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.7106e-04 - mae: 0.0110 - mse: 2.7106e-04 - val_loss: 0.0011 - val_mae: 0.0218 - val_mse: 0.0011\n",
      "Epoch 155/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 2.8430e-04 - mae: 0.0112 - mse: 2.8430e-04 - val_loss: 0.0012 - val_mae: 0.0227 - val_mse: 0.0012\n",
      "Epoch 156/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.9535e-04 - mae: 0.0115 - mse: 2.9535e-04 - val_loss: 0.0012 - val_mae: 0.0226 - val_mse: 0.0012\n",
      "Epoch 157/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.1312e-04 - mae: 0.0118 - mse: 3.1312e-04 - val_loss: 0.0012 - val_mae: 0.0235 - val_mse: 0.0012\n",
      "Epoch 158/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 3.4111e-04 - mae: 0.0122 - mse: 3.4111e-04 - val_loss: 0.0012 - val_mae: 0.0230 - val_mse: 0.0012\n",
      "Epoch 159/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 3.6231e-04 - mae: 0.0127 - mse: 3.6231e-04 - val_loss: 0.0013 - val_mae: 0.0238 - val_mse: 0.0013\n",
      "Epoch 160/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 4.2365e-04 - mae: 0.0137 - mse: 4.2365e-04 - val_loss: 0.0012 - val_mae: 0.0235 - val_mse: 0.0012\n",
      "Epoch 161/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 4.7174e-04 - mae: 0.0144 - mse: 4.7174e-04 - val_loss: 0.0011 - val_mae: 0.0224 - val_mse: 0.0011\n",
      "Epoch 162/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 4.9117e-04 - mae: 0.0150 - mse: 4.9117e-04 - val_loss: 0.0011 - val_mae: 0.0224 - val_mse: 0.0011\n",
      "Epoch 163/200\n",
      "2440/2440 [==============================] - 0s 60us/step - loss: 5.3913e-04 - mae: 0.0157 - mse: 5.3913e-04 - val_loss: 9.5241e-04 - val_mae: 0.0222 - val_mse: 9.5241e-04\n",
      "Epoch 164/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 5.3461e-04 - mae: 0.0156 - mse: 5.3461e-04 - val_loss: 8.9685e-04 - val_mae: 0.0209 - val_mse: 8.9685e-04\n",
      "Epoch 165/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 4.7561e-04 - mae: 0.0147 - mse: 4.7561e-04 - val_loss: 9.6787e-04 - val_mae: 0.0199 - val_mse: 9.6787e-04\n",
      "Epoch 166/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 4.3481e-04 - mae: 0.0143 - mse: 4.3481e-04 - val_loss: 0.0010 - val_mae: 0.0189 - val_mse: 0.0010\n",
      "Epoch 167/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 4.2368e-04 - mae: 0.0142 - mse: 4.2368e-04 - val_loss: 0.0010 - val_mae: 0.0186 - val_mse: 0.0010\n",
      "Epoch 168/200\n",
      "2440/2440 [==============================] - 0s 61us/step - loss: 3.9789e-04 - mae: 0.0138 - mse: 3.9789e-04 - val_loss: 9.8909e-04 - val_mae: 0.0182 - val_mse: 9.8909e-04\n",
      "Epoch 169/200\n",
      "2440/2440 [==============================] - 0s 61us/step - loss: 3.9173e-04 - mae: 0.0136 - mse: 3.9173e-04 - val_loss: 9.2152e-04 - val_mae: 0.0175 - val_mse: 9.2152e-04\n",
      "Epoch 170/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.6072e-04 - mae: 0.0130 - mse: 3.6072e-04 - val_loss: 8.5602e-04 - val_mae: 0.0168 - val_mse: 8.5602e-04\n",
      "Epoch 171/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.3614e-04 - mae: 0.0125 - mse: 3.3614e-04 - val_loss: 8.3807e-04 - val_mae: 0.0166 - val_mse: 8.3807e-04\n",
      "Epoch 172/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.9581e-04 - mae: 0.0117 - mse: 2.9581e-04 - val_loss: 8.0513e-04 - val_mae: 0.0163 - val_mse: 8.0513e-04\n",
      "Epoch 173/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.7408e-04 - mae: 0.0113 - mse: 2.7408e-04 - val_loss: 7.8354e-04 - val_mae: 0.0163 - val_mse: 7.8354e-04\n",
      "Epoch 174/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.5227e-04 - mae: 0.0108 - mse: 2.5227e-04 - val_loss: 7.6903e-04 - val_mae: 0.0162 - val_mse: 7.6903e-04\n",
      "Epoch 175/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.2914e-04 - mae: 0.0103 - mse: 2.2914e-04 - val_loss: 7.5292e-04 - val_mae: 0.0162 - val_mse: 7.5292e-04\n",
      "Epoch 176/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.1341e-04 - mae: 0.0099 - mse: 2.1341e-04 - val_loss: 7.4790e-04 - val_mae: 0.0161 - val_mse: 7.4790e-04\n",
      "Epoch 177/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 1.9729e-04 - mae: 0.0095 - mse: 1.9729e-04 - val_loss: 7.3509e-04 - val_mae: 0.0161 - val_mse: 7.3509e-04\n",
      "Epoch 178/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.8408e-04 - mae: 0.0092 - mse: 1.8408e-04 - val_loss: 7.3708e-04 - val_mae: 0.0162 - val_mse: 7.3708e-04\n",
      "Epoch 179/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 1.7284e-04 - mae: 0.0089 - mse: 1.7284e-04 - val_loss: 7.3218e-04 - val_mae: 0.0163 - val_mse: 7.3218e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 1.6284e-04 - mae: 0.0087 - mse: 1.6284e-04 - val_loss: 7.2596e-04 - val_mae: 0.0163 - val_mse: 7.2596e-04\n",
      "Epoch 181/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 1.5364e-04 - mae: 0.0084 - mse: 1.5364e-04 - val_loss: 7.1443e-04 - val_mae: 0.0162 - val_mse: 7.1443e-04\n",
      "Epoch 182/200\n",
      "2440/2440 [==============================] - 0s 50us/step - loss: 1.4952e-04 - mae: 0.0083 - mse: 1.4952e-04 - val_loss: 7.1289e-04 - val_mae: 0.0163 - val_mse: 7.1289e-04\n",
      "Epoch 183/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 1.4290e-04 - mae: 0.0081 - mse: 1.4290e-04 - val_loss: 7.1534e-04 - val_mae: 0.0163 - val_mse: 7.1534e-04\n",
      "Epoch 184/200\n",
      "2440/2440 [==============================] - 0s 50us/step - loss: 1.3574e-04 - mae: 0.0079 - mse: 1.3574e-04 - val_loss: 7.1468e-04 - val_mae: 0.0163 - val_mse: 7.1468e-04\n",
      "Epoch 185/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.3248e-04 - mae: 0.0078 - mse: 1.3248e-04 - val_loss: 7.0923e-04 - val_mae: 0.0163 - val_mse: 7.0923e-04\n",
      "Epoch 186/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.2584e-04 - mae: 0.0075 - mse: 1.2584e-04 - val_loss: 7.1209e-04 - val_mae: 0.0164 - val_mse: 7.1209e-04\n",
      "Epoch 187/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 1.2555e-04 - mae: 0.0075 - mse: 1.2555e-04 - val_loss: 7.1400e-04 - val_mae: 0.0165 - val_mse: 7.1400e-04\n",
      "Epoch 188/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 1.2155e-04 - mae: 0.0074 - mse: 1.2155e-04 - val_loss: 7.0811e-04 - val_mae: 0.0164 - val_mse: 7.0811e-04\n",
      "Epoch 189/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 1.2418e-04 - mae: 0.0075 - mse: 1.2418e-04 - val_loss: 7.1538e-04 - val_mae: 0.0165 - val_mse: 7.1538e-04\n",
      "Epoch 190/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.2681e-04 - mae: 0.0076 - mse: 1.2681e-04 - val_loss: 7.3078e-04 - val_mae: 0.0165 - val_mse: 7.3078e-04\n",
      "Epoch 191/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.3147e-04 - mae: 0.0077 - mse: 1.3147e-04 - val_loss: 7.3704e-04 - val_mae: 0.0168 - val_mse: 7.3704e-04\n",
      "Epoch 192/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.3827e-04 - mae: 0.0079 - mse: 1.3827e-04 - val_loss: 7.4607e-04 - val_mae: 0.0169 - val_mse: 7.4607e-04\n",
      "Epoch 193/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.4623e-04 - mae: 0.0080 - mse: 1.4623e-04 - val_loss: 7.2504e-04 - val_mae: 0.0167 - val_mse: 7.2504e-04\n",
      "Epoch 194/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 1.3674e-04 - mae: 0.0078 - mse: 1.3674e-04 - val_loss: 7.1974e-04 - val_mae: 0.0167 - val_mse: 7.1974e-04\n",
      "Epoch 195/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 1.3485e-04 - mae: 0.0078 - mse: 1.3485e-04 - val_loss: 7.1795e-04 - val_mae: 0.0165 - val_mse: 7.1795e-04\n",
      "Epoch 196/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 1.3506e-04 - mae: 0.0079 - mse: 1.3506e-04 - val_loss: 7.3165e-04 - val_mae: 0.0167 - val_mse: 7.3165e-04\n",
      "Epoch 197/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.3254e-04 - mae: 0.0078 - mse: 1.3254e-04 - val_loss: 7.2423e-04 - val_mae: 0.0164 - val_mse: 7.2423e-04\n",
      "Epoch 198/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 1.2816e-04 - mae: 0.0076 - mse: 1.2816e-04 - val_loss: 7.1680e-04 - val_mae: 0.0163 - val_mse: 7.1680e-04\n",
      "Epoch 199/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 1.3320e-04 - mae: 0.0077 - mse: 1.3320e-04 - val_loss: 7.2532e-04 - val_mae: 0.0165 - val_mse: 7.2532e-04\n",
      "Epoch 200/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.2088e-04 - mae: 0.0076 - mse: 1.2088e-04 - val_loss: 7.1277e-04 - val_mae: 0.0162 - val_mse: 7.1277e-04\n"
     ]
    }
   ],
   "source": [
    "# Fit CNN\n",
    "cnnhistory = cnn.fit(X_train_CNN, y_train_CNN,\n",
    "                       epochs=EPOCHS,\n",
    "                       validation_data=(X_test_CNN, y_test_CNN),\n",
    "                       verbose=1,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph CNN Error, mae and mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Model WITHOUT Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4222 samples, validate on 1056 samples\n",
      "Epoch 1/200\n",
      "4222/4222 [==============================] - 0s 74us/step - loss: 0.0293 - mae: 0.1033 - mse: 0.0293 - val_loss: 0.0145 - val_mae: 0.0674 - val_mse: 0.0145\n",
      "Epoch 2/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0145 - mae: 0.0632 - mse: 0.0145 - val_loss: 0.0122 - val_mae: 0.0598 - val_mse: 0.0122\n",
      "Epoch 3/200\n",
      "4222/4222 [==============================] - 0s 54us/step - loss: 0.0133 - mae: 0.0587 - mse: 0.0133 - val_loss: 0.0115 - val_mae: 0.0586 - val_mse: 0.0115\n",
      "Epoch 4/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0127 - mae: 0.0572 - mse: 0.0127 - val_loss: 0.0109 - val_mae: 0.0582 - val_mse: 0.0109\n",
      "Epoch 5/200\n",
      "4222/4222 [==============================] - 0s 51us/step - loss: 0.0122 - mae: 0.0558 - mse: 0.0122 - val_loss: 0.0106 - val_mae: 0.0599 - val_mse: 0.0106\n",
      "Epoch 6/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0118 - mae: 0.0544 - mse: 0.0118 - val_loss: 0.0102 - val_mae: 0.0579 - val_mse: 0.0102\n",
      "Epoch 7/200\n",
      "4222/4222 [==============================] - 0s 50us/step - loss: 0.0114 - mae: 0.0532 - mse: 0.0114 - val_loss: 0.0098 - val_mae: 0.0575 - val_mse: 0.0098\n",
      "Epoch 8/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0111 - mae: 0.0531 - mse: 0.0111 - val_loss: 0.0097 - val_mae: 0.0588 - val_mse: 0.0097\n",
      "Epoch 9/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0108 - mae: 0.0522 - mse: 0.0108 - val_loss: 0.0096 - val_mae: 0.0594 - val_mse: 0.0096\n",
      "Epoch 10/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0106 - mae: 0.0515 - mse: 0.0106 - val_loss: 0.0092 - val_mae: 0.0574 - val_mse: 0.0092\n",
      "Epoch 11/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0102 - mae: 0.0499 - mse: 0.0102 - val_loss: 0.0090 - val_mae: 0.0569 - val_mse: 0.0090\n",
      "Epoch 12/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0098 - mae: 0.0489 - mse: 0.0098 - val_loss: 0.0088 - val_mae: 0.0553 - val_mse: 0.0088\n",
      "Epoch 13/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0097 - mae: 0.0487 - mse: 0.0097 - val_loss: 0.0084 - val_mae: 0.0536 - val_mse: 0.0084\n",
      "Epoch 14/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0094 - mae: 0.0478 - mse: 0.0094 - val_loss: 0.0083 - val_mae: 0.0524 - val_mse: 0.0083\n",
      "Epoch 15/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0092 - mae: 0.0472 - mse: 0.0092 - val_loss: 0.0083 - val_mae: 0.0542 - val_mse: 0.0083\n",
      "Epoch 16/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0090 - mae: 0.0475 - mse: 0.0090 - val_loss: 0.0082 - val_mae: 0.0551 - val_mse: 0.0082\n",
      "Epoch 17/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0089 - mae: 0.0473 - mse: 0.0089 - val_loss: 0.0079 - val_mae: 0.0514 - val_mse: 0.0079\n",
      "Epoch 18/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0085 - mae: 0.0461 - mse: 0.0085 - val_loss: 0.0077 - val_mae: 0.0511 - val_mse: 0.0077\n",
      "Epoch 19/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0084 - mae: 0.0451 - mse: 0.0084 - val_loss: 0.0075 - val_mae: 0.0499 - val_mse: 0.0075\n",
      "Epoch 20/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0083 - mae: 0.0449 - mse: 0.0083 - val_loss: 0.0075 - val_mae: 0.0501 - val_mse: 0.0075\n",
      "Epoch 21/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0081 - mae: 0.0447 - mse: 0.0081 - val_loss: 0.0074 - val_mae: 0.0491 - val_mse: 0.0074\n",
      "Epoch 22/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0079 - mae: 0.0435 - mse: 0.0079 - val_loss: 0.0072 - val_mae: 0.0479 - val_mse: 0.0072\n",
      "Epoch 23/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0081 - mae: 0.0438 - mse: 0.0081 - val_loss: 0.0073 - val_mae: 0.0498 - val_mse: 0.0073\n",
      "Epoch 24/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0082 - mae: 0.0452 - mse: 0.0082 - val_loss: 0.0070 - val_mae: 0.0455 - val_mse: 0.0070\n",
      "Epoch 25/200\n",
      "4222/4222 [==============================] - 0s 62us/step - loss: 0.0076 - mae: 0.0422 - mse: 0.0076 - val_loss: 0.0070 - val_mae: 0.0461 - val_mse: 0.0070\n",
      "Epoch 26/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0075 - mae: 0.0418 - mse: 0.0075 - val_loss: 0.0069 - val_mae: 0.0457 - val_mse: 0.0069\n",
      "Epoch 27/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0074 - mae: 0.0415 - mse: 0.0074 - val_loss: 0.0069 - val_mae: 0.0451 - val_mse: 0.0069\n",
      "Epoch 28/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0073 - mae: 0.0413 - mse: 0.0073 - val_loss: 0.0068 - val_mae: 0.0439 - val_mse: 0.0068\n",
      "Epoch 29/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0072 - mae: 0.0408 - mse: 0.0072 - val_loss: 0.0067 - val_mae: 0.0437 - val_mse: 0.0067\n",
      "Epoch 30/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0071 - mae: 0.0404 - mse: 0.0071 - val_loss: 0.0066 - val_mae: 0.0427 - val_mse: 0.0066\n",
      "Epoch 31/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0070 - mae: 0.0396 - mse: 0.0070 - val_loss: 0.0066 - val_mae: 0.0435 - val_mse: 0.0066\n",
      "Epoch 32/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0069 - mae: 0.0393 - mse: 0.0069 - val_loss: 0.0065 - val_mae: 0.0422 - val_mse: 0.0065\n",
      "Epoch 33/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0069 - mae: 0.0393 - mse: 0.0069 - val_loss: 0.0065 - val_mae: 0.0423 - val_mse: 0.0065\n",
      "Epoch 34/200\n",
      "4222/4222 [==============================] - 0s 51us/step - loss: 0.0068 - mae: 0.0390 - mse: 0.0068 - val_loss: 0.0065 - val_mae: 0.0425 - val_mse: 0.0065\n",
      "Epoch 35/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0068 - mae: 0.0389 - mse: 0.0068 - val_loss: 0.0064 - val_mae: 0.0425 - val_mse: 0.0064\n",
      "Epoch 36/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0066 - mae: 0.0377 - mse: 0.0066 - val_loss: 0.0064 - val_mae: 0.0416 - val_mse: 0.0064\n",
      "Epoch 37/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0067 - mae: 0.0378 - mse: 0.0067 - val_loss: 0.0063 - val_mae: 0.0414 - val_mse: 0.0063\n",
      "Epoch 38/200\n",
      "4222/4222 [==============================] - 0s 60us/step - loss: 0.0066 - mae: 0.0378 - mse: 0.0066 - val_loss: 0.0063 - val_mae: 0.0412 - val_mse: 0.0063\n",
      "Epoch 39/200\n",
      "4222/4222 [==============================] - 0s 51us/step - loss: 0.0065 - mae: 0.0372 - mse: 0.0065 - val_loss: 0.0063 - val_mae: 0.0410 - val_mse: 0.0063\n",
      "Epoch 40/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0065 - mae: 0.0368 - mse: 0.0065 - val_loss: 0.0062 - val_mae: 0.0402 - val_mse: 0.0062\n",
      "Epoch 41/200\n",
      "4222/4222 [==============================] - 0s 53us/step - loss: 0.0064 - mae: 0.0369 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0415 - val_mse: 0.0062\n",
      "Epoch 42/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0064 - mae: 0.0369 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0415 - val_mse: 0.0062\n",
      "Epoch 43/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0063 - mae: 0.0362 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0400 - val_mse: 0.0061\n",
      "Epoch 44/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0063 - mae: 0.0360 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0384 - val_mse: 0.0061\n",
      "Epoch 45/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0063 - mae: 0.0363 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0400 - val_mse: 0.0061\n",
      "Epoch 46/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0062 - mae: 0.0354 - mse: 0.0062 - val_loss: 0.0061 - val_mae: 0.0397 - val_mse: 0.0061\n",
      "Epoch 47/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0062 - mae: 0.0353 - mse: 0.0062 - val_loss: 0.0060 - val_mae: 0.0382 - val_mse: 0.0060\n",
      "Epoch 48/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0063 - mae: 0.0361 - mse: 0.0063 - val_loss: 0.0060 - val_mae: 0.0395 - val_mse: 0.0060\n",
      "Epoch 49/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0062 - mae: 0.0350 - mse: 0.0062 - val_loss: 0.0059 - val_mae: 0.0380 - val_mse: 0.0059\n",
      "Epoch 50/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0061 - mae: 0.0349 - mse: 0.0061 - val_loss: 0.0059 - val_mae: 0.0379 - val_mse: 0.0059\n",
      "Epoch 51/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0061 - mae: 0.0348 - mse: 0.0061 - val_loss: 0.0059 - val_mae: 0.0374 - val_mse: 0.0059\n",
      "Epoch 52/200\n",
      "4222/4222 [==============================] - 0s 52us/step - loss: 0.0061 - mae: 0.0350 - mse: 0.0061 - val_loss: 0.0059 - val_mae: 0.0372 - val_mse: 0.0059\n",
      "Epoch 53/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0061 - mae: 0.0349 - mse: 0.0061 - val_loss: 0.0059 - val_mae: 0.0378 - val_mse: 0.0059\n",
      "Epoch 54/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0060 - mae: 0.0343 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0374 - val_mse: 0.0059\n",
      "Epoch 55/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0060 - mae: 0.0344 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0372 - val_mse: 0.0059\n",
      "Epoch 56/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0060 - mae: 0.0342 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0358 - val_mse: 0.0059\n",
      "Epoch 57/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0060 - mae: 0.0345 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0366 - val_mse: 0.0059\n",
      "Epoch 58/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0060 - mae: 0.0343 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0369 - val_mse: 0.0059\n",
      "Epoch 59/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0059 - mae: 0.0339 - mse: 0.0059 - val_loss: 0.0059 - val_mae: 0.0375 - val_mse: 0.0059\n",
      "Epoch 60/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0060 - mae: 0.0341 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0375 - val_mse: 0.0059\n",
      "Epoch 61/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0059 - mae: 0.0340 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0357 - val_mse: 0.0058\n",
      "Epoch 62/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0059 - mae: 0.0338 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0356 - val_mse: 0.0058\n",
      "Epoch 63/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0059 - mae: 0.0338 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0372 - val_mse: 0.0058\n",
      "Epoch 64/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0058 - mae: 0.0335 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0371 - val_mse: 0.0058\n",
      "Epoch 65/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0058 - mae: 0.0331 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0338 - val_mse: 0.0058\n",
      "Epoch 66/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0059 - mae: 0.0339 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0366 - val_mse: 0.0058\n",
      "Epoch 67/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0058 - mae: 0.0332 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0350 - val_mse: 0.0058\n",
      "Epoch 68/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0059 - mae: 0.0342 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0372 - val_mse: 0.0058\n",
      "Epoch 69/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0058 - mae: 0.0333 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0376 - val_mse: 0.0058\n",
      "Epoch 70/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0058 - mae: 0.0330 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0373 - val_mse: 0.0058\n",
      "Epoch 71/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0058 - mae: 0.0330 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0370 - val_mse: 0.0058\n",
      "Epoch 72/200\n",
      "4222/4222 [==============================] - 0s 54us/step - loss: 0.0058 - mae: 0.0332 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0369 - val_mse: 0.0058\n",
      "Epoch 73/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0058 - mae: 0.0338 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0373 - val_mse: 0.0058\n",
      "Epoch 74/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0058 - mae: 0.0331 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0370 - val_mse: 0.0058\n",
      "Epoch 75/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0057 - mae: 0.0325 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0361 - val_mse: 0.0058\n",
      "Epoch 76/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0057 - mae: 0.0321 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0372 - val_mse: 0.0058\n",
      "Epoch 77/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0057 - mae: 0.0330 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0365 - val_mse: 0.0058\n",
      "Epoch 78/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0057 - mae: 0.0329 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0360 - val_mse: 0.0058\n",
      "Epoch 79/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0057 - mae: 0.0325 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0370 - val_mse: 0.0058\n",
      "Epoch 80/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0056 - mae: 0.0323 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0361 - val_mse: 0.0058\n",
      "Epoch 81/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0321 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0362 - val_mse: 0.0058\n",
      "Epoch 82/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0057 - mae: 0.0328 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0381 - val_mse: 0.0058\n",
      "Epoch 83/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0321 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0375 - val_mse: 0.0058\n",
      "Epoch 84/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0321 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0364 - val_mse: 0.0057\n",
      "Epoch 85/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0056 - mae: 0.0323 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0361 - val_mse: 0.0058\n",
      "Epoch 86/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0322 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0360 - val_mse: 0.0057\n",
      "Epoch 87/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0056 - mae: 0.0322 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0366 - val_mse: 0.0058\n",
      "Epoch 88/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0321 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0352 - val_mse: 0.0057\n",
      "Epoch 89/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0319 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0363 - val_mse: 0.0057\n",
      "Epoch 90/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0056 - mae: 0.0325 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0368 - val_mse: 0.0058\n",
      "Epoch 91/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0056 - mae: 0.0319 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0358 - val_mse: 0.0057\n",
      "Epoch 92/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0055 - mae: 0.0315 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0359 - val_mse: 0.0057\n",
      "Epoch 93/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0056 - mae: 0.0324 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 94/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0056 - mae: 0.0318 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 95/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0056 - mae: 0.0322 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0355 - val_mse: 0.0057\n",
      "Epoch 96/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0056 - mae: 0.0322 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0340 - val_mse: 0.0057\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222/4222 [==============================] - 0s 65us/step - loss: 0.0055 - mae: 0.0317 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0363 - val_mse: 0.0057\n",
      "Epoch 98/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0055 - mae: 0.0316 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 99/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0055 - mae: 0.0321 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0346 - val_mse: 0.0057\n",
      "Epoch 100/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0055 - mae: 0.0314 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 101/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0055 - mae: 0.0313 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 102/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0055 - mae: 0.0311 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0345 - val_mse: 0.0057\n",
      "Epoch 103/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0055 - mae: 0.0312 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0342 - val_mse: 0.0057\n",
      "Epoch 104/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0055 - mae: 0.0319 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 105/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0054 - mae: 0.0307 - mse: 0.0054 - val_loss: 0.0056 - val_mae: 0.0337 - val_mse: 0.0056\n",
      "Epoch 106/200\n",
      "4222/4222 [==============================] - 0s 55us/step - loss: 0.0054 - mae: 0.0307 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0368 - val_mse: 0.0057\n",
      "Epoch 107/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0054 - mae: 0.0306 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0351 - val_mse: 0.0057\n",
      "Epoch 108/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0306 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0350 - val_mse: 0.0057\n",
      "Epoch 109/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0307 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0341 - val_mse: 0.0057\n",
      "Epoch 110/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0055 - mae: 0.0314 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0366 - val_mse: 0.0057\n",
      "Epoch 111/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0055 - mae: 0.0314 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0354 - val_mse: 0.0057\n",
      "Epoch 112/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0312 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0359 - val_mse: 0.0057\n",
      "Epoch 113/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0307 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 114/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0055 - mae: 0.0312 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0369 - val_mse: 0.0057\n",
      "Epoch 115/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0055 - mae: 0.0317 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 116/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0312 - mse: 0.0054 - val_loss: 0.0058 - val_mae: 0.0366 - val_mse: 0.0058\n",
      "Epoch 117/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0310 - mse: 0.0054 - val_loss: 0.0058 - val_mae: 0.0354 - val_mse: 0.0058\n",
      "Epoch 118/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0309 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0341 - val_mse: 0.0057\n",
      "Epoch 119/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0054 - mae: 0.0306 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0352 - val_mse: 0.0057\n",
      "Epoch 120/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0054 - mae: 0.0307 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 121/200\n",
      "4222/4222 [==============================] - 0s 51us/step - loss: 0.0053 - mae: 0.0304 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0350 - val_mse: 0.0057\n",
      "Epoch 122/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0054 - mae: 0.0304 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0352 - val_mse: 0.0057\n",
      "Epoch 123/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0054 - mae: 0.0306 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0361 - val_mse: 0.0057\n",
      "Epoch 124/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0054 - mae: 0.0304 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0349 - val_mse: 0.0057\n",
      "Epoch 125/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0303 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 126/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0054 - mae: 0.0310 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0341 - val_mse: 0.0057\n",
      "Epoch 127/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0053 - mae: 0.0303 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0366 - val_mse: 0.0057\n",
      "Epoch 128/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0053 - mae: 0.0302 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0347 - val_mse: 0.0057\n",
      "Epoch 129/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0053 - mae: 0.0302 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 130/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0054 - mae: 0.0308 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0349 - val_mse: 0.0057\n",
      "Epoch 131/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0053 - mae: 0.0298 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0349 - val_mse: 0.0057\n",
      "Epoch 132/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0302 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 133/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0300 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0351 - val_mse: 0.0057\n",
      "Epoch 134/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0300 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0351 - val_mse: 0.0057\n",
      "Epoch 135/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0053 - mae: 0.0301 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0338 - val_mse: 0.0057\n",
      "Epoch 136/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0052 - mae: 0.0296 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0358 - val_mse: 0.0057\n",
      "Epoch 137/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0053 - mae: 0.0299 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0345 - val_mse: 0.0056\n",
      "Epoch 138/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0307 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0354 - val_mse: 0.0057\n",
      "Epoch 139/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0303 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0345 - val_mse: 0.0056\n",
      "Epoch 140/200\n",
      "4222/4222 [==============================] - 0s 53us/step - loss: 0.0053 - mae: 0.0298 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0355 - val_mse: 0.0057\n",
      "Epoch 141/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0054 - mae: 0.0316 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0345 - val_mse: 0.0057\n",
      "Epoch 142/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0053 - mae: 0.0309 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0341 - val_mse: 0.0057\n",
      "Epoch 143/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0052 - mae: 0.0295 - mse: 0.0052 - val_loss: 0.0056 - val_mae: 0.0351 - val_mse: 0.0056\n",
      "Epoch 144/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0298 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0344 - val_mse: 0.0057\n",
      "Epoch 145/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0303 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0341 - val_mse: 0.0057\n",
      "Epoch 146/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0300 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0343 - val_mse: 0.0057\n",
      "Epoch 147/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0052 - mae: 0.0298 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 148/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0053 - mae: 0.0302 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0340 - val_mse: 0.0056\n",
      "Epoch 149/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0054 - mae: 0.0315 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0351 - val_mse: 0.0057\n",
      "Epoch 150/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0053 - mae: 0.0307 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0360 - val_mse: 0.0056\n",
      "Epoch 151/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0053 - mae: 0.0306 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0344 - val_mse: 0.0057\n",
      "Epoch 152/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0052 - mae: 0.0298 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 153/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0053 - mae: 0.0310 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0357 - val_mse: 0.0057\n",
      "Epoch 154/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0051 - mae: 0.0288 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0355 - val_mse: 0.0057\n",
      "Epoch 155/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0052 - mae: 0.0291 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0360 - val_mse: 0.0057\n",
      "Epoch 156/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0052 - mae: 0.0297 - mse: 0.0052 - val_loss: 0.0056 - val_mae: 0.0342 - val_mse: 0.0056\n",
      "Epoch 157/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0052 - mae: 0.0299 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0343 - val_mse: 0.0057\n",
      "Epoch 158/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0052 - mae: 0.0294 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0350 - val_mse: 0.0057\n",
      "Epoch 159/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0053 - mae: 0.0309 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0343 - val_mse: 0.0057\n",
      "Epoch 160/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0052 - mae: 0.0294 - mse: 0.0052 - val_loss: 0.0056 - val_mae: 0.0340 - val_mse: 0.0056\n",
      "Epoch 161/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0052 - mae: 0.0291 - mse: 0.0052 - val_loss: 0.0056 - val_mae: 0.0348 - val_mse: 0.0056\n",
      "Epoch 162/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0053 - mae: 0.0307 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0335 - val_mse: 0.0057\n",
      "Epoch 163/200\n",
      "4222/4222 [==============================] - 0s 53us/step - loss: 0.0053 - mae: 0.0308 - mse: 0.0053 - val_loss: 0.0058 - val_mae: 0.0340 - val_mse: 0.0058\n",
      "Epoch 164/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0053 - mae: 0.0309 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0331 - val_mse: 0.0056\n",
      "Epoch 165/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0053 - mae: 0.0306 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0357 - val_mse: 0.0056\n",
      "Epoch 166/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0052 - mae: 0.0300 - mse: 0.0052 - val_loss: 0.0056 - val_mae: 0.0347 - val_mse: 0.0056\n",
      "Epoch 167/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0051 - mae: 0.0292 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0346 - val_mse: 0.0057\n",
      "Epoch 168/200\n",
      "4222/4222 [==============================] - 0s 52us/step - loss: 0.0051 - mae: 0.0291 - mse: 0.0051 - val_loss: 0.0056 - val_mae: 0.0343 - val_mse: 0.0056\n",
      "Epoch 169/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0051 - mae: 0.0286 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0349 - val_mse: 0.0057\n",
      "Epoch 170/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0051 - mae: 0.0286 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0357 - val_mse: 0.0057\n",
      "Epoch 171/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0051 - mae: 0.0288 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0361 - val_mse: 0.0057\n",
      "Epoch 172/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0052 - mae: 0.0296 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 173/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0051 - mae: 0.0290 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 174/200\n",
      "4222/4222 [==============================] - 0s 52us/step - loss: 0.0051 - mae: 0.0292 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0358 - val_mse: 0.0057\n",
      "Epoch 175/200\n",
      "4222/4222 [==============================] - 0s 53us/step - loss: 0.0051 - mae: 0.0292 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0352 - val_mse: 0.0057\n",
      "Epoch 176/200\n",
      "4222/4222 [==============================] - 0s 52us/step - loss: 0.0051 - mae: 0.0286 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 177/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0051 - mae: 0.0291 - mse: 0.0051 - val_loss: 0.0056 - val_mae: 0.0344 - val_mse: 0.0056\n",
      "Epoch 178/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0051 - mae: 0.0290 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 179/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0051 - mae: 0.0289 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 180/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0051 - mae: 0.0288 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0344 - val_mse: 0.0057\n",
      "Epoch 181/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0051 - mae: 0.0290 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0359 - val_mse: 0.0057\n",
      "Epoch 182/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0051 - mae: 0.0294 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0375 - val_mse: 0.0057\n",
      "Epoch 183/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0051 - mae: 0.0298 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0365 - val_mse: 0.0057\n",
      "Epoch 184/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0050 - mae: 0.0290 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0356 - val_mse: 0.0057\n",
      "Epoch 185/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0053 - mae: 0.0323 - mse: 0.0053 - val_loss: 0.0058 - val_mae: 0.0371 - val_mse: 0.0058\n",
      "Epoch 186/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0054 - mae: 0.0322 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0359 - val_mse: 0.0057\n",
      "Epoch 187/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0051 - mae: 0.0290 - mse: 0.0051 - val_loss: 0.0056 - val_mae: 0.0336 - val_mse: 0.0056\n",
      "Epoch 188/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0050 - mae: 0.0290 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0337 - val_mse: 0.0056\n",
      "Epoch 189/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0050 - mae: 0.0282 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0337 - val_mse: 0.0056\n",
      "Epoch 190/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0050 - mae: 0.0282 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0349 - val_mse: 0.0057\n",
      "Epoch 191/200\n",
      "4222/4222 [==============================] - 0s 51us/step - loss: 0.0050 - mae: 0.0283 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0368 - val_mse: 0.0057\n",
      "Epoch 192/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0050 - mae: 0.0286 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0359 - val_mse: 0.0057\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0050 - mae: 0.0288 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0356 - val_mse: 0.0057\n",
      "Epoch 194/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0050 - mae: 0.0288 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0350 - val_mse: 0.0057\n",
      "Epoch 195/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0051 - mae: 0.0289 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 196/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0050 - mae: 0.0287 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0349 - val_mse: 0.0056\n",
      "Epoch 197/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0050 - mae: 0.0286 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0335 - val_mse: 0.0056\n",
      "Epoch 198/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0050 - mae: 0.0292 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0350 - val_mse: 0.0056\n",
      "Epoch 199/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0050 - mae: 0.0291 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 200/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0050 - mae: 0.0286 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0340 - val_mse: 0.0057\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_CNN_nomob, y_CNN_nomob = get_CNN_states_dataset(state_cases_df, n_steps=N_STEPS)\n",
    "X_train_CNN_nomob, X_test_CNN_nomob, y_train_CNN_nomob, y_test_CNN_nomob = train_test_split(X_CNN_nomob, y_CNN_nomob, test_size=SPLIT, shuffle=False)\n",
    "\n",
    "# Set up model\n",
    "n_cnn_features = X_train_CNN_nomob.shape[2]\n",
    "cnn_nomob = get_CNN_model(N_STEPS, n_cnn_features)\n",
    "\n",
    "# Fit CNN\n",
    "cnn_nomob_history = cnn_nomob.fit(X_train_CNN_nomob, y_train_CNN_nomob,\n",
    "                                     epochs=EPOCHS,\n",
    "                                     validation_data=(X_test_CNN_nomob, y_test_CNN_nomob),\n",
    "                                     verbose=1,\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yunzhou's Neural Net Model With Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "# Set up NN, Yunzhou's model\n",
    "# Model Build functions\n",
    "def build_YNN_model(n_features):\n",
    "    # 2 hidden layers, 128 units, activation functions specified as relu\n",
    "    # and 1 final layer for prediction result\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=[n_features]),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    \n",
    "    model.compile(loss=\"mse\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mae', 'mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compile model\n",
    "n_ynn_features = X_train_YNN.shape[1]\n",
    "ynn = build_YNN_model(n_ynn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit NN\n",
    "# early_stop=keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "ynnhistory=ynn.fit(X_train_YNN, y_train_YNN,\n",
    "                   epochs=EPOCHS,\n",
    "                   validation_data=(X_test_YNN, y_test_YNN),\n",
    "                   verbose=0)#,\n",
    "                   #callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fb6cf59668>]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph NN Error, mae and mse\n",
    "plt.plot(ynnhistory.history['loss'])\n",
    "plt.plot(ynnhistory.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yunzhou's Neural Net Model WITHOUT Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4222 samples, validate on 1056 samples\n",
      "Epoch 1/200\n",
      "4222/4222 [==============================] - 1s 130us/sample - loss: 0.0187 - mae: 0.0700 - mse: 0.0187 - val_loss: 0.0115 - val_mae: 0.0586 - val_mse: 0.0115\n",
      "Epoch 2/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0128 - mae: 0.0546 - mse: 0.0128 - val_loss: 0.0077 - val_mae: 0.0429 - val_mse: 0.0077    \n",
      "Epoch 3/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0115 - mae: 0.0495 - mse: 0.0115 - val_loss: 0.0069 - val_mae: 0.0413 - val_mse: 0.0069\n",
      "Epoch 4/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0102 - mae: 0.0461 - mse: 0.0102 - val_loss: 0.0061 - val_mae: 0.0392 - val_mse: 0.0061\n",
      "Epoch 5/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0092 - mae: 0.0439 - mse: 0.0092 - val_loss: 0.0058 - val_mae: 0.0390 - val_mse: 0.0058\n",
      "Epoch 6/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0084 - mae: 0.0418 - mse: 0.0084 - val_loss: 0.0052 - val_mae: 0.0375 - val_mse: 0.0052\n",
      "Epoch 7/200\n",
      "4222/4222 [==============================] - 0s 53us/sample - loss: 0.0077 - mae: 0.0407 - mse: 0.0077 - val_loss: 0.0048 - val_mae: 0.0375 - val_mse: 0.0048\n",
      "Epoch 8/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0070 - mae: 0.0393 - mse: 0.0070 - val_loss: 0.0048 - val_mae: 0.0376 - val_mse: 0.0048\n",
      "Epoch 9/200\n",
      "4222/4222 [==============================] - 0s 61us/sample - loss: 0.0066 - mae: 0.0380 - mse: 0.0066 - val_loss: 0.0047 - val_mae: 0.0377 - val_mse: 0.0047\n",
      "Epoch 10/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0061 - mae: 0.0369 - mse: 0.0061 - val_loss: 0.0049 - val_mae: 0.0396 - val_mse: 0.0049\n",
      "Epoch 11/200\n",
      "4222/4222 [==============================] - 0s 44us/sample - loss: 0.0057 - mae: 0.0362 - mse: 0.0057 - val_loss: 0.0048 - val_mae: 0.0401 - val_mse: 0.0048\n",
      "Epoch 12/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0053 - mae: 0.0347 - mse: 0.0053 - val_loss: 0.0041 - val_mae: 0.0362 - val_mse: 0.0041\n",
      "Epoch 13/200\n",
      "4222/4222 [==============================] - 0s 49us/sample - loss: 0.0050 - mae: 0.0333 - mse: 0.0050 - val_loss: 0.0038 - val_mae: 0.0344 - val_mse: 0.0038\n",
      "Epoch 14/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0047 - mae: 0.0320 - mse: 0.0047 - val_loss: 0.0042 - val_mae: 0.0388 - val_mse: 0.0042\n",
      "Epoch 15/200\n",
      "4222/4222 [==============================] - 0s 48us/sample - loss: 0.0044 - mae: 0.0311 - mse: 0.0044 - val_loss: 0.0036 - val_mae: 0.0356 - val_mse: 0.0036\n",
      "Epoch 16/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0042 - mae: 0.0301 - mse: 0.0042 - val_loss: 0.0047 - val_mae: 0.0417 - val_mse: 0.0047\n",
      "Epoch 17/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0041 - mae: 0.0298 - mse: 0.0041 - val_loss: 0.0049 - val_mae: 0.0430 - val_mse: 0.0049\n",
      "Epoch 18/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0039 - mae: 0.0289 - mse: 0.0039 - val_loss: 0.0036 - val_mae: 0.0348 - val_mse: 0.0036\n",
      "Epoch 19/200\n",
      "4222/4222 [==============================] - 0s 51us/sample - loss: 0.0037 - mae: 0.0277 - mse: 0.0037 - val_loss: 0.0051 - val_mae: 0.0441 - val_mse: 0.0051\n",
      "Epoch 20/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0036 - mae: 0.0278 - mse: 0.0036 - val_loss: 0.0042 - val_mae: 0.0397 - val_mse: 0.0042\n",
      "Epoch 21/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0036 - mae: 0.0275 - mse: 0.0036 - val_loss: 0.0037 - val_mae: 0.0365 - val_mse: 0.0037\n",
      "Epoch 22/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0037 - mae: 0.0276 - mse: 0.0037 - val_loss: 0.0042 - val_mae: 0.0406 - val_mse: 0.0042\n",
      "Epoch 23/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0035 - mae: 0.0273 - mse: 0.0035 - val_loss: 0.0040 - val_mae: 0.0398 - val_mse: 0.0040\n",
      "Epoch 24/200\n",
      "4222/4222 [==============================] - 0s 50us/sample - loss: 0.0034 - mae: 0.0269 - mse: 0.0034 - val_loss: 0.0042 - val_mae: 0.0403 - val_mse: 0.0042\n",
      "Epoch 25/200\n",
      "4222/4222 [==============================] - 0s 50us/sample - loss: 0.0032 - mae: 0.0258 - mse: 0.0032 - val_loss: 0.0041 - val_mae: 0.0403 - val_mse: 0.0041\n",
      "Epoch 26/200\n",
      "4222/4222 [==============================] - 0s 52us/sample - loss: 0.0031 - mae: 0.0254 - mse: 0.0031 - val_loss: 0.0045 - val_mae: 0.0425 - val_mse: 0.0045\n",
      "Epoch 27/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0032 - mae: 0.0251 - mse: 0.0032 - val_loss: 0.0043 - val_mae: 0.0432 - val_mse: 0.0043\n",
      "Epoch 28/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0030 - mae: 0.0249 - mse: 0.0030 - val_loss: 0.0039 - val_mae: 0.0402 - val_mse: 0.0039\n",
      "Epoch 29/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0031 - mae: 0.0247 - mse: 0.0031 - val_loss: 0.0042 - val_mae: 0.0402 - val_mse: 0.0042\n",
      "Epoch 30/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0029 - mae: 0.0243 - mse: 0.0029 - val_loss: 0.0043 - val_mae: 0.0422 - val_mse: 0.0043\n",
      "Epoch 31/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0029 - mae: 0.0240 - mse: 0.0029 - val_loss: 0.0039 - val_mae: 0.0402 - val_mse: 0.0039\n",
      "Epoch 32/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0029 - mae: 0.0238 - mse: 0.0029 - val_loss: 0.0043 - val_mae: 0.0429 - val_mse: 0.0043\n",
      "Epoch 33/200\n",
      "4222/4222 [==============================] - 0s 51us/sample - loss: 0.0030 - mae: 0.0245 - mse: 0.0030 - val_loss: 0.0041 - val_mae: 0.0389 - val_mse: 0.0041\n",
      "Epoch 34/200\n",
      "4222/4222 [==============================] - 0s 54us/sample - loss: 0.0028 - mae: 0.0232 - mse: 0.0028 - val_loss: 0.0044 - val_mae: 0.0406 - val_mse: 0.0044\n",
      "Epoch 35/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0028 - mae: 0.0239 - mse: 0.0028 - val_loss: 0.0042 - val_mae: 0.0399 - val_mse: 0.0042\n",
      "Epoch 36/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0028 - mae: 0.0237 - mse: 0.0028 - val_loss: 0.0039 - val_mae: 0.0391 - val_mse: 0.0039\n",
      "Epoch 37/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0027 - mae: 0.0230 - mse: 0.0027 - val_loss: 0.0040 - val_mae: 0.0390 - val_mse: 0.0040\n",
      "Epoch 38/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0029 - mae: 0.0238 - mse: 0.0029 - val_loss: 0.0037 - val_mae: 0.0369 - val_mse: 0.0037\n",
      "Epoch 39/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0027 - mae: 0.0228 - mse: 0.0027 - val_loss: 0.0036 - val_mae: 0.0360 - val_mse: 0.0036\n",
      "Epoch 40/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0026 - mae: 0.0221 - mse: 0.0026 - val_loss: 0.0029 - val_mae: 0.0314 - val_mse: 0.0029\n",
      "Epoch 41/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0026 - mae: 0.0225 - mse: 0.0026 - val_loss: 0.0031 - val_mae: 0.0320 - val_mse: 0.0031\n",
      "Epoch 42/200\n",
      "4222/4222 [==============================] - 0s 48us/sample - loss: 0.0025 - mae: 0.0211 - mse: 0.0025 - val_loss: 0.0038 - val_mae: 0.0378 - val_mse: 0.0038\n",
      "Epoch 43/200\n",
      "4222/4222 [==============================] - 0s 50us/sample - loss: 0.0025 - mae: 0.0214 - mse: 0.0025 - val_loss: 0.0035 - val_mae: 0.0360 - val_mse: 0.0035\n",
      "Epoch 44/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0027 - mae: 0.0217 - mse: 0.0027 - val_loss: 0.0033 - val_mae: 0.0336 - val_mse: 0.0033\n",
      "Epoch 45/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0025 - mae: 0.0215 - mse: 0.0025 - val_loss: 0.0036 - val_mae: 0.0363 - val_mse: 0.0036\n",
      "Epoch 46/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0024 - mae: 0.0207 - mse: 0.0024 - val_loss: 0.0040 - val_mae: 0.0384 - val_mse: 0.0040\n",
      "Epoch 47/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0024 - mae: 0.0210 - mse: 0.0024 - val_loss: 0.0034 - val_mae: 0.0352 - val_mse: 0.0034\n",
      "Epoch 48/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0025 - mae: 0.0213 - mse: 0.0025 - val_loss: 0.0025 - val_mae: 0.0269 - val_mse: 0.0025\n",
      "Epoch 49/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0025 - mae: 0.0213 - mse: 0.0025 - val_loss: 0.0027 - val_mae: 0.0292 - val_mse: 0.0027\n",
      "Epoch 50/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0025 - mae: 0.0215 - mse: 0.0025 - val_loss: 0.0028 - val_mae: 0.0314 - val_mse: 0.0028\n",
      "Epoch 51/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0024 - mae: 0.0209 - mse: 0.0024 - val_loss: 0.0030 - val_mae: 0.0319 - val_mse: 0.0030\n",
      "Epoch 52/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0023 - mae: 0.0206 - mse: 0.0023 - val_loss: 0.0023 - val_mae: 0.0257 - val_mse: 0.0023\n",
      "Epoch 53/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0024 - mae: 0.0204 - mse: 0.0024 - val_loss: 0.0026 - val_mae: 0.0271 - val_mse: 0.0026\n",
      "Epoch 54/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0024 - mae: 0.0204 - mse: 0.0024 - val_loss: 0.0027 - val_mae: 0.0297 - val_mse: 0.0027\n",
      "Epoch 55/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0024 - mae: 0.0208 - mse: 0.0024 - val_loss: 0.0026 - val_mae: 0.0301 - val_mse: 0.0026\n",
      "Epoch 56/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0023 - mae: 0.0204 - mse: 0.0023 - val_loss: 0.0025 - val_mae: 0.0281 - val_mse: 0.0025\n",
      "Epoch 57/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0023 - mae: 0.0204 - mse: 0.0023 - val_loss: 0.0025 - val_mae: 0.0282 - val_mse: 0.0025\n",
      "Epoch 58/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0022 - mae: 0.0206 - mse: 0.0022 - val_loss: 0.0023 - val_mae: 0.0242 - val_mse: 0.0023\n",
      "Epoch 59/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0025 - mae: 0.0207 - mse: 0.0025 - val_loss: 0.0025 - val_mae: 0.0276 - val_mse: 0.0025\n",
      "Epoch 60/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0026 - mae: 0.0211 - mse: 0.0026 - val_loss: 0.0025 - val_mae: 0.0272 - val_mse: 0.0025\n",
      "Epoch 61/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0024 - mae: 0.0202 - mse: 0.0024 - val_loss: 0.0034 - val_mae: 0.0277 - val_mse: 0.0034\n",
      "Epoch 62/200\n",
      "4222/4222 [==============================] - 0s 48us/sample - loss: 0.0021 - mae: 0.0196 - mse: 0.0021 - val_loss: 0.0022 - val_mae: 0.0237 - val_mse: 0.0022\n",
      "Epoch 63/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0025 - mae: 0.0202 - mse: 0.0025 - val_loss: 0.0021 - val_mae: 0.0233 - val_mse: 0.0021\n",
      "Epoch 64/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0023 - mae: 0.0200 - mse: 0.0023 - val_loss: 0.0021 - val_mae: 0.0241 - val_mse: 0.0021\n",
      "Epoch 65/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0022 - mae: 0.0200 - mse: 0.0022 - val_loss: 0.0022 - val_mae: 0.0233 - val_mse: 0.0022\n",
      "Epoch 66/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0022 - mae: 0.0197 - mse: 0.0022 - val_loss: 0.0021 - val_mae: 0.0238 - val_mse: 0.0021\n",
      "Epoch 67/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0022 - mae: 0.0198 - mse: 0.0022 - val_loss: 0.0023 - val_mae: 0.0254 - val_mse: 0.0023\n",
      "Epoch 68/200\n",
      "4222/4222 [==============================] - 0s 54us/sample - loss: 0.0021 - mae: 0.0196 - mse: 0.0021 - val_loss: 0.0023 - val_mae: 0.0249 - val_mse: 0.0023\n",
      "Epoch 69/200\n",
      "4222/4222 [==============================] - 0s 49us/sample - loss: 0.0021 - mae: 0.0192 - mse: 0.0021 - val_loss: 0.0023 - val_mae: 0.0249 - val_mse: 0.0023\n",
      "Epoch 70/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0021 - mae: 0.0193 - mse: 0.0021 - val_loss: 0.0021 - val_mae: 0.0233 - val_mse: 0.0021\n",
      "Epoch 71/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0020 - mae: 0.0193 - mse: 0.0020 - val_loss: 0.0023 - val_mae: 0.0240 - val_mse: 0.0023\n",
      "Epoch 72/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0021 - mae: 0.0198 - mse: 0.0021 - val_loss: 0.0022 - val_mae: 0.0233 - val_mse: 0.0022\n",
      "Epoch 73/200\n",
      "4222/4222 [==============================] - 0s 49us/sample - loss: 0.0019 - mae: 0.0191 - mse: 0.0019 - val_loss: 0.0023 - val_mae: 0.0244 - val_mse: 0.0023\n",
      "Epoch 74/200\n",
      "4222/4222 [==============================] - 0s 50us/sample - loss: 0.0019 - mae: 0.0186 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0242 - val_mse: 0.0021\n",
      "Epoch 75/200\n",
      "4222/4222 [==============================] - 0s 53us/sample - loss: 0.0019 - mae: 0.0191 - mse: 0.0019 - val_loss: 0.0022 - val_mae: 0.0232 - val_mse: 0.0022\n",
      "Epoch 76/200\n",
      "4222/4222 [==============================] - 0s 39us/sample - loss: 0.0020 - mae: 0.0189 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0247 - val_mse: 0.0022\n",
      "Epoch 77/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0023 - mae: 0.0201 - mse: 0.0023 - val_loss: 0.0024 - val_mae: 0.0249 - val_mse: 0.0024\n",
      "Epoch 78/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0021 - mae: 0.0199 - mse: 0.0021 - val_loss: 0.0022 - val_mae: 0.0236 - val_mse: 0.0022\n",
      "Epoch 79/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0021 - mae: 0.0193 - mse: 0.0021 - val_loss: 0.0022 - val_mae: 0.0250 - val_mse: 0.0022\n",
      "Epoch 80/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0022 - mae: 0.0203 - mse: 0.0022 - val_loss: 0.0022 - val_mae: 0.0251 - val_mse: 0.0022\n",
      "Epoch 81/200\n",
      "4222/4222 [==============================] - 0s 40us/sample - loss: 0.0021 - mae: 0.0194 - mse: 0.0021 - val_loss: 0.0022 - val_mae: 0.0239 - val_mse: 0.0022\n",
      "Epoch 82/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0020 - mae: 0.0194 - mse: 0.0020 - val_loss: 0.0023 - val_mae: 0.0239 - val_mse: 0.0023\n",
      "Epoch 83/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0019 - mae: 0.0188 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0235 - val_mse: 0.0021\n",
      "Epoch 84/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0018 - mae: 0.0188 - mse: 0.0018 - val_loss: 0.0021 - val_mae: 0.0243 - val_mse: 0.0021\n",
      "Epoch 85/200\n",
      "4222/4222 [==============================] - 0s 40us/sample - loss: 0.0019 - mae: 0.0192 - mse: 0.0019 - val_loss: 0.0024 - val_mae: 0.0251 - val_mse: 0.0024\n",
      "Epoch 86/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0019 - mae: 0.0189 - mse: 0.0019 - val_loss: 0.0027 - val_mae: 0.0248 - val_mse: 0.0027\n",
      "Epoch 87/200\n",
      "4222/4222 [==============================] - 0s 39us/sample - loss: 0.0019 - mae: 0.0192 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0223 - val_mse: 0.0021\n",
      "Epoch 88/200\n",
      "4222/4222 [==============================] - 0s 40us/sample - loss: 0.0019 - mae: 0.0184 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0250 - val_mse: 0.0021\n",
      "Epoch 89/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0019 - mae: 0.0186 - mse: 0.0019 - val_loss: 0.0022 - val_mae: 0.0258 - val_mse: 0.0022\n",
      "Epoch 90/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0019 - mae: 0.0193 - mse: 0.0019 - val_loss: 0.0025 - val_mae: 0.0242 - val_mse: 0.0025\n",
      "Epoch 91/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0016 - mae: 0.0181 - mse: 0.0016 - val_loss: 0.0024 - val_mae: 0.0237 - val_mse: 0.0024\n",
      "Epoch 92/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0019 - mae: 0.0187 - mse: 0.0019 - val_loss: 0.0032 - val_mae: 0.0264 - val_mse: 0.0032\n",
      "Epoch 93/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0017 - mae: 0.0183 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0231 - val_mse: 0.0024\n",
      "Epoch 94/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0018 - mae: 0.0184 - mse: 0.0018 - val_loss: 0.0029 - val_mae: 0.0258 - val_mse: 0.0029\n",
      "Epoch 95/200\n",
      "4222/4222 [==============================] - 0s 40us/sample - loss: 0.0018 - mae: 0.0184 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0261 - val_mse: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200\n",
      "4222/4222 [==============================] - 0s 40us/sample - loss: 0.0017 - mae: 0.0179 - mse: 0.0017 - val_loss: 0.0032 - val_mae: 0.0311 - val_mse: 0.0032\n",
      "Epoch 97/200\n",
      "4222/4222 [==============================] - 0s 40us/sample - loss: 0.0019 - mae: 0.0188 - mse: 0.0019 - val_loss: 0.0024 - val_mae: 0.0237 - val_mse: 0.0024\n",
      "Epoch 98/200\n",
      "4222/4222 [==============================] - 0s 39us/sample - loss: 0.0017 - mae: 0.0184 - mse: 0.0017 - val_loss: 0.0029 - val_mae: 0.0259 - val_mse: 0.0029\n",
      "Epoch 99/200\n",
      "4222/4222 [==============================] - 0s 39us/sample - loss: 0.0018 - mae: 0.0185 - mse: 0.0018 - val_loss: 0.0023 - val_mae: 0.0257 - val_mse: 0.0023\n",
      "Epoch 100/200\n",
      "4222/4222 [==============================] - 0s 39us/sample - loss: 0.0017 - mae: 0.0180 - mse: 0.0017 - val_loss: 0.0023 - val_mae: 0.0250 - val_mse: 0.0023\n",
      "Epoch 101/200\n",
      "4222/4222 [==============================] - 0s 48us/sample - loss: 0.0017 - mae: 0.0183 - mse: 0.0017 - val_loss: 0.0027 - val_mae: 0.0270 - val_mse: 0.0027\n",
      "Epoch 102/200\n",
      "4222/4222 [==============================] - 0s 44us/sample - loss: 0.0018 - mae: 0.0185 - mse: 0.0018 - val_loss: 0.0027 - val_mae: 0.0271 - val_mse: 0.0027\n",
      "Epoch 103/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0018 - mae: 0.0182 - mse: 0.0018 - val_loss: 0.0035 - val_mae: 0.0274 - val_mse: 0.0035\n",
      "Epoch 104/200\n",
      "4222/4222 [==============================] - 0s 40us/sample - loss: 0.0018 - mae: 0.0179 - mse: 0.0018 - val_loss: 0.0027 - val_mae: 0.0285 - val_mse: 0.0027\n",
      "Epoch 105/200\n",
      "4222/4222 [==============================] - 0s 51us/sample - loss: 0.0018 - mae: 0.0185 - mse: 0.0018 - val_loss: 0.0022 - val_mae: 0.0257 - val_mse: 0.0022\n",
      "Epoch 106/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0018 - mae: 0.0190 - mse: 0.0018 - val_loss: 0.0022 - val_mae: 0.0247 - val_mse: 0.0022\n",
      "Epoch 107/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0017 - mae: 0.0179 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0269 - val_mse: 0.0024\n",
      "Epoch 108/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0016 - mae: 0.0178 - mse: 0.0016 - val_loss: 0.0024 - val_mae: 0.0255 - val_mse: 0.0024\n",
      "Epoch 109/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0017 - mae: 0.0182 - mse: 0.0017 - val_loss: 0.0028 - val_mae: 0.0280 - val_mse: 0.0028\n",
      "Epoch 110/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0017 - mae: 0.0185 - mse: 0.0017 - val_loss: 0.0021 - val_mae: 0.0244 - val_mse: 0.0021\n",
      "Epoch 111/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0017 - mae: 0.0182 - mse: 0.0017 - val_loss: 0.0025 - val_mae: 0.0267 - val_mse: 0.0025\n",
      "Epoch 112/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0017 - mae: 0.0177 - mse: 0.0017 - val_loss: 0.0029 - val_mae: 0.0267 - val_mse: 0.0029\n",
      "Epoch 113/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0018 - mae: 0.0186 - mse: 0.0018 - val_loss: 0.0025 - val_mae: 0.0252 - val_mse: 0.0025\n",
      "Epoch 114/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0017 - mae: 0.0183 - mse: 0.0017 - val_loss: 0.0023 - val_mae: 0.0232 - val_mse: 0.0023\n",
      "Epoch 115/200\n",
      "4222/4222 [==============================] - 0s 40us/sample - loss: 0.0016 - mae: 0.0177 - mse: 0.0016 - val_loss: 0.0025 - val_mae: 0.0253 - val_mse: 0.0025\n",
      "Epoch 116/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0017 - mae: 0.0188 - mse: 0.0017 - val_loss: 0.0025 - val_mae: 0.0263 - val_mse: 0.0025\n",
      "Epoch 117/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0017 - mae: 0.0188 - mse: 0.0017 - val_loss: 0.0026 - val_mae: 0.0284 - val_mse: 0.0026\n",
      "Epoch 118/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0016 - mae: 0.0178 - mse: 0.0016 - val_loss: 0.0027 - val_mae: 0.0282 - val_mse: 0.0027\n",
      "Epoch 119/200\n",
      "4222/4222 [==============================] - 0s 44us/sample - loss: 0.0017 - mae: 0.0181 - mse: 0.0017 - val_loss: 0.0026 - val_mae: 0.0273 - val_mse: 0.0026\n",
      "Epoch 120/200\n",
      "4222/4222 [==============================] - 0s 70us/sample - loss: 0.0017 - mae: 0.0181 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0259 - val_mse: 0.0024\n",
      "Epoch 121/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0018 - mae: 0.0180 - mse: 0.0018 - val_loss: 0.0032 - val_mae: 0.0286 - val_mse: 0.0032\n",
      "Epoch 122/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0017 - mae: 0.0189 - mse: 0.0017 - val_loss: 0.0026 - val_mae: 0.0272 - val_mse: 0.0026\n",
      "Epoch 123/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0017 - mae: 0.0184 - mse: 0.0017 - val_loss: 0.0026 - val_mae: 0.0263 - val_mse: 0.0026\n",
      "Epoch 124/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0016 - mae: 0.0188 - mse: 0.0016 - val_loss: 0.0025 - val_mae: 0.0259 - val_mse: 0.0025\n",
      "Epoch 125/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0018 - mae: 0.0181 - mse: 0.0018 - val_loss: 0.0025 - val_mae: 0.0262 - val_mse: 0.0025\n",
      "Epoch 126/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0016 - mae: 0.0176 - mse: 0.0016 - val_loss: 0.0023 - val_mae: 0.0242 - val_mse: 0.0023\n",
      "Epoch 127/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0017 - mae: 0.0178 - mse: 0.0017 - val_loss: 0.0026 - val_mae: 0.0259 - val_mse: 0.0026\n",
      "Epoch 128/200\n",
      "4222/4222 [==============================] - 0s 51us/sample - loss: 0.0018 - mae: 0.0188 - mse: 0.0018 - val_loss: 0.0037 - val_mae: 0.0301 - val_mse: 0.0037\n",
      "Epoch 129/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0018 - mae: 0.0188 - mse: 0.0018 - val_loss: 0.0028 - val_mae: 0.0280 - val_mse: 0.0028\n",
      "Epoch 130/200\n",
      "4222/4222 [==============================] - 0s 40us/sample - loss: 0.0016 - mae: 0.0177 - mse: 0.0016 - val_loss: 0.0025 - val_mae: 0.0258 - val_mse: 0.0025\n",
      "Epoch 131/200\n",
      "4222/4222 [==============================] - 0s 44us/sample - loss: 0.0017 - mae: 0.0182 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0267 - val_mse: 0.0024\n",
      "Epoch 132/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0018 - mae: 0.0188 - mse: 0.0018 - val_loss: 0.0024 - val_mae: 0.0254 - val_mse: 0.0024\n",
      "Epoch 133/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0018 - mae: 0.0179 - mse: 0.0018 - val_loss: 0.0022 - val_mae: 0.0254 - val_mse: 0.0022\n",
      "Epoch 134/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0018 - mae: 0.0179 - mse: 0.0018 - val_loss: 0.0029 - val_mae: 0.0281 - val_mse: 0.0029\n",
      "Epoch 135/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0017 - mae: 0.0182 - mse: 0.0017 - val_loss: 0.0037 - val_mae: 0.0315 - val_mse: 0.0037\n",
      "Epoch 136/200\n",
      "4222/4222 [==============================] - 0s 49us/sample - loss: 0.0017 - mae: 0.0184 - mse: 0.0017 - val_loss: 0.0025 - val_mae: 0.0251 - val_mse: 0.0025\n",
      "Epoch 137/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0016 - mae: 0.0177 - mse: 0.0016 - val_loss: 0.0024 - val_mae: 0.0248 - val_mse: 0.0024\n",
      "Epoch 138/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0016 - mae: 0.0175 - mse: 0.0016 - val_loss: 0.0029 - val_mae: 0.0266 - val_mse: 0.0029\n",
      "Epoch 139/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0016 - mae: 0.0173 - mse: 0.0016 - val_loss: 0.0022 - val_mae: 0.0234 - val_mse: 0.0022\n",
      "Epoch 140/200\n",
      "4222/4222 [==============================] - 0s 54us/sample - loss: 0.0016 - mae: 0.0176 - mse: 0.0016 - val_loss: 0.0029 - val_mae: 0.0301 - val_mse: 0.0029\n",
      "Epoch 141/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0016 - mae: 0.0175 - mse: 0.0016 - val_loss: 0.0026 - val_mae: 0.0264 - val_mse: 0.0026\n",
      "Epoch 142/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0015 - mae: 0.0176 - mse: 0.0015 - val_loss: 0.0030 - val_mae: 0.0289 - val_mse: 0.0030\n",
      "Epoch 143/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0018 - mae: 0.0184 - mse: 0.0018 - val_loss: 0.0033 - val_mae: 0.0304 - val_mse: 0.0033\n",
      "Epoch 144/200\n",
      "4222/4222 [==============================] - 0s 44us/sample - loss: 0.0017 - mae: 0.0180 - mse: 0.0017 - val_loss: 0.0042 - val_mae: 0.0355 - val_mse: 0.0042\n",
      "Epoch 145/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0018 - mae: 0.0191 - mse: 0.0018 - val_loss: 0.0024 - val_mae: 0.0262 - val_mse: 0.0024\n",
      "Epoch 146/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0017 - mae: 0.0183 - mse: 0.0017 - val_loss: 0.0031 - val_mae: 0.0298 - val_mse: 0.0031\n",
      "Epoch 147/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0015 - mae: 0.0178 - mse: 0.0015 - val_loss: 0.0031 - val_mae: 0.0276 - val_mse: 0.0031\n",
      "Epoch 148/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0017 - mae: 0.0185 - mse: 0.0017 - val_loss: 0.0028 - val_mae: 0.0269 - val_mse: 0.0028\n",
      "Epoch 149/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0016 - mae: 0.0180 - mse: 0.0016 - val_loss: 0.0030 - val_mae: 0.0287 - val_mse: 0.0030\n",
      "Epoch 150/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0018 - mae: 0.0181 - mse: 0.0018 - val_loss: 0.0026 - val_mae: 0.0249 - val_mse: 0.0026\n",
      "Epoch 151/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0016 - mae: 0.0183 - mse: 0.0016 - val_loss: 0.0019 - val_mae: 0.0226 - val_mse: 0.0019\n",
      "Epoch 152/200\n",
      "4222/4222 [==============================] - 0s 44us/sample - loss: 0.0016 - mae: 0.0173 - mse: 0.0016 - val_loss: 0.0025 - val_mae: 0.0253 - val_mse: 0.0025\n",
      "Epoch 153/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0017 - mae: 0.0180 - mse: 0.0017 - val_loss: 0.0027 - val_mae: 0.0261 - val_mse: 0.0027\n",
      "Epoch 154/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0018 - mae: 0.0184 - mse: 0.0018 - val_loss: 0.0025 - val_mae: 0.0256 - val_mse: 0.0025\n",
      "Epoch 155/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0017 - mae: 0.0180 - mse: 0.0017 - val_loss: 0.0025 - val_mae: 0.0251 - val_mse: 0.0025\n",
      "Epoch 156/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0018 - mae: 0.0182 - mse: 0.0018 - val_loss: 0.0026 - val_mae: 0.0269 - val_mse: 0.0026\n",
      "Epoch 157/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0016 - mae: 0.0190 - mse: 0.0016 - val_loss: 0.0035 - val_mae: 0.0277 - val_mse: 0.0035\n",
      "Epoch 158/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0016 - mae: 0.0182 - mse: 0.0016 - val_loss: 0.0022 - val_mae: 0.0222 - val_mse: 0.0022\n",
      "Epoch 159/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0015 - mae: 0.0183 - mse: 0.0015 - val_loss: 0.0023 - val_mae: 0.0234 - val_mse: 0.0023\n",
      "Epoch 160/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0016 - mae: 0.0181 - mse: 0.0016 - val_loss: 0.0025 - val_mae: 0.0252 - val_mse: 0.0025\n",
      "Epoch 161/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0016 - mae: 0.0179 - mse: 0.0016 - val_loss: 0.0023 - val_mae: 0.0245 - val_mse: 0.0023\n",
      "Epoch 162/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0015 - mae: 0.0177 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0239 - val_mse: 0.0021\n",
      "Epoch 163/200\n",
      "4222/4222 [==============================] - 0s 44us/sample - loss: 0.0016 - mae: 0.0179 - mse: 0.0016 - val_loss: 0.0023 - val_mae: 0.0236 - val_mse: 0.0023\n",
      "Epoch 164/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0015 - mae: 0.0178 - mse: 0.0015 - val_loss: 0.0024 - val_mae: 0.0253 - val_mse: 0.0024\n",
      "Epoch 165/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0018 - mae: 0.0186 - mse: 0.0018 - val_loss: 0.0035 - val_mae: 0.0314 - val_mse: 0.0035\n",
      "Epoch 166/200\n",
      "4222/4222 [==============================] - 0s 44us/sample - loss: 0.0016 - mae: 0.0183 - mse: 0.0016 - val_loss: 0.0030 - val_mae: 0.0281 - val_mse: 0.0030\n",
      "Epoch 167/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0015 - mae: 0.0171 - mse: 0.0015 - val_loss: 0.0020 - val_mae: 0.0245 - val_mse: 0.0020\n",
      "Epoch 168/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0017 - mae: 0.0180 - mse: 0.0017 - val_loss: 0.0022 - val_mae: 0.0230 - val_mse: 0.0022\n",
      "Epoch 169/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0015 - mae: 0.0173 - mse: 0.0015 - val_loss: 0.0024 - val_mae: 0.0263 - val_mse: 0.0024\n",
      "Epoch 170/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0016 - mae: 0.0177 - mse: 0.0016 - val_loss: 0.0028 - val_mae: 0.0243 - val_mse: 0.0028\n",
      "Epoch 171/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0015 - mae: 0.0182 - mse: 0.0015 - val_loss: 0.0022 - val_mae: 0.0229 - val_mse: 0.0022\n",
      "Epoch 172/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0017 - mae: 0.0181 - mse: 0.0017 - val_loss: 0.0023 - val_mae: 0.0262 - val_mse: 0.0023\n",
      "Epoch 173/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0017 - mae: 0.0179 - mse: 0.0017 - val_loss: 0.0029 - val_mae: 0.0266 - val_mse: 0.0029\n",
      "Epoch 174/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0016 - mae: 0.0180 - mse: 0.0016 - val_loss: 0.0021 - val_mae: 0.0237 - val_mse: 0.0021\n",
      "Epoch 175/200\n",
      "4222/4222 [==============================] - 0s 48us/sample - loss: 0.0017 - mae: 0.0187 - mse: 0.0017 - val_loss: 0.0021 - val_mae: 0.0257 - val_mse: 0.0021\n",
      "Epoch 176/200\n",
      "4222/4222 [==============================] - 0s 50us/sample - loss: 0.0016 - mae: 0.0179 - mse: 0.0016 - val_loss: 0.0023 - val_mae: 0.0250 - val_mse: 0.0023\n",
      "Epoch 177/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0016 - mae: 0.0180 - mse: 0.0016 - val_loss: 0.0025 - val_mae: 0.0264 - val_mse: 0.0025\n",
      "Epoch 178/200\n",
      "4222/4222 [==============================] - 0s 44us/sample - loss: 0.0017 - mae: 0.0184 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0272 - val_mse: 0.0024\n",
      "Epoch 179/200\n",
      "4222/4222 [==============================] - 0s 53us/sample - loss: 0.0014 - mae: 0.0168 - mse: 0.0014 - val_loss: 0.0023 - val_mae: 0.0254 - val_mse: 0.0023\n",
      "Epoch 180/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0016 - mae: 0.0175 - mse: 0.0016 - val_loss: 0.0025 - val_mae: 0.0262 - val_mse: 0.0025\n",
      "Epoch 181/200\n",
      "4222/4222 [==============================] - 0s 48us/sample - loss: 0.0016 - mae: 0.0184 - mse: 0.0016 - val_loss: 0.0024 - val_mae: 0.0257 - val_mse: 0.0024\n",
      "Epoch 182/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0019 - mae: 0.0183 - mse: 0.0019 - val_loss: 0.0036 - val_mae: 0.0360 - val_mse: 0.0036\n",
      "Epoch 183/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0015 - mae: 0.0176 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0273 - val_mse: 0.0021\n",
      "Epoch 184/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0015 - mae: 0.0178 - mse: 0.0015 - val_loss: 0.0024 - val_mae: 0.0272 - val_mse: 0.0024\n",
      "Epoch 185/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0017 - mae: 0.0185 - mse: 0.0017 - val_loss: 0.0028 - val_mae: 0.0313 - val_mse: 0.0028\n",
      "Epoch 186/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0017 - mae: 0.0186 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0259 - val_mse: 0.0024\n",
      "Epoch 187/200\n",
      "4222/4222 [==============================] - 0s 43us/sample - loss: 0.0014 - mae: 0.0172 - mse: 0.0014 - val_loss: 0.0023 - val_mae: 0.0264 - val_mse: 0.0023\n",
      "Epoch 188/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0015 - mae: 0.0175 - mse: 0.0015 - val_loss: 0.0022 - val_mae: 0.0266 - val_mse: 0.0022\n",
      "Epoch 189/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0017 - mae: 0.0180 - mse: 0.0017 - val_loss: 0.0034 - val_mae: 0.0367 - val_mse: 0.0034\n",
      "Epoch 190/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0017 - mae: 0.0186 - mse: 0.0017 - val_loss: 0.0023 - val_mae: 0.0245 - val_mse: 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0017 - mae: 0.0176 - mse: 0.0017 - val_loss: 0.0036 - val_mae: 0.0320 - val_mse: 0.0036\n",
      "Epoch 192/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0016 - mae: 0.0183 - mse: 0.0016 - val_loss: 0.0021 - val_mae: 0.0240 - val_mse: 0.0021\n",
      "Epoch 193/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0017 - mae: 0.0180 - mse: 0.0017 - val_loss: 0.0033 - val_mae: 0.0273 - val_mse: 0.0033\n",
      "Epoch 194/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0015 - mae: 0.0172 - mse: 0.0015 - val_loss: 0.0031 - val_mae: 0.0299 - val_mse: 0.0031\n",
      "Epoch 195/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0016 - mae: 0.0172 - mse: 0.0016 - val_loss: 0.0032 - val_mae: 0.0283 - val_mse: 0.0032\n",
      "Epoch 196/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0018 - mae: 0.0182 - mse: 0.0018 - val_loss: 0.0029 - val_mae: 0.0298 - val_mse: 0.0029\n",
      "Epoch 197/200\n",
      "4222/4222 [==============================] - 0s 42us/sample - loss: 0.0017 - mae: 0.0175 - mse: 0.0017 - val_loss: 0.0038 - val_mae: 0.0313 - val_mse: 0.0038\n",
      "Epoch 198/200\n",
      "4222/4222 [==============================] - 0s 47us/sample - loss: 0.0016 - mae: 0.0176 - mse: 0.0016 - val_loss: 0.0025 - val_mae: 0.0256 - val_mse: 0.0025\n",
      "Epoch 199/200\n",
      "4222/4222 [==============================] - 0s 45us/sample - loss: 0.0018 - mae: 0.0175 - mse: 0.0018 - val_loss: 0.0027 - val_mae: 0.0270 - val_mse: 0.0027\n",
      "Epoch 200/200\n",
      "4222/4222 [==============================] - 0s 46us/sample - loss: 0.0017 - mae: 0.0183 - mse: 0.0017 - val_loss: 0.0023 - val_mae: 0.0253 - val_mse: 0.0023\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_YNN_nomob, y_YNN_nomob = get_YNN_states_dataset(state_cases_df, n_steps=N_STEPS)\n",
    "X_train_YNN_nomob, X_test_YNN_nomob, y_train_YNN_nomob, y_test_YNN_nomob = train_test_split(X_YNN_nomob, y_YNN_nomob, test_size=SPLIT, shuffle=False)\n",
    "\n",
    "# Set up model\n",
    "n_ynn_features = X_train_YNN_nomob.shape[1]\n",
    "ynn_nomob = build_YNN_model(n_ynn_features)\n",
    "\n",
    "# Fit CNN\n",
    "ynn_nomob_history = ynn_nomob.fit(X_train_YNN_nomob, y_train_YNN_nomob,\n",
    "                                     epochs=EPOCHS,\n",
    "                                     validation_data=(X_test_YNN_nomob, y_test_YNN_nomob),\n",
    "                                     verbose=1,\n",
    "                                     shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
